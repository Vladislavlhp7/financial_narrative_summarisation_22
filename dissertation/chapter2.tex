\section{Background}\label{sec:background}

\subsection{Supervised Learning}\label{subsec:supervised-learning}

\subsection{TFIDF}\label{subsec:tfidf}

\subsection{Word Embeddings}\label{subsec:word-embeddings}

\subsection{Attention}\label{subsec:attention}

\subsection{RNN}\label{subsec:rnn}

\subsection{Transformers}\label{subsec:transformers}

\subsection{Text Summarisation}\label{subsec:text-summarisation}
Text summarisation is the task of transforming a piece of text into a shorter  version that retains the most important information.
There are two overarching categories: extractive and abstractive text summarisation.
The former formulates the problem as a subset selection problem by returning only the most salient text excerpts from the original document (\cite{zhong-etal-2020-extractive}), while the latter aims to generate content anew, similar to how humans would do.

We will outline some key models that inspired our work below:
\begin{itemize}
    \item \textbf{Gokhan}: The authors employ an unsupervised summariser based on K-Means clustering of sentences encoded with SentenceBERT (\cite{reimers2019sentence}).
    However, their embeddings are pre-trained on general text, and they suggest that employing in-domain language models would result in a better performance.

    \item \textbf{AMUSE} (\cite{litvak-vanetik-2021-summarization}): The authors design an ETS system comprised of the following steps \begin{enumerate*}
        \item shortening of report with an existing Genetic Algorithm~\cite{litvak-last-2013-multilingual},
        \item encoding sentences with BERT vectors, and
        \item performing binary classification with LSTMs for salient sentence extraction
    \end{enumerate*}.
    They suggest that further work should incorporate \begin{enumerate*}
        \item efficient preliminary sentence removal, and
        \item additional neural modelling stages for the representation and detection of relevant input text parts.
    \end{enumerate*}

    \item \textbf{Hybrid model with RL} (\cite{zmandar-etal-2021-joint}): The authors train a joint extractive-abstractive summarisation model with reinforcement learning optimised for the ROUGE-2 F1 metric.
    Their networks are based on attentive LSTMs augmented with an additional copy mechanism (\cite{vinyals2015pointer}) achieving the second highest F1 score in the FNS21 competition.

    \item \textbf{T5} Hybrid (\cite{orzhenovskii-2021-t5}): The author used T5 (\cite{rayson2019t5}) for a hybrid model fine-tuned to generate the beginning of an abstractive summary and find the closest match of the output in the reportâ€™s full text.
    This is the best performing algorithm in the FNS21 competition but also the first to consider transformer models from an abstractive summarisation perspective in the FNP workshops so far.

\end{itemize}

In this work we will be solely exploring the extractive method, and more specifically - the \emph{supervised neural-based} (i.e., RNN, Transformer) type and the \emph{unsupervised graph-based} (i.e., TextRank, LexRank) type.


\subsection{LexRank}\label{subsec:lexrank}
LexRank (\cite{Erkan2004LexRankGC}) is an unsupervised extractive summarisation method consistently used as a baseline in the FNS21 and previous challenges.
It retrieves the most salient document sentences by computing their importance based on \emph{eigenvector centrality}.
To do that the algorithm creates a graph where each sentence represents a node and each edge is a weight between two nodes (\cite{Shearing2020AutomatedTS}).
The sentences are encoded as bag-of-words vectors of size $N$ - the vocabulary size, and the weight metric is a combination of tf-idf (Eq.\ref{eq:idf},\ref{eq:tfidf}) and cosine similarity - Eq.\ref{eq:cosinesimtfidf}.

\begin{equation}
    \text{idf}(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|} \label{eq:idf}
\end{equation}

\begin{equation}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
    \label{eq:tfidf}
\end{equation}

\begin{equation}
    \text{tf\_idf\_cosine\_similarity}(s_1, s_2) = \frac{\sum_{t \in T} \text{tf-idf}(t, s_1, D) \cdot \text{tf-idf}(t, s_2, D)}{ \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_1, D)^2} \cdot \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_2, D)^2}}
    \label{eq:cosinesimtfidf}
\end{equation}

where $t$ is a term, $d$ is a document within a collection of documents/sentences $D$.

Also, $s_1$ and $s_2$ are two sentences and $T$ represents the set of all terms in both of them while $tf(t, d)$ denotes the term frequency of $t$ in $d$, and $idf(t, D)$ is the inverse document frequency of $t$ in the collection $D$.

The authors further propose finding the most important sentences by \begin{enumerate*}
    \item applying a threshold for the creation of edges with Eq.\ref{eq:cosinesimtfidf},
    \item building an adjacency matrix and normalizing it to produce \emph{transition probabilities},
    \item computing in an iterative fashion the \emph{eigenvector centrality} until convergence, and finally
    \item ranking sentences based on their \emph{lexical} PageRank (\cite{page1998anatomy}) score.
\end{enumerate*}