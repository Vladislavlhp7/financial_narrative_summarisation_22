\section{Background}\label{sec:background}

\subsection{Supervised Learning}\label{subsec:supervised-learning}

\subsection{TFIDF}\label{subsec:tfidf}

\subsection{Word Embeddings}\label{subsec:word-embeddings}
Historically, to represent a token (i.e., word) $w_{i}$ in a vocabulary $V$ numerically, we define a one-hot-encoding vector of all zeroes except of a one at the index of the word $w_{i}$ in $V$ (i.e., $i$).

The results are sparse individual word vectors being orthogonal to each other which \begin{enumerate*}
    \item waste memory (each word is a $|V|$-sized vector, hence a total of $|V|^{2}$ for all tokens) and more importantly
    \item fail to encode semantic similarity due to their cosine similarity being always zero
\end{enumerate*}.


Traditionally, AF research has represented an input text with the help of bag-of-words (BOW) models which can be viewed from the  \begin{enumerate*}
    \item the binary perspective - represent a whole document $d$ as a binary vector containing ones for all words $w_{i}$ occurring in $d$ from $V$, 
    \item the term frequency perspective - encode number of word occurrences in documents instead of binary representation (\cite{Xu2013AnAT}), and 
    \item the tf-idf perspective - extend the latter to penalise ubiquitous terms (\cite{SprckJones1972ASI}).
\end{enumerate*} 
Nevertheless, these vectors are very sparse and unable to encode more complex contextual and semantic meaning.

To address these shortcomings \emph{short}\footnote{i.e., with a small number of dimensions} and \emph{dense}\footnote{i.e., continuous real-numbered values instead of 0/1s} word embeddings like Word2Vec (\cite{mikolov2013efficient}) and FastText (\cite{bojanowski-etal-2017-enriching}) have been developed.
In~\cite{mikolov2013efficient} the authors manage to condense the vector space and ensure that word representations have \emph{multiple degrees of similarity} (\cite{mikolov-etal-2013-linguistic}) (e.g., semantic - the meaning of words, morphological - structure of sub-words, etc).


Furthermore, the proposed models - CBOW (Continuous Bag of Words\footnote{
    CBOW naming is derived from \begin{enumerate*}
        \item the continuous distributed representation of the context and 
        \item the projection layer being shared across context words, i.e., the order of words does not affect the projection (similar to how bag-of-words model fails to encode word order).     
    \end{enumerate*} 
}) and Skip-gram evidently capture subtle semantic relationships and allow intuitive arithmetic operations as shown in the popular analogy: $\overrightarrow{\text{king}}\ - \overrightarrow{\text{man}} +\overrightarrow{\text{woman}}\ \approx \overrightarrow{\text{queen}}$\footnote{
    For a formal explanation on how analogies are realised in word embeddings we direct the readers to~\cite{ethayarajh-etal-2019-towards}
}.


The intuition for the two Word2Vec models is that in CBOW, the context (i.e., the surrounding tokens) is used to predict the middle token, while in skip-gram, the input token is used to predict the context (i.e., the surrounding tokens) (Figure~\ref{fig:cbow_skipgram}).

Meanwhile, internally, the context prediction is cast as a binary classification task with positive examples being the target word and its surroundings, whereas the negatives ones are generated through random sampling from the dictionary. 
Then, the CBOW embeddings are the learned weights of a logistic regression classifier with future and history words (i.e., the context window) as the input and the goal of correctly classifying the word in-between. 
In contrast, the Skip-gram uses the middle word as an input to the classifier and predicts the individual context words around it.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{../charts/cbow_skipgram}
    \caption{Word-from-context and context-from-word prediction in CBOW and Skip-gram, respectively. Figure is from~\cite{mikolov2013efficient}}.
    \label{fig:cbow_skipgram}
\end{figure}

A downside to the Word2Vec models is that they cannot handle out-of-vocabulary (OOV) word tokens, i.e., they cannot generate an embedding vector for words missing from the training data, which is crucial in real-life problems with \emph{noisy} input or morphologically rich languages.
For this reason~\cite{bojanowski-etal-2017-enriching}, propose FastText as an extension to the Skip-gram model that makes use of character-level information to deal with unknown tokens.
Here, each word is itself a bag of character n-grams which captures meaning of prefixes, suffixes, and morphemes. 
Additionally, two symbols are further introduced to mark the beginning and the end of a token, and help differentiate between sub-words and short words.
For example, the character trigrams of the word \emph{believe} are \emph{$<$be, bel, eli, lie, iev, eve, ve$>$} where the sub-word \emph{lie} is different from the word token \emph{$<$lie$>$}.

Therefore, the final target word embedding is the sum of its constituent character n-grams which are learned via the Skip-gram model.
This makes FastText very convenient for representing unknown words as the sum of \emph{static} constituent n-grams (\cite{jurafsky2000}).

Nevertheless, when it comes to domain-specific problems, general pre-trained word embeddings do not perform very well~\cite{rahimikia2021realised}.
demonstrate that even state-of-the-art embedding models like Google's Word2Vec(skip-gram)\footnote{\url{https://code.google.com/archive/p/word2vec/}} and Facebookâ€™s FastText(skip-gram)\footnote{\url{https://fasttext.cc/}}trained on 100 billion and 16 billion words, respectively, struggle to understand financial language like \begin{enumerate*}
    \item \emph{apple} standing for the company \emph{Apple},
    \item ticker analogies, e.g., \emph{amazon} is to \emph{X} as \emph{microsoft} is to \emph{msft},
    \item grouping company name to ticker, exchange and country
\end{enumerate*}.
In the same paper, the authors propose using the same algorithms (i.e., CBOW, Skip-gram, and FastText) but training solely on financial data instead - 15 years of financial news from the Dow Jones Newswires Text News Feed database, to produce the FinText models.
They report a substantial increase in performance in and sensitivity to detecting financial jargon and relationships.
In our project we acknowledge that a purpose-built financial word embedding (trained on proprietary data) will be more beneficial and more suited for the task of text summarisation of annual financial reports, which is why we select FinText as our preferred model.

\subsection{Recurrent Neural Networks (RNNs)}\label{subsec:rnn}
The vanilla RNN is a basic type of RNN architecture designed for processing sequential data.
It learns temporal patterns from the initial data by looping over the hidden layers which allow information to persist (i.e., they serve as a network memory) (\cite{olah2015understandingLSTM}).
The key component is the recurrent hidden state $h_i$ (Figure~\ref{fig:rnn}) updated at each time step using input data and the previous hidden state (Eq.\ref{eq:rnn_hidden}).
This allows the RNN to capture contextual information and temporal dependencies in the sequence.
However, due to the inherent vanishing and exploding gradient problems with the vanilla RNNs, they have limited ability to learn long-term dependencies (\cite{bengio1994learning}).
To resolve these issues more advanced RNN architectures like LSTMs and GRUs have been developed.

\begin{equation}\label{eq:rnn_hidden}
    h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
    \footnote{
        The algebraic formulation of the vanilla RNN has the following variables: \begin{enumerate*}
            \item $x_t$ is the input at time step $t$.
            \item $h_{t-1}$ is the hidden state at time step $t-1$.
            \item $h_t$ is the hidden state at time step $t$.
            \item $y_t$ is the output at time step $t$.
            \item $f$ and $g$ are activation functions for the hidden and output layers, respectively.
            \item $W_{hh}$, $W_{xh}$, $W_{hy}$ are weight matrices for the hidden-to-hidden, input-to-hidden, and hidden-to-output connections, respectively.
            \item $b_h$ and $b_y$ are bias terms for the hidden and output layers, respectively.
        \end{enumerate*}
    }
\end{equation}
\begin{equation}
    y_t = g(W_{hy} h_t + b_y)\label{eq:rnn_output}
\end{equation}

The Long Short-Term Memory (LSTM) recurrent neural network has become a ubiquitous method in sequential problems (e.g., language modelling, time series forecasting).
This is so because it allows long-term dependencies to propagate through the network with the help of control gates \- \emph{input} and \emph{forget}, which reduce the effect of the vanishing gradient issue in the vanilla RNN\footnote{
    We direct readers to~\cite{bayer2015learning} where the authors demonstrate that the LSTM's \enquote{temporal} gradient is unaffected by the fixed weight factor $W$ of the vanilla RNN that is driving the derivative to zero. 
    This is ensured by the additional architecture unit \- the \emph{forget} gate, which learns to  control the gradient flow in the network.
}.

A more simple variant of the LSTM is the Gated Recurrent Unit (GRU) which combines the \emph{input} and \emph{forget} gates into an \emph{update} gate for a model with fewer parameters and faster training (\cite{cahuantzi2021gru}).
Nevertheless, due to the sequential nature of the LSTM, the training process cannot be parallelised across GPUs, i.e., the learning cannot be made quicker by more computational resources.

As noted by~\cite{graveshinton2013speech}, a limitation of the unidirectional RNNs is that they only make use of previous context in the sequence.
To alleviate this and establish more complex relationships between words~\cite{schuster1997birnn}, propose a bi-directional architecture consisting of a forward and a backward RNN\@.

We can therefore represent the hidden layers ($h_i$) per time-step (where $T$ is the sequence size) with the following notation: \begin{enumerate*}
    \item $(\overset{\longrightarrow}{h_1}, \ldots, \overset{\longrightarrow}{h_T})$ are the forward hidden states from left to right (i.e., $x_1,\ldots,x_T$) and
    \item $(\overset{\longleftarrow}{h_1}, \ldots, \overset{\longleftarrow}{h_T})$ are the backward hidden states from right to left (i.e., $x_T,\ldots,x_1$)
\end{enumerate*} (Figure~\ref{fig:bilstm}).

Then, for a single word $x_i$, its respective \emph{annotation} (i.e., condensed representation) is constructed by the concatenation of the forward and backward hidden states - $h_i = [\overset{\longrightarrow}{h_i^T}; \overset{\longleftarrow}{h_i^T}]^T$ as specified in~\cite{bahdanau2016neural}.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/rnn}
        \caption{Unfolded RNN}
        \label{fig:rnn}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/bi-lstm}
        \caption{Unfolded bi-directional LSTM}
        \label{fig:bilstm}
    \end{subfigure}~\caption{Unfolded recurrent architectures (\cite{zhiyong2018bilstm})}
    \label{fig:recurrent_unfolded}
\end{figure}

\subsection{Encoder-Decoder and Attention}\label{subsec:seq2seq}
However, to deal with many-to-many sequence-to-sequence problems (e.g., machine translation, speech recognition, abstractive text summarisation) a new type of neural architecture is necessary~\cite{sutskever2014sequence}.
and~\cite{cho-etal-2014-learning} introduced the Encoder-Decoder network (Figure~\ref{fig:encoderdecoder}) with \begin{enumerate*}
    \item the encoder being an RNN that maps an input sequence $(x_1, x_2, \dots, x_n)$ to a continuous fixed-length context vector $c$ and
    \item the decoder, also an RNN, taking this vector and producing an output sequence $(y_1, y_2, \dots, y_m)$
\end{enumerate*}.
They train the two RNNs jointly, maximising the conditional probability of the target sequence given a source sequence, i.e., $p(y_1, \ldots, y_{T'} | x_1, \ldots, x_T)$.
For the selection of the neural model the authors had naturally chosen LSTM and GRU, respectively, due to the resolution of the vanishing gradient problem (as discussed in Section~\ref{subsec:rnn}) and for the time being~\cite{sutskever2014sequence} managed to achieve state-of-the-art results in machine translation.


% Encoder-decoder schema
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=1.5cm and 1cm,>=stealth',auto,font=\footnotesize]
        % Encoder
        \node[draw,minimum height=1cm] (x1) {$x_1$};
        \node[draw,minimum height=1cm, right=of x1] (x2) {$x_2$};
        \node[right=of x2] (dots) {$\dots$};
        \node[draw,minimum height=1cm, right=of dots] (xT) {$x_T$};
        \node[draw,minimum height=1cm, circle, above=of x1] (h1) {$h_1$};
        \node[draw,minimum height=1cm, circle, above=of x2] (h2) {$h_2$};
        \node[draw,minimum height=1cm, circle, above=of xT] (hT) {$h_T$};

        \draw[->] (x1) -- (h1);
        \draw[->] (x2) -- (h2);
        \draw[->] (xT) -- (hT);
        \draw[->] (h1) -- (h2);
        \draw[->] (h2) -- (hT);

        % Context vector
        \node[draw,minimum height=1cm, circle, right=of hT] (c) {c};

        \draw[->] (hT) -- (c);

        % Decoder
        \node[draw,minimum height=1cm, circle, right=of c, yshift=1.5cm] (s1) {$s_1$};
        \node[draw,minimum height=1cm, circle, right=of s1] (s2) {$s_2$};
        \node[right=of s2, yshift=0.25cm] (sdots) {$\dots$};
        \node[draw,minimum height=1cm, circle, right=of sdots, yshift=-0.25cm] (sT) {$s_{T'}$};
        \node[draw,minimum height=1cm, above=of s1] (y1) {$y_1$};
        \node[draw,minimum height=1cm, above=of s2] (y2) {$y_2$};
        \node[draw,minimum height=1cm, above=of sT] (yT) {$y_{T'}$};

        \draw[->, dotted] (c) -- (y1);
        \draw[->, dotted] (c) -- (y2);
        \draw[->, dotted] (c) -- (yT);
        \draw[->, dotted] (c) -- (s1);
        \draw[->, dotted] (c) -- (s2);
        \draw[->, dotted] (c) -- (sT);

        \draw[->] (y1) -- (y2);
        \draw[->] (y2) -- (yT);

        \draw[->] (y1) -- (s2);
        \draw[->] (y2) -- (sT);

        \draw[->] (s1) -- (s2);
        \draw[->] (s2) -- (sT);

        \draw[->] (s1) -- (y1);
        \draw[->] (s2) -- (y2);
        \draw[->] (sT) -- (yT);
    \end{tikzpicture}
    \caption{Encoder-Decoder Schema}
    \label{fig:encoderdecoder}
\end{figure}


It is in~\cite{bahdanau2016neural} where the authors suppose that the fixed-length vector results in a bottleneck such that the longer the sequences, the worse the compression performance is for neural networks.
Meanwhile, they propose to replace the fixed-length vector with a variable-length one which searches for the most relevant information from the source sequence.
This is achieved through an alignment model which accepts as input the produced annotations from the encoder $h_1,\ldots, h_T$ (see Section~\ref{subsec:rnn} for details on how these are generated).
Then the context vector $c_{i}$ which is now distinct for each input word (unlike in~\cite{cho-etal-2014-learning,sutskever2014sequence}) becomes a weighted sum\footnote{
    And hence it is known as the \emph{additive attention}.
} of the annotations $h_{j}$ (Eq.\ref{eq:context}) and each weight $\alpha_{ij}$ is calculated as the normalized attention score (Eq.\ref{eq:attention_score}).
The alignment model is itself a neural network trained jointly with the encoder-decoder system and it evaluates the importance $e_{ij}$ of an annotation $h_{j}$ in generating a new state $s_{i}$ and output token $y_{i}$ (Eq.\ref{eq:energy}).
Intuitively, as noted in~\cite{galassi2020attention}, this attention mechanism computes a weight distribution on the input sequence and \emph{attends} (i.e., assigns a larger weight) to the most relevant parts.

\begin{equation}\label{eq:context}
    c_i = \sum_j^T{\alpha_{ij}h_j}
\end{equation}
\begin{equation}\label{eq:attention_score}
    \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T} \exp(e_{ik})}
\end{equation}
\begin{equation}\label{eq:energy}
    e_{ij} = a(s_{i-1}, h_j)
\end{equation}


\subsection{Transformers}\label{subsec:transformers}

The Transformer (\cite{vaswani2017attention}) is another sequence-to-sequence architecture which follows the overall encoder-decoder architecture (\cite{sutskever2014sequence}), but differs in the following aspects:
\begin{enumerate}
    \item \emph{Internal architecture} - instead of RNNs (\cite{cho-etal-2014-learning}), the encoder and the decoder are based on fully-connected feed-forward neural networks.
    The reasoning is that, although recurrent hidden layers have been used for compressing sequences into a context vector, the authors of the transformer introduce the concept of self-attention (extending the attention from~\cite{bahdanau2016neural}) to represent only the most salient parts of the input.
    \item \emph{Self-Attention and Global context} - while RNN architectures struggle to deal with long-range dependencies (due to the vanishing gradient problem and the last-layer bottleneck~\cite{bahdanau2016neural}), the authors propose
    self-attention to allow capturing of global context.
    This mechanism has two key components (Fig.\ref{fig:attention_dot_product}):
    \begin{itemize}
        \item \emph{The scaled dot-product} (Eq.\ref{fig:attention}) computes the attention weights (see Section~\ref{subsec:seq2seq}) to be used for generating a weighted representation of the input sequence.
        Each token from the input sequence is represented as a $q$-query, $k$-key, and $v$-value vector (packed together into linearly-projected\footnote{
            The query vectors, key vectors, and value vectors are linearly projected from the input token embeddings using separate learnable weight matrices.
        } $Q$, $K$, $V$ matrices, respectively).
        Meanwhile, the dot product computes the similarity score between $Q$ - the current token's focus and $K$ - the context of the other tokens, whereas the scaling factor $\sqrt{d_{k}}$ prevents against extreme differences in softmax calculation - leading to slow convergence.
        Once the attention weights have been calculated, they are multiplied with $V$ for a new context-aware representation combining information from other tokens in the sequence.
        This mechanism allows the Transformer to learn relationships between any two tokens and hence the significant computational load $O(n^2)$.
        Nevertheless, since these computations can be performed independently for each token, the entire input sequence can be processed simultaneously unlike in RNN architectures.

        \begin{figure}
            \begin{equation}
                \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\label{eq:equation2}
            \end{equation}
            \caption{Attention calculation (Query, Key, Value)}
            \label{fig:attention}
        \end{figure}

        \item \emph{The multi-head attention} (\cite{vaswani2017attention}) is a mechanism that allows the Transformer to learn multiple representations of the input sequence.
        This is achieved by stacking multiple attention heads (each with its own $Q$, $K$, $V$ matrices) and concatenating their outputs (Figure~\ref{fig:attention_dot_product}).
        For each head $h_i$, the $Q$, $K$, and $V$ vectors will be linearly projected with different weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$, respectively.
        The intuition is that each attention head learns different aspects of the relationships that exist among inputs (e.g., syntactic, semantic, and discourse relationships~\cite{jurafsky2000}) and the concatenation of the different representations allows the Transformer to learn a richer overall representation of the input sequence.

    \end{itemize}
    \item \emph{Positional Encoding} - the Transformer does not have a recurrent hidden layer to capture the sequential nature of the input sequence.
    Instead, the authors (\cite{vaswani2017attention}) propose adding a positional encoding to the input embeddings as a way to capture positional information.
    \item \emph{Training Details} - Because of the non-sequential nature of the Transformer, the heavy self-attention computations can be parallelised on modern hardware - GPUs and TPUs which makes this model easy during training and inference time.
    Additionally, the proposed dot-product attention is practically much faster and more space-efficient in comparison to the additive one (\cite{bahdanau2016neural}), due to highly optimized matrix multiplication libraries.
    Nevertheless, at least two computational drawbacks are that Transformers require an additional positional embedding and also have a quadratic complexity when calculating the self-attention which is extremely expensive for long sequences (\cite{jurafsky2000}).
\end{enumerate}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\columnwidth]{../charts/attention}~\caption{Scaled dot-product and multi-head attention (\cite{vaswani2017attention})}
    \label{fig:attention_dot_product}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
  input/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=red!20},
  token/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=blue!20},
  seg_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=orange!20},
  pos_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=green!20},
  input_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=purple!20},
  plus/.style={}
]

    % Tokens
    \node[input] (i1) {[CLS]};
    \node[input, right=0.5cm of i1] (i2) {It};
    \node[input, right=0.5cm of i2] (i3) {is};
    \node[input, right=0.5cm of i3] (i4) {sunny};
    \node[input, right=0.5cm of i4] (i5) {[SEP]};
    \node[input, right=0.5cm of i5] (i6) {But};
    \node[input, right=0.5cm of i6] (i7) {also};
    \node[input, right=0.5cm of i7] (i8) {cold};
    \node[input, right=0.5cm of i8] (i9) {[SEP]};

        % Tokens
    \node[token, below=1cm of i1] (t1) {$E_{[CLS]}$};
    \node[token, below=1cm of i2] (t2) {$E_{It}$};
    \node[token, below=1cm of i3] (t3) {$E_{is}$};
    \node[token, below=1cm of i4] (t4) {$E_{sunny}$};
    \node[token, below=1cm of i5] (t5) {$E_{[SEP]}$};
    \node[token, below=1cm of i6] (t6) {$E_{But}$};
    \node[token, below=1cm of i7] (t7) {$E_{also}$};
    \node[token, below=1cm of i8] (t8) {$E_{cold}$};
    \node[token, below=1cm of i9] (t9) {$E_{[SEP]}$};

    % Add plus nodes between inputs
    \draw ($(i1.south west) + (0,-0.3)$) -- ($(i9.south east) + (0,-0.3)$);

        % Segmentation embeddings
    \node[seg_embed, below=1cm of t1] (s1) {$E_{A}$};
    \node[seg_embed, below=1cm of t2] (s2) {$E_{A}$};
    \node[seg_embed, below=1cm of t3] (s3) {$E_{A}$};
    \node[seg_embed, below=1cm of t4] (s4) {$E_{A}$};
    \node[seg_embed, below=1cm of t5] (s5) {$E_{A}$};
    \node[seg_embed, below=1cm of t6] (s6) {$E_{B}$};
    \node[seg_embed, below=1cm of t7] (s7) {$E_{B}$};
    \node[seg_embed, below=1cm of t8] (s8) {$E_{B}$};
    \node[seg_embed, below=1cm of t9] (s9) {$E_{B}$};

    % Position embeddings
    \node[pos_embed, below=1cm of s1] (p1) {$E_{0}$};
    \node[pos_embed, below=1cm of s2] (p2) {$E_{1}$};
    \node[pos_embed, below=1cm of s3] (p3) {$E_{2}$};
    \node[pos_embed, below=1cm of s4] (p4) {$E_{3}$};
    \node[pos_embed, below=1cm of s5] (p5) {$E_{4}$};
    \node[pos_embed, below=1cm of s6] (p6) {$E_{5}$};
    \node[pos_embed, below=1cm of s7] (p7) {$E_{6}$};
    \node[pos_embed, below=1cm of s8] (p8) {$E_{7}$};
    \node[pos_embed, below=1cm of s9] (p9) {$E_{8}$};

    % Pluses at 0.25 between each (s, t) pair
    \node[above=0.25cm of $(s1)!.25!(t1)$, font=\large] {$+$};
    \node[above=0.25cm of $(s2)!.25!(t2)$, font=\large] {$+$};
    \node[above=0.25cm of $(s3)!.25!(t3)$, font=\large] {$+$};
    \node[above=0.25cm of $(s4)!.25!(t4)$, font=\large] {$+$};
    \node[above=0.25cm of $(s5)!.25!(t5)$, font=\large] {$+$};
    \node[above=0.25cm of $(s6)!.25!(t6)$, font=\large] {$+$};
    \node[above=0.25cm of $(s7)!.25!(t7)$, font=\large] {$+$};
    \node[above=0.25cm of $(s8)!.25!(t8)$, font=\large] {$+$};
    \node[above=0.25cm of $(s9)!.25!(t9)$, font=\large] {$+$};


    % Pluses at 0.25 between each (p, s) pair
    \node[above=0.25cm of $(p1)!.25!(s1)$, font=\large] {$+$};
    \node[above=0.25cm of $(p2)!.25!(s2)$, font=\large] {$+$};
    \node[above=0.25cm of $(p3)!.25!(s3)$, font=\large] {$+$};
    \node[above=0.25cm of $(p4)!.25!(s4)$, font=\large] {$+$};
    \node[above=0.25cm of $(p5)!.25!(s5)$, font=\large] {$+$};
    \node[above=0.25cm of $(p6)!.25!(s6)$, font=\large] {$+$};
    \node[above=0.25cm of $(p7)!.25!(s7)$, font=\large] {$+$};
    \node[above=0.25cm of $(p8)!.25!(s8)$, font=\large] {$+$};
    \node[above=0.25cm of $(p9)!.25!(s9)$, font=\large] {$+$};

\end{tikzpicture}
    \caption{BERT: Input Embeddings}
    \label{fig:bert_input}
\end{figure}

\subsection{Text Summarisation}\label{subsec:text-summarisation}
Text summarisation is the task of transforming a piece of text into a shorter  version that retains the most important information.
There are two overarching categories: extractive and abstractive text summarisation.
The former formulates the problem as a subset selection problem by returning only the most salient text excerpts from the original document (\cite{zhong-etal-2020-extractive}), while the latter aims to generate content anew, similar to how humans would do.

We will outline some key models that inspired our work below:
\begin{itemize}
    \item \textbf{Gokhan}: The authors employ an unsupervised summariser based on K-Means clustering of sentences encoded with SentenceBERT (\cite{reimers2019sentence}).
    However, their embeddings are pre-trained on general text, and they suggest that employing in-domain language models would result in a better performance.

    \item \textbf{AMUSE} (\cite{litvak-vanetik-2021-summarization}): The authors design an ETS system comprised of the following steps \begin{enumerate*}
        \item shortening of report with an existing Genetic Algorithm~\cite{litvak-last-2013-multilingual},
        \item encoding sentences with BERT vectors, and
        \item performing binary classification with LSTMs for salient sentence extraction
    \end{enumerate*}.
    They suggest that further work should incorporate \begin{enumerate*}
        \item efficient preliminary sentence removal, and
        \item additional neural modelling stages for the representation and detection of relevant input text parts.
    \end{enumerate*}

    \item \textbf{Hybrid model with RL} (\cite{zmandar-etal-2021-joint}): The authors train a joint extractive-abstractive summarisation model with reinforcement learning optimised for the ROUGE-2 F1 metric.
    Their networks are based on attentive LSTMs augmented with an additional copy mechanism (\cite{vinyals2015pointer}) achieving the second highest F1 score in the FNS21 competition.

    \item \textbf{T5 Hybrid} (\cite{orzhenovskii-2021-t5}): The author used T5 (\cite{rayson2019t5}) for a hybrid model fine-tuned to generate the beginning of an abstractive summary and find the closest match of the output in the reportâ€™s full text.
    This is the best performing algorithm in the FNS21 competition but also the first to consider transformer models from an abstractive summarisation perspective in the FNP workshops so far.

\end{itemize}

In this work we will be solely exploring the extractive method, and more specifically - the \emph{supervised neural-based} (i.e., RNN, Transformer) type and the \emph{unsupervised graph-based} (i.e., TextRank, LexRank) type.


\subsection{LexRank}\label{subsec:lexrank}
LexRank (\cite{Erkan2004LexRankGC}) is an unsupervised extractive summarisation method consistently used as a baseline in the FNS21 and previous challenges.
It retrieves the most salient document sentences by computing their importance based on \emph{eigenvector centrality}.
To do that the algorithm creates a graph where each sentence represents a node and each edge is a weight between two nodes (\cite{Shearing2020AutomatedTS}).
The sentences are encoded as bag-of-words vectors of size $N$ - the vocabulary size, and the weight metric is a combination of tf-idf (Eq.\ref{eq:idf},\ref{eq:tfidf}) and cosine similarity - Eq.\ref{eq:cosinesimtfidf}.

\begin{equation}
    \text{idf}(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|} \label{eq:idf}
\end{equation}

\begin{equation}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
    \label{eq:tfidf}
\end{equation}

\begin{equation}
    \text{tf\_idf\_cosine\_similarity}(s_1, s_2) = \frac{\sum_{t \in T} \text{tf-idf}(t, s_1, D) \cdot \text{tf-idf}(t, s_2, D)}{ \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_1, D)^2} \cdot \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_2, D)^2}}
    \label{eq:cosinesimtfidf}
\end{equation}

where $t$ is a term, $d$ is a document within a collection of documents/sentences $D$.

Also, $s_1$ and $s_2$ are two sentences and $T$ represents the set of all terms in both of them while $tf(t, d)$ denotes the term frequency of $t$ in $d$, and $idf(t, D)$ is the inverse document frequency of $t$ in the collection $D$.

The authors further propose finding the most important sentences by \begin{enumerate*}
    \item applying a threshold for the creation of edges with Eq.\ref{eq:cosinesimtfidf},
    \item building an adjacency matrix and normalizing it to produce \emph{transition probabilities},
    \item computing in an iterative fashion the \emph{eigenvector centrality} until convergence, and finally
    \item ranking sentences based on their \emph{lexical} PageRank (\cite{page1998anatomy}) score.
\end{enumerate*}