\chapter{Background}\label{ch:background}
In this chapter we provide the necessary background knowledge for the reader to understand our proposed solution in Chapter~\ref{ch:methodology}.
We will describe key concepts in modern extractive summarisation, such as word embeddings, Recurrent Neural Networks, attention mechanism and Transformer architecture.
We also describe various official FNS and baseline models which we use as benchmarks in our evaluation phase (Section~\ref{sec:quantitative-evaluation}).

\section{Supervised Learning}\label{sec:supervised_learning}
Supervised learning is a machine learning paradigm where a training set $X$ with data points $x$ is provided, along with the corresponding set $Y$ with labels $y$.
Then the goal is to learn a function $f$ that maps $X$ to $Y$, i.e., $f: X \rightarrow Y$, which makes predictions $f(x)$ as close as possible to the true labels $y$~\cite{sammut2011encyclopedia}.
The difference between $f(x)$ and $y$ is quantified by a loss function $L$, which is being minimized during the training process by updating the model parameters with respect to the gradient of the loss~\cite{Goodfellow-et-al-2016}.
In the context of extractive summarisation, supervised learning can be used to identify the most important sentences in a document, which can then be used to generate a summary.
In this case, the task can be cast as a binary classification problem, where the sentence labels are either $1$ (summarising) or $0$ (non-summarising)~\cite{manning_raghavan_schutze_2008}.


\section{Tf-Idf}\label{sec:tfidf}
Tf-Idf (Term Frequency - Inverse Document Frequency) is a statistical technique intended to reflect the importance of a word to a document in a corpus.
It is often used as a weighting factor in information retrieval and text mining.
The term frequency (Eq.\ref{eq:tf}) is the number of occurrences of term $t$ in document $d$~\cite{luhn1957termfreq},
but it is not enough to capture the importance of a term due to \emph{all terms being considered equally important}~\cite{manning_raghavan_schutze_2008}.
Meanwhile, the \emph{document frequency ($df_{t}$)} is the number of documents in the corpus containing the term $t$,
which evaluates how \emph{common} and \emph{unimportant} a term is~\cite{leskovec_rajaraman_ullman_2020}.
Therefore, \emph{idf} becomes $\frac{|D|}{\text{df}_{t}}$, where $|D|$ is the corpus size.
It is also sometimes written in log-form as in Equation~\ref{eq:idf} due to the $|D|$ being large.
Then the tf-idf term weight $w_{t,d}$ is calculated as the product of the two elements (Eq.\ref{eq:tfidf})~\cite{jurafsky2000}.
This formulation now represents the importance of a term $t$ normalised by its \emph{commonness}.
The tf-idf has also been successfully integrated in summarisation methods like LexRank~\cite{Erkan2004LexRankGC} (Section~\ref{sec:lexrank})
and in FNS competitive systems~\cite{litvak2021tiber, el-haj-ogden-2022-financial}.

% tft,d = log10(count(t,d) +1)
\begin{equation} \label{eq:tf}
    \text{tf}(t, d) = \log \left( \text{count}(t, d) + 1 \right)
\end{equation}

\begin{equation}\label{eq:idf}
    \text{idf}(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|}
\end{equation}

\begin{equation}\label{eq:tfidf}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
\end{equation}

where $t$ is a term, $d$ is a document within a collection of documents/sentences $D$.

\section{Word Embeddings}\label{sec:word-embeddings}
Historically, to represent a token (i.e., word) $w_{i}$ in a vocabulary $V$ numerically, we define a one-hot-encoding vector of all zeroes except of a one at the index of the word $w_{i}$ in $V$ (i.e., $i$).

The results are sparse individual word vectors being orthogonal to each other which \begin{enumerate*}
    \item waste memory (each word is a $|V|$-sized vector, hence a total of $|V|^{2}$ for all tokens) and more importantly
    \item fail to encode semantic similarity due to their cosine similarity being always zero
\end{enumerate*}.


Traditionally, AF research has represented an input text with the help of bag-of-words (BOW) models which can be viewed from the  \begin{enumerate*}
    \item the binary perspective - represent a whole document $d$ as a binary vector containing ones for all words $w_{i}$ occurring in $d$ from $V$,
    \item the term frequency perspective - encode number of word occurrences in documents instead of binary representation~\cite{Xu2013AnAT}, and
    \item the tf-idf perspective - extend the latter to penalise ubiquitous terms~\cite{SprckJones1972ASI}.
\end{enumerate*}
Nevertheless, these vectors are very sparse and unable to encode more complex contextual and semantic meaning.

To address these shortcomings \emph{short}\footnote{i.e., with a small number of dimensions} and \emph{dense}\footnote{i.e., continuous real-numbered values instead of 0/1s} word embeddings like Word2Vec~\cite{mikolov2013efficient} and FastText~\cite{bojanowski-etal-2017-enriching} have been developed.
In~\cite{mikolov2013efficient} the authors manage to condense the vector space and ensure that word representations have \emph{multiple degrees of similarity}~\cite{mikolov-etal-2013-linguistic} (e.g., semantic - the meaning of words, morphological - structure of sub-words, etc).


Furthermore, the proposed models - CBOW (Continuous Bag of Words\footnote{
    CBOW naming is derived from \begin{enumerate*}
        \item the continuous distributed representation of the context and
        \item the projection layer being shared across context words, i.e., the order of words does not affect the projection (similar to how bag-of-words model fails to encode word order).
    \end{enumerate*}
}) and Skip-gram evidently capture subtle semantic relationships and allow intuitive arithmetic operations as shown in the popular analogy: $\overrightarrow{\text{king}}\ - \overrightarrow{\text{man}} +\overrightarrow{\text{woman}}\ \approx \overrightarrow{\text{queen}}$\footnote{
    For a formal explanation on how analogies are realised in word embeddings we direct the readers to~\cite{ethayarajh-etal-2019-towards}
}.


The intuition for the two Word2Vec models is that in CBOW, the context (i.e., the surrounding tokens) is used to predict the middle token, while in skip-gram, the input token is used to predict the context (i.e., the surrounding tokens) (Figure~\ref{fig:cbow_skipgram}).

Meanwhile, internally, the context prediction is cast as a binary classification task with positive examples being the target word and its surroundings, whereas the negatives ones are generated through random sampling from the dictionary.
Then, the CBOW embeddings are the learned weights of a logistic regression classifier with future and history words (i.e., the context window) as the input and the goal of correctly classifying the word in-between.
In contrast, the Skip-gram uses the middle word as an input to the classifier and predicts the individual context words around it.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{../charts/cbow_skipgram}~\caption{Word-from-context and context-from-word prediction in CBOW and Skip-gram, respectively, taken from~\cite{mikolov2013efficient}}
    \label{fig:cbow_skipgram}
\end{figure}

A downside to the Word2Vec models is that they cannot handle out-of-vocabulary (OOV) word tokens, i.e., they cannot generate an embedding vector for words missing from the training data, which is crucial in real-life problems with \emph{noisy} input or morphologically rich languages.
For this reason~\cite{bojanowski-etal-2017-enriching}, propose FastText as an extension to the Skip-gram model that makes use of character-level information to deal with unknown tokens.
Here, each word is itself a bag of character n-grams which captures meaning of prefixes, suffixes, and morphemes.
Additionally, two symbols are further introduced to mark the beginning and the end of a token, and help differentiate between sub-words and short words.
For example, the character trigrams of the word \emph{believe} are \emph{$<$be, bel, eli, lie, iev, eve, ve$>$} where the sub-word \emph{lie} is different from the word token \emph{$<$lie$>$}.

Therefore, the final target word embedding is the sum of its constituent character n-grams which are learned via the Skip-gram model.
This makes FastText very convenient for representing unknown words as the sum of \emph{static} constituent n-grams~\cite{jurafsky2000}.

Nevertheless, when it comes to domain-specific problems, general pre-trained word embeddings do not perform very well~\cite{rahimikia2021realised}.
demonstrate that even state-of-the-art embedding models like Google's Word2Vec(skip-gram)\footnote{\url{https://code.google.com/archive/p/word2vec/}} and Facebook's FastText(skip-gram)\footnote{\url{https://fasttext.cc/}}trained on 100 billion and 16 billion words, respectively, struggle to understand financial language like \begin{enumerate*}
    \item \emph{apple} standing for the company \emph{Apple},
    \item ticker analogies, e.g., \emph{amazon} is to \emph{X} as \emph{microsoft} is to \emph{msft},
    \item grouping company name to ticker, exchange and country
\end{enumerate*}.
In the same paper, the authors propose using the same algorithms (i.e., CBOW, Skip-gram, and FastText) but training solely on financial data instead - 15 years of financial news from the Dow Jones Newswires Text News Feed database, to produce the FinText models.
They report a substantial increase in performance in and sensitivity to detecting financial jargon and relationships.
In our project we acknowledge that a purpose-built financial word embedding (trained on proprietary data) will be more beneficial and more suited for the task of text summarisation of annual financial reports, which is why we select FinText as our preferred model.

\section{Recurrent Neural Networks (RNNs)}\label{sec:rnn}
The vanilla RNN is a basic type of RNN architecture designed for processing sequential data.
It learns temporal patterns from the initial data by looping over the hidden layers which allow information to persist (i.e., they serve as a network memory)~\cite{olah2015understandingLSTM}.
The key component is the recurrent hidden state $h_i$ (Figure~\ref{fig:rnn}) updated at each time step using input data and the previous hidden state (Eq.\ref{eq:rnn_hidden}).
This allows the RNN to capture contextual information and temporal dependencies in the sequence.
However, due to the inherent vanishing and exploding gradient problems with the vanilla RNNs, they have limited ability to learn long-term dependencies~\cite{bengio1994learning}.
To resolve these issues more advanced RNN architectures like LSTMs and GRUs have been developed.

\begin{equation}\label{eq:rnn_hidden}
    h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
    \footnote{
        The algebraic formulation of the vanilla RNN has the following variables: \begin{enumerate*}
            \item $x_t$ is the input at time step $t$.
            \item $h_{t-1}$ is the hidden state at time step $t-1$.
            \item $h_t$ is the hidden state at time step $t$.
            \item $y_t$ is the output at time step $t$.
            \item $f$ and $g$ are activation functions for the hidden and output layers, respectively.
            \item $W_{hh}$, $W_{xh}$, $W_{hy}$ are weight matrices for the hidden-to-hidden, input-to-hidden, and hidden-to-output connections, respectively.
            \item $b_h$ and $b_y$ are bias terms for the hidden and output layers, respectively.
        \end{enumerate*}
    }
\end{equation}
\begin{equation}
    y_t = g(W_{hy} h_t + b_y)\label{eq:rnn_output}
\end{equation}

The Long Short-Term Memory (LSTM) recurrent neural network has become a ubiquitous method in sequential problems (e.g., language modelling, time series forecasting).
This is so because it allows long-term dependencies to propagate through the network with the help of the control gates \- \emph{input} and \emph{forget}, which reduce the effect of the vanishing gradient issue in the vanilla RNN\footnote{
    We direct readers to~\cite{bayer2015learning} where the authors demonstrate that the LSTM's \enquote{temporal} gradient is unaffected by the fixed weight factor $W$ of the vanilla RNN that is driving the derivative to zero.
    This is ensured by the additional architecture unit - the \emph{forget} gate, which learns to  control the gradient flow in the network.
}.

A more simple variant of the LSTM is the Gated Recurrent Unit (GRU) which combines the \emph{input} and \emph{forget} gates into an \emph{update} gate for a model with fewer parameters and faster training~\cite{cahuantzi2021gru}.
Nevertheless, due to the sequential nature of the LSTM, the training process cannot be parallelised across GPUs, i.e., the learning cannot be made quicker by more computational resources.

As noted by~\cite{graveshinton2013speech}, a limitation of the unidirectional RNNs is that they only make use of previous context in the sequence.
To alleviate this and establish more complex relationships between words~\cite{schuster1997birnn}, propose a bi-directional architecture consisting of a forward and a backward RNN\@.

We can therefore represent the hidden layers ($h_i$) per time-step (where $T$ is the sequence size) with the following notation: \begin{enumerate*}
    \item $(\overset{\longrightarrow}{h_1}, \ldots, \overset{\longrightarrow}{h_T})$ are the forward hidden states from left to right (i.e., $x_1,\ldots,x_T$) and
    \item $(\overset{\longleftarrow}{h_1}, \ldots, \overset{\longleftarrow}{h_T})$ are the backward hidden states from right to left (i.e., $x_T,\ldots,x_1$)
\end{enumerate*} (Figure~\ref{fig:bilstm}).

Then, for a single word $x_i$, its respective \emph{annotation} (i.e., condensed representation) is constructed by the concatenation of the forward and backward hidden states - $h_i = [\overset{\longrightarrow}{h_i^T}; \overset{\longleftarrow}{h_i^T}]^T$ as specified in~\cite{bahdanau2016neural}.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/rnn}
        \caption{Unfolded RNN}
        \label{fig:rnn}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/bi-lstm}
        \caption{Unfolded bi-directional LSTM}
        \label{fig:bilstm}
    \end{subfigure}~\caption{Unfolded recurrent architectures, taken from~\cite{zhiyong2018bilstm}}
    \label{fig:recurrent_unfolded}
\end{figure}

\section{Encoder-Decoder and Attention}\label{sec:seq2seq}
However, to deal with many-to-many sequence-to-sequence problems (e.g., machine translation, speech recognition, abstractive text summarisation) a new type of neural architecture is necessary~\cite{sutskever2014sequence}.
and~\cite{cho-etal-2014-learning} introduced the Encoder-Decoder network (Figure~\ref{fig:encoderdecoder}) with \begin{enumerate*}
    \item the encoder being an RNN that maps an input sequence $(x_1, x_2, \dots, x_n)$ to a continuous fixed-length context vector $c$ and
    \item the decoder, also an RNN, taking this vector and producing an output sequence $(y_1, y_2, \dots, y_m)$
\end{enumerate*}.
They train the two RNNs jointly, maximising the conditional probability of the target sequence given a source sequence, i.e., $p(y_1, \ldots, y_{T'} | x_1, \ldots, x_T)$.
For the selection of the neural model the authors had naturally chosen LSTM and GRU, respectively, due to the resolution of the vanishing gradient problem (as discussed in Section~\ref{sec:rnn}) and for the time being~\cite{sutskever2014sequence} managed to achieve state-of-the-art results in machine translation.


% Encoder-decoder schema
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=1.5cm and 1cm, >=stealth,auto,font=\footnotesize]
        % Encoder
        \node[draw,minimum height=1cm] (x1) {$x_1$};
        \node[draw,minimum height=1cm, right=of x1] (x2) {$x_2$};
        \node[right=of x2] (dots) {$\dots$};
        \node[draw,minimum height=1cm, right=of dots] (xT) {$x_T$};
        \node[draw,minimum height=1cm, circle, above=of x1] (h1) {$h_1$};
        \node[draw,minimum height=1cm, circle, above=of x2] (h2) {$h_2$};
        \node[draw,minimum height=1cm, circle, above=of xT] (hT) {$h_T$};

        \draw[->] (x1) -- (h1);
        \draw[->] (x2) -- (h2);
        \draw[->] (xT) -- (hT);
        \draw[->] (h1) -- (h2);
        \draw[->] (h2) -- (hT);

        % Context vector
        \node[draw,minimum height=1cm, circle, right=of hT] (c) {c};

        \draw[->] (hT) -- (c);

        % Decoder
        \node[draw,minimum height=1cm, circle, right=of c, yshift=1.5cm] (s1) {$s_1$};
        \node[draw,minimum height=1cm, circle, right=of s1] (s2) {$s_2$};
        \node[right=of s2, yshift=0.25cm] (sdots) {$\dots$};
        \node[draw,minimum height=1cm, circle, right=of sdots, yshift=-0.25cm] (sT) {$s_{T'}$};
        \node[draw,minimum height=1cm, above=of s1] (y1) {$y_1$};
        \node[draw,minimum height=1cm, above=of s2] (y2) {$y_2$};
        \node[draw,minimum height=1cm, above=of sT] (yT) {$y_{T'}$};

        \draw[->, dotted] (c) -- (y1);
        \draw[->, dotted] (c) -- (y2);
        \draw[->, dotted] (c) -- (yT);
        \draw[->, dotted] (c) -- (s1);
        \draw[->, dotted] (c) -- (s2);
        \draw[->, dotted] (c) -- (sT);

        \draw[->] (y1) -- (y2);
        \draw[->] (y2) -- (yT);

        \draw[->] (y1) -- (s2);
        \draw[->] (y2) -- (sT);

        \draw[->] (s1) -- (s2);
        \draw[->] (s2) -- (sT);

        \draw[->] (s1) -- (y1);
        \draw[->] (s2) -- (y2);
        \draw[->] (sT) -- (yT);
    \end{tikzpicture}
    \caption{Encoder-Decoder Schema}
    \label{fig:encoderdecoder}
\end{figure}


It is in~\cite{bahdanau2016neural} where the authors suppose that the fixed-length vector results in a bottleneck such that the longer the sequences, the worse the compression performance is for neural networks.
Meanwhile, they propose to replace the fixed-length vector with a variable-length one which searches for the most relevant information from the source sequence.
This is achieved through an alignment model which accepts as input the produced annotations from the encoder $h_1,\ldots, h_T$ (see Section~\ref{sec:rnn} for details on how these are generated).
Then the context vector $c_{i}$ which is now distinct for each input word (unlike in~\cite{cho-etal-2014-learning,sutskever2014sequence}) becomes a weighted sum\footnote{
    And hence it is known as the \emph{additive attention}.
} of the annotations $h_{j}$ (Eq.\ref{eq:context}) and each weight $\alpha_{ij}$ is calculated as the normalized attention score (Eq.\ref{eq:attention_score}).
The alignment model is itself a neural network trained jointly with the encoder-decoder system and it evaluates the importance $e_{ij}$ of an annotation $h_{j}$ in generating a new state $s_{i}$ and output token $y_{i}$ (Eq.\ref{eq:energy}).
Intuitively, as noted in~\cite{galassi2020attention}, this attention mechanism computes a weight distribution on the input sequence and \emph{attends} (i.e., assigns a larger weight) to the most relevant parts.

\begin{equation}\label{eq:context}
    c_i = \sum_j^T{\alpha_{ij}h_j}
\end{equation}
\begin{equation}\label{eq:attention_score}
    \alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T} \exp(e_{ik})}
\end{equation}
\begin{equation}\label{eq:energy}
    e_{ij} = a(s_{i-1}, h_j)
\end{equation}

\section{Transformers}\label{sec:transformers}

The Transformer~\cite{vaswani2017attention} is another sequence-to-sequence architecture which follows the overall encoder-decoder architecture~\cite{sutskever2014sequence}, but differs in the following aspects:
\begin{enumerate}
    \item \emph{Internal architecture} - instead of RNNs~\cite{cho-etal-2014-learning}, the encoder and the decoder have multiple identical transformer blocks based on fully-connected feed-forward neural networks with a newly introduced concept of self-attention (Figure~\ref{fig:transformer}).
    Although, prior to that, recurrent hidden layers have been used for compressing sequences into a context vector, the authors of the transformer extend the attention from~\cite{bahdanau2016neural} to generate salient context-aware representations of the input sequence.
    \item \emph{Self-Attention and Global context} - while RNN architectures struggle to deal with long-range dependencies (due to the vanishing gradient problem and the last-layer bottleneck~\cite{bahdanau2016neural}), the authors propose
    \emph{causal}\footnote{
        Causal attention is a special case of self-attention where the attention weights are computed only from the past tokens.
        This is achieved by masking the future tokens in the attention score computation (Figure~\ref{fig:causal_grid}).
        We direct readers to~\cite{jurafsky2000} for a detailed discussion on Transformer language modelling.
    } self-attention to allow capturing of global context.
    This mechanism has two key components (Fig.\ref{fig:attention_dot_product}):
    \begin{itemize}
        \item \emph{The scaled dot-product} (Eq.\ref{eq:attention}) computes the attention weights (see Section~\ref{sec:seq2seq}) to be used for generating a weighted representation of the input sequence.
        Each token from the input sequence is represented as a $q$-query, $k$-key, and $v$-value vector (packed together into linearly-projected\footnote{
            The query vectors, key vectors, and value vectors are linearly projected from the input token embeddings using separate learnable weight matrices.
        } $Q$, $K$, $V$ matrices, respectively).
        Meanwhile, the dot product computes the similarity score between $Q$ - the current token's focus and $K$ - the context of the other tokens, whereas the scaling factor $\sqrt{d_{k}}$ prevents against extreme differences in softmax calculation - leading to slow convergence.
        Once the attention weights have been calculated, they are multiplied with $V$ for a new context-aware representation combining information from other tokens in the sequence.
        This mechanism allows the Transformer to learn relationships between any two tokens and hence the significant computational load $O(n^2)$.
        Nevertheless, since these computations can be performed independently for each token, the entire input sequence can be processed simultaneously unlike in RNN architectures.

        \begin{equation}\label{eq:attention}
            \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
        \end{equation}

        \item \emph{The multi-head attention}~\cite{vaswani2017attention} is a mechanism that allows the Transformer to learn multiple representations of the input sequence.
        This is achieved by stacking multiple attention heads (each with its own $Q$, $K$, $V$ matrices) and concatenating their outputs (Figure~\ref{fig:attention_dot_product}).
        For each head $h_i$, the $Q$, $K$, and $V$ vectors will be linearly projected with different weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$, respectively.
        The intuition is that each attention head learns different aspects of the relationships that exist among inputs (e.g., syntactic, semantic, and discourse relationships~\cite{jurafsky2000}) and the concatenation of the different representations allows the Transformer to learn a richer overall representation of the input sequence.

    \end{itemize}
    \item \emph{Positional Encoding} - the Transformer does not have a recurrent hidden layer to capture the sequential nature of the input sequence.
    Instead, the authors~\cite{vaswani2017attention} propose adding a positional encoding to the input embeddings as a way to capture positional information.
    \item \emph{Training Details} - Because of the non-sequential nature of the Transformer, the heavy self-attention computations can be parallelised on modern hardware - GPUs and TPUs which makes this model easy during training and inference time.
    Additionally, the proposed dot-product attention is practically much faster and more space-efficient in comparison to the additive one~\cite{bahdanau2016neural}, due to highly optimized matrix multiplication libraries.
    Nevertheless, at least two computational drawbacks are that Transformers require an additional positional embedding and also have a quadratic complexity when calculating the self-attention which is extremely expensive for long sequences~\cite{jurafsky2000}.
\end{enumerate}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\columnwidth]{../charts/attention}~\caption{Scaled dot-product and multi-head attention, taken from~\cite{vaswani2017attention}}
    \label{fig:attention_dot_product}
\end{figure}

% Transformer Architecture
\begin{figure}[h]
    \centering
    \begin{tikzpicture}[
        block/.style={rectangle, draw, anchor=center, minimum width=3cm, minimum height=1cm},
        attention/.style={fill=orange!30},
        ffn/.style={fill=green!30},
        inputlayer/.style={fill=blue!30},
        outputlayer/.style={fill=red!30},
        >=latex
    ]
        % Encoder
        \node[block, inputlayer] (encoderInput) {Input};
        \node[block, attention, below=of encoderInput] (encoderMultihead) {Multi-head Attention};
        \node[block, ffn, below=of encoderMultihead] (encoderFfn) {Feed Forward Network};
        \node[fit=(encoderMultihead)(encoderFfn), draw, dashed, inner sep=8pt] (encoder) {};
        \node[anchor=south] at (encoder.north) {Encoder};
        \node[anchor=east] at (encoder.west) {N$\times$};

        % Decoder
        \node[block, inputlayer, right=4cm of encoderInput] (decoderInput) {Target Input};
        \node[block, attention, below=of decoderInput] (decoderMultihead1) {Multi-head Attention};
        \node[block, attention, below=of decoderMultihead1] (decoderMultihead2) {Multi-head Attention};
        \node[block, ffn, below=of decoderMultihead2] (decoderFfn) {Feed Forward Network};
        \node[block, outputlayer, below=of decoderFfn] (decoderOutput) {Output};
        \node[fit=(decoderMultihead1)(decoderMultihead2)(decoderFfn), draw, dashed, inner sep=8pt] (decoder) {};
        \node[anchor=south] at (decoder.north) {Decoder};
        \node[anchor=west] at (decoder.east) {$\times$N};

        % Arrows
        \draw[->] (encoderInput) -- (encoderMultihead);
        \draw[->] (encoderMultihead) -- (encoderFfn);
        \draw[->] (decoderInput) -- (decoderMultihead1);
        \draw[->] (decoderMultihead1) -- (decoderMultihead2);
        \draw[->] (decoderMultihead2) -- (decoderFfn);
        \draw[->] (decoderFfn) -- (decoderOutput);
        \draw[->] (encoderFfn) -- node[above] {Context Vector} (decoderMultihead2);
    \end{tikzpicture}\caption{Simplified Transformer encoder-decoder architecture}
    \label{fig:transformer}
\end{figure}

\section{BERT}\label{sec:bert}
BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based language model that was proposed by Google AI in 2018~\cite{devlin-etal-2019-bert}.
It revolutionised the field of NLP by introducing a new pre-training paradigm that outperformed previous state-of-the-art models on a wide range of NLP tasks while being easily applicable to autoregressive generation problems (e.g., abstractive summarization and machine translation).
We will discuss the main components of BERT and how it differs from the original Transformer model.
\begin{enumerate}
    \item \emph{Bidirectional Nature} - ~\cite{jurafsky2000} argue that the uni-directional nature of the Transformer (i.e., causal self-attention) is a drawback that
    prevents it from capturing the full context of a sentence, especially when applied to sequence classification and labelling tasks.
    BERT addresses this issue by using a bidirectional Transformer architecture that allows the model to capture the full context of a sentence.
    This contextualisation is achieved by allowing the self-attention mechanism to range over the entire input as visible from the query-key comparisons in Figure~\ref{fig:bert_grid}.
    %Attention grid
    \begin{figure}[h]
        \centering
        \begin{subfigure}{0.45\textwidth}
            \centering
            \begin{tikzpicture}[cell/.style={rectangle, draw=black, minimum size=0.6cm}]
                % Transformer subfigure
                \matrix (transformer) [matrix of nodes, nodes={cell}]{
                    $q_1 k_1$ & |[fill=gray!30]| $-\infty$ & |[fill=gray!30]| $-\infty$ & |[fill=gray!30]| $-\infty$ \\
                    $q_2 k_1$ & $q_2 k_2$ & |[fill=gray!30]| $-\infty$ & |[fill=gray!30]| $-\infty$ \\
                    $q_3 k_1$ & $q_3 k_2$ & $q_3 k_3$ & |[fill=gray!30]| $-\infty$ \\
                    $q_4 k_1$ & $q_4 k_2$ & $q_4 k_3$ & $q_4 k_4$ \\
                };
            \end{tikzpicture}
            \caption{Transformer}
            \label{fig:causal_grid}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.45\textwidth}
            \centering
            \begin{tikzpicture}[cell/.style={rectangle, draw=black, minimum size=0.6cm}]
                % BERT subfigure
                \matrix (bert) [matrix of nodes, nodes={cell}]{
                    $q_1 k_1$ & $q_1 k_2$ & $q_1 k_3$ & $q_1 k_4$ \\
                    $q_2 k_1$ & $q_2 k_2$ & $q_2 k_3$ & $q_2 k_4$ \\
                    $q_3 k_1$ & $q_3 k_2$ & $q_3 k_3$ & $q_3 k_4$ \\
                    $q_4 k_1$ & $q_4 k_2$ & $q_4 k_3$ & $q_4 k_4$ \\
                };
            \end{tikzpicture}
            \caption{BERT}
            \label{fig:bert_grid}
        \end{subfigure}
        \caption{Comparison of query-key dot product representations for Transformer and BERT models.}
        \label{fig:grid_comparison}
    \end{figure}
    \item \emph{Pre-training} - BERT is a model pre-trained on two tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP).
    \begin{enumerate}
        \item \emph{Masked Language Model} - BERT uses a pre-training task called Masked Language Model (MLM) to learn the representations of words in a sentence.
        The authors~\cite{devlin-etal-2019-bert} propose masking 15\% of the input tokens at random and then training the model to predict the masked tokens.
        To make it more accessible for fine-tuning where the [MASK] token is not available, they replace it with a random or with the original token with probabilities of 10\% each.
        \item \emph{Next Sentence Prediction} - BERT is also pre-trained on Next Sentence Prediction (NSP) to learn the representations of sentences in a document.
        To understand the relationship between two sentences, BERT uses a binary classification task where the model is trained to predict whether a sentence $B$ is following another sentence $A$.
        The authors~\cite{devlin-etal-2019-bert} capture the sentence structure with the help of two new tokens - [CLS] (added before the sentence pair) and [SEP] (inserted in-between sentences), which are essential for the fine-tuning process (Figure~\ref{fig:bert_input}).
        They further propose using 50\% of the training data as positive examples (i.e., $A$ and $B$ are consecutive sentences) and 50\% as negative examples (i.e., $B$ is a random sentence from the corpus).
    \end{enumerate}
    \item \emph{Fine-tuning for Sequence Classification} - Unlike in Word2Vec~\cite{mikolov2013efficient} where the word embeddings are \emph{static} (i.e., related to the single word token only), BERT learns \emph{contextualised word embeddings} which can produce different representations for the same word depending on the context around it.
    Here, instead of using the output of the last hidden layer (as we do with RNNs and the Transformer), the authors~\cite{devlin-etal-2019-bert} propose a \emph{sentence embedding} $y_{CLS}$ that summarizes the entire sequence of hidden states - the output vector of the model for the [CLS] token.
    The reasoning is that BERT is pre-trained with the [CLS] token being prepended to the input sequence (during the NSP task) and it can be used as an aggregate representation of the entire sequence.
    Therefore, fine-tuning for text classification amounts to learning the probability distribution over the possible labels (e.g., 0/1 for a binary task) from the linearly-projected $y_{CLS}$, i.e. $\text{softmax}(y_{CLS}W_{C} + b)$, complying with the typical supervised learning paradigm.
    A slight difference, as~\cite{jurafsky2000} suggest, is that the backpropagation can affect not only the classifier but also the pretrained language model (resulting in minimal changes in practice).
\end{enumerate}

% BERT Embeddings
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[
  input/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=2.8em, draw, fill=red!20},
  token/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=2.8em, draw, fill=blue!20},
  seg_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=2.8em, draw, fill=orange!20},
  pos_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=2.8em, draw, fill=green!20},
  input_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=2.8em, draw, fill=purple!20},
  plus/.style={}
]

    % Tokens
    \node[input] (i1) {[CLS]};
    \node[input, right=0.3cm of i1] (i2) {It};
    \node[input, right=0.3cm of i2] (i3) {is};
    \node[input, right=0.3cm of i3] (i4) {sunny};
    \node[input, right=0.3cm of i4] (i5) {[SEP]};
    \node[input, right=0.3cm of i5] (i6) {But};
    \node[input, right=0.3cm of i6] (i7) {also};
    \node[input, right=0.3cm of i7] (i8) {cold};
    \node[input, right=0.3cm of i8] (i9) {[SEP]};

        % Tokens
    \node[token, below=1cm of i1] (t1) {$E_{[CLS]}$};
    \node[token, below=1cm of i2] (t2) {$E_{It}$};
    \node[token, below=1cm of i3] (t3) {$E_{is}$};
    \node[token, below=1cm of i4] (t4) {$E_{sunny}$};
    \node[token, below=1cm of i5] (t5) {$E_{[SEP]}$};
    \node[token, below=1cm of i6] (t6) {$E_{But}$};
    \node[token, below=1cm of i7] (t7) {$E_{also}$};
    \node[token, below=1cm of i8] (t8) {$E_{cold}$};
    \node[token, below=1cm of i9] (t9) {$E_{[SEP]}$};

    % Add plus nodes between inputs
    \draw ($(i1.south west) + (0,-0.3)$) -- ($(i9.south east) + (0,-0.3)$);

        % Segmentation embeddings
    \node[seg_embed, below=1cm of t1] (s1) {$E_{A}$};
    \node[seg_embed, below=1cm of t2] (s2) {$E_{A}$};
    \node[seg_embed, below=1cm of t3] (s3) {$E_{A}$};
    \node[seg_embed, below=1cm of t4] (s4) {$E_{A}$};
    \node[seg_embed, below=1cm of t5] (s5) {$E_{A}$};
    \node[seg_embed, below=1cm of t6] (s6) {$E_{B}$};
    \node[seg_embed, below=1cm of t7] (s7) {$E_{B}$};
    \node[seg_embed, below=1cm of t8] (s8) {$E_{B}$};
    \node[seg_embed, below=1cm of t9] (s9) {$E_{B}$};

    % Position embeddings
    \node[pos_embed, below=1cm of s1] (p1) {$E_{0}$};
    \node[pos_embed, below=1cm of s2] (p2) {$E_{1}$};
    \node[pos_embed, below=1cm of s3] (p3) {$E_{2}$};
    \node[pos_embed, below=1cm of s4] (p4) {$E_{3}$};
    \node[pos_embed, below=1cm of s5] (p5) {$E_{4}$};
    \node[pos_embed, below=1cm of s6] (p6) {$E_{5}$};
    \node[pos_embed, below=1cm of s7] (p7) {$E_{6}$};
    \node[pos_embed, below=1cm of s8] (p8) {$E_{7}$};
    \node[pos_embed, below=1cm of s9] (p9) {$E_{8}$};

    % Pluses at 0.25 between each (s, t) pair
    \node[above=0.25cm of $(s1)!.25!(t1)$, font=\large] {$+$};
    \node[above=0.25cm of $(s2)!.25!(t2)$, font=\large] {$+$};
    \node[above=0.25cm of $(s3)!.25!(t3)$, font=\large] {$+$};
    \node[above=0.25cm of $(s4)!.25!(t4)$, font=\large] {$+$};
    \node[above=0.25cm of $(s5)!.25!(t5)$, font=\large] {$+$};
    \node[above=0.25cm of $(s6)!.25!(t6)$, font=\large] {$+$};
    \node[above=0.25cm of $(s7)!.25!(t7)$, font=\large] {$+$};
    \node[above=0.25cm of $(s8)!.25!(t8)$, font=\large] {$+$};
    \node[above=0.25cm of $(s9)!.25!(t9)$, font=\large] {$+$};


    % Pluses at 0.25 between each (p, s) pair
    \node[above=0.25cm of $(p1)!.25!(s1)$, font=\large] {$+$};
    \node[above=0.25cm of $(p2)!.25!(s2)$, font=\large] {$+$};
    \node[above=0.25cm of $(p3)!.25!(s3)$, font=\large] {$+$};
    \node[above=0.25cm of $(p4)!.25!(s4)$, font=\large] {$+$};
    \node[above=0.25cm of $(p5)!.25!(s5)$, font=\large] {$+$};
    \node[above=0.25cm of $(p6)!.25!(s6)$, font=\large] {$+$};
    \node[above=0.25cm of $(p7)!.25!(s7)$, font=\large] {$+$};
    \node[above=0.25cm of $(p8)!.25!(s8)$, font=\large] {$+$};
    \node[above=0.25cm of $(p9)!.25!(s9)$, font=\large] {$+$};

    % Details
    \node[left=0.3cm of i1] (d1) {Input};
    \node[left=0.3cm of t1] (d2) {Token Emb.};
    \node[left=0.3cm of s1] (d3) {Segment Emb.};
    \node[left=0.3cm of p1] (d4) {Position Emb.};

\end{tikzpicture}
    \caption{BERT: Input Embeddings}
    \label{fig:bert_input}
\end{figure}

\section{FinBERT}\label{sec:finbert}
FinBERT\footnote{\url{https://github.com/yya518/FinBERT}}~\cite{yang2020finbert} is a pre-trained domain-specific language model based on
BERT~\cite{devlin-etal-2019-bert} (Section~\ref{sec:bert}) that is trained on a total of $4.9B$ tokens from financial corpora:
\begin{enumerate*}
    \item Corporate Reports 10-K \& 10-Q (introduced in Section~\ref{sec:financial_reports}),
    \item Earnings Call Transcripts (discussing financial performance, business updates, and future expectations), and
    \item Analyst Reports (providing an in-depth textual analysis of the company and an earnings forecast)
\end{enumerate*}.
This makes FinBERT a natural choice for our experiments, as the language of the training data is very similar to the one of the UK annual reports (Section~\ref{sec:uk-annual-reports}).
Furthermore, the authors demonstrate that FinBERT outperforms BERT on three financial sentiment classification tasks, which is why we select to fine-tune it on our extractive summarisation task.

\section{Text Summarisation}\label{sec:text-summarisation}
Text summarisation is the task of transforming a piece of text into a shorter  version that retains the most important information.
There are two overarching categories: extractive and abstractive text summarisation.
The former formulates the problem as a subset selection problem by returning only the most salient text excerpts from the original document~\cite{zhong-etal-2020-extractive}, while the latter aims to generate content anew, similar to how humans would do.

We will outline some key models from the FNS21 (the previous year competition) that inspired our work below:
\begin{itemize}
    \item \textbf{Extractive SentenceBERT~\cite{gokhan-etal-2021-extractive})}: The authors employ an unsupervised summariser based on K-Means clustering of sentences encoded with SentenceBERT~\cite{reimers2019sentence}.
    However, their embeddings are pre-trained on general text, and they suggest that employing in-domain language models would result in a better performance.

    \item \textbf{AMUSE}~\cite{litvak-vanetik-2021-summarization}: The authors design an ETS system comprised of the following steps \begin{enumerate*}
        \item shortening of report with an existing Genetic Algorithm~\cite{litvak-last-2013-multilingual},
        \item encoding sentences with BERT vectors, and
        \item performing binary classification with LSTMs for salient sentence extraction
    \end{enumerate*}.
    They suggest that further work should incorporate \begin{enumerate*}[label=(\alph*)]
        \item efficient preliminary sentence removal, and
        \item additional neural modelling stages for the representation and detection of relevant input text parts.
    \end{enumerate*}

    \item \textbf{Hybrid model with RL}~\cite{zmandar-etal-2021-joint}: The authors train a joint extractive-abstractive summarisation model with reinforcement learning optimised for the ROUGE-2 F1 metric.
    Their networks are based on attentive LSTMs augmented with an additional copy mechanism~\cite{vinyals2015pointer} achieving the second highest F1 score in the FNS21 competition.

    \item \textbf{T5-LONG-EXTRACT}~\cite{orzhenovskii-2021-t5}: The author used T5~\cite{rayson2019t5} for an extractive model fine--tuned to generate the beginning of an abstractive summary and find the closest match of the output in the report's full text.
    This is the best performing algorithm in the FNS21 competition but also the first to consider transformer models from an extractive-abstractive perspective in the FNP workshops so far.
    It serves as an inspiration for the best English FNS22 model~\cite{el-haj-etal-2022-financial} and other contestants~\cite{khanna-etal-2022-transformer, foroutan-etal-2022-multilingual}.
\end{itemize}

Additionally, we briefly describe the FNS22 competitors' approaches, against which we perform our evaluation in Section~\ref{sec:quantitative-evaluation}:
\begin{itemize}
    \item \textbf{Team LIPI's T5~\cite{pant-chopra-2022-multilingual}}: This model is entirely based on the FNS21's T5-LONG-EXTRACT
    approach~\cite{orzhenovskii-2021-t5} and achieves the best ROUGE-2 $F1$ score of $0.374$ on the English testing set.
    Unfortunately, results on the validation set are not made available.

    \item \textbf{Top-K Narrative Extractor~\cite{shukla-etal-2022-dimsum}}: The authors implement a narrative
    classification method recognising unique report sections, their titles (see Section~\ref{sec:uk-annual-reports}), and the Table of Content.
    They further propose an algorithm for word allocation to assemble various sentences from different sections into a single summary.
    To fit in the $1,000$ word limit, they use a Top-k summarizer, which extracts the first $k$ words from a given text.
    Their system achieves a ROUGE-2 $F1$ score of $0.425$ on the English validation set, while ranking in the top three
    overall for Spanish and Greek report summarisation.

    \item \textbf{Longformer-Encoder-Decoder (LED)~\cite{khanna-etal-2022-transformer}}: The authors use a general
    transformer-based encoder-decoder model building on the LongFormer~\cite{Beltagy2020LongformerTL} for an efficient calculation of long-range attention.
    They fine-tune the model on the FNS21 dataset to generate in an abstractive manner the start of summarising paragraphs, which are then extracted from the original text similaraly to~\cite{orzhenovskii-2021-t5}.
    While their LED model performs well in recognising the beginning of the summary, it struggles with identifying the end,
    and hence their summarisation suffers from a lower ROUGE-2 score of $0.302$ on the English validation set.

    \item \textbf{Team LSIR's mT5}~\cite{foroutan-etal-2022-multilingual}: The authors propose the usage of a multilingual
    pre-trained text-to-text transformer~\cite{raffel2020exploring} to generate the start of a report's summary in the same way
    as~\cite{orzhenovskii-2021-t5, khanna-etal-2022-transformer}.
    This system is ranked best overall for the three languages, while achieving a ROUGE-2 $F1$ score of $0.365$ on the English validation set.
    The authors also explore unsupervised methods based on the multilingual BERT model~\cite{devlin-etal-2019-bert} generating
    representations for blocks (i.e., spans) of 64 tokens for each summary and report.
    They further cluster the summary blocks with k-means and compute the cosine similarity between them and the report blocks.
    While this unsupervised method does not perform best for English, it proves to be very effective for Spanish and Greek, where
    the number of reports is significantly smaller than the one in English.
\end{itemize}

In this work we will be solely exploring the extractive summarisation method, and more specifically - the \emph{supervised neural-based} (i.e., RNN, Transformer) type and the \emph{unsupervised graph-based} (i.e., TextRank, LexRank) type.

\section{TextRank}\label{sec:textrank}
TextRank~\cite{mihalcea-tarau-2004-textrank} is a graph-based unsupervised algorithm for extractive summarisation with the following key components:
\begin{enumerate}
    \item \textbf{Sentence similarity}: TextRank computes the similarity between two sentences based on either the \emph{cosine similarity}
        \footnote{
            Cosine similarity is defined as the dot product of two vectors divided by the product of their norms.
        } (Eq.\ref{eq:cosine_similarity}) of their vectors (e.g., bag-of-words, Word2Vec, FastText, etc.) or the \emph{Jaccard similarity}
        \footnote{
            Jaccard similarity is defined as the size of the intersection divided by the size of the union of two sets.
        } of their sets of words (Eq.\ref{eq:jaccard_similarity} and Example~\ref{ex:jaccard}),
        \begin{equation}\label{eq:cosine_similarity}
            \cos(\theta) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
        \end{equation}
        \begin{equation}\label{eq:jaccard_similarity}
            J(A,B)= \frac{|A\cup B|}{|A\cap B|}
        \end{equation}
        \begin{example}\label{ex:jaccard}
            If sentence A is ``I love apples'' and sentence B is ``I love oranges'', then
            \[
                J(A,B)=\frac{2}{4}=\frac{1}{2},
            \]
            which indicates that $A$ and $B$ share 50\% of their unique words.
        \end{example}
    \item \textbf{Graph Construction}: TextRank represents sentences as nodes in a graph, and the similarity between each two sentences is represented as an edge between them.
    \item \textbf{Sentence ranking}: The constructed graph is passed into the PageRank algorithm~\cite{page1998anatomy} that assigns each sentence a score based on the importance of its neighbours.
    The final summary is then assembled from the selection of the top $k$ sentences with the highest scores.
\end{enumerate}
Nevertheless, ~\cite{Shearing2020AutomatedTS} report two considerable weaknesses of TextRank (hence the need for a better baseline - LexRank):
\begin{enumerate}
    \item \textbf{Extraneous words} - TextRank does not penalise the extraneous words (i.e., words that do not add any essential information to the sentence) which can artificially increase the PageRank score (i.e., the importance) of a sentence.
    \item \textbf{Frequent words} - There is no weighting applied regarding the frequency (or rarity) of words in the sentence, which can lead to a bias towards sentences with more frequent words.
\end{enumerate}
As a countermeasure, \emph{stop words} (e.g, ``a'', ``the'', ``and'', etc) can be removed and \emph{tf-idf} can be integrated into the similarity metric for a more balanced scoring mechanism.
LexRank~\cite{Erkan2004LexRankGC} builds up and resolves the issues of this algorithm, and we will discuss it in the next section.


\section{LexRank}\label{sec:lexrank}
LexRank~\cite{Erkan2004LexRankGC} is another unsupervised extractive summarisation method consistently used as a baseline in the FNS challenges over the years.
It retrieves the most salient document sentences by computing their importance based on \emph{eigenvector centrality}.
To do that the algorithm creates a graph where each sentence represents a node and each edge is a weight between two nodes~\cite{Shearing2020AutomatedTS}.
The sentences are encoded as bag-of-words vectors of size $N$ - the vocabulary size, and the weight metric is a combination of tf-idf (Eq.\ref{eq:tfidf}) and cosine similarity - Eq.\ref{eq:cosinesimtfidf}.

\begin{equation}
    \text{tf\_idf\_cosine\_similarity}(s_1, s_2) = \frac{\sum_{t \in T} \text{tf-idf}(t, s_1, D) \cdot \text{tf-idf}(t, s_2, D)}{ \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_1, D)^2} \cdot \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_2, D)^2}}
    \label{eq:cosinesimtfidf}
\end{equation}

where $t$ is a term, $d$ is a document within a collection of documents/sentences $D$.

Also, $s_1$ and $s_2$ are two sentences and $T$ represents the set of all terms in both of them while $tf(t, d)$ denotes the term frequency of $t$ in $d$, and $idf(t, D)$ is the inverse document frequency of $t$ in the collection $D$.

The authors further propose finding the most important sentences by \begin{enumerate*}
    \item applying a threshold for the creation of edges with Eq.\ref{eq:cosinesimtfidf},
    \item building an adjacency matrix and normalizing it to produce \emph{transition probabilities},
    \item computing in an iterative fashion the \emph{eigenvector centrality} until convergence, and finally
    \item ranking sentences based on their \emph{lexical} PageRank~\cite{page1998anatomy} score similarly to TextRank~\cite{mihalcea-tarau-2004-textrank}.
\end{enumerate*}

\section{Evaluation Metrics}\label{sec:evaluation-metrics}
\subsection{Confusion Matrix}\label{subsec:confusion-matrix}
The confusion matrix is an essential tool to visualise and help assessing the performance of trained classifiers against
the true labels $y_{i}$.

For the problem of binary classification, it is a square matrix (Table~\ref{tab:confusion_matrix}) that displays the
following key elements:

\begin{itemize}
    \item True Positives (TP): Correct predictions of the positive class.
    \item True Negatives (TN): Correct predictions of the negative class.
    \item False Positives (FP): Incorrect predictions of the positive class (Type I error).
    \item False Negatives (FN): Incorrect predictions of the negative class (Type II error).
\end{itemize}

where for the problem of extractive text summarisation, the positive and negative classes correspond to \emph{summary}
and \emph{non-summary} sentences, respectively.
These matrix elements can be further combined into informative classification metrics:

\begin{itemize}
    \item Accuracy: Proportion of correctly classified instances out of the total instances.
    Formulated as $\frac{TP + TN}{TP + TN + FP + FN}$.
    \item Precision (or Positive Predictive Value): Proportion of true positive instances out of all instances predicted as positive.
    Formulated as $\frac{TP}{TP + FP}$.
    \item Recall (or Sensitivity): Proportion of true positive instances out of all actual positive instances.
    Formulated as $\frac{TP}{TP + FN}$.
    \item F1-score: Harmonic mean of precision and recall (i.e., the trade-off between the two).
    Formulated as $2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}$.
\end{itemize}



\begin{table}[ht]
    \centering
    \begin{tabular}{c c c}
        \toprule
        \multirow{2}{*}{} & \multicolumn{2}{c}{\textbf{Actual}} \\
        \cmidrule(lr){2-3}
        & \textbf{Positive} & \textbf{Negative} \\
        \midrule
        \textbf{Predicted Positive} & TP & FP \\
        \textbf{Predicted Negative} & FN & TN \\
        \bottomrule
    \end{tabular}
    \caption{Confusion Matrix}
    \label{tab:confusion_matrix}
\end{table}

\subsection{ROUGE}\label{subsec:rouge}
The Recall-Oriented Understudy for Gisting Evaluation (ROUGE)~\cite{lin2004rouge} is a popular recall-based evaluation metric for
summarisation tasks which measures the overlapping units (i.e., n-grams) of a predicted summary $c$ and a reference summary $c^{*}$.
Here we briefly describe some of the most common variants of ROUGE (especially in the context of the FNS Task):
\begin{enumerate}
    \item \textbf{ROUGE-N} measures the overlap of $n$-grams (sequences of $n$ words) between $c$ and $c^{*}$.
    For example, ROUGE-1 and ROUGE-2 which is the official FNS metric (see Example~\ref{ex:rouge}) correspond to the overlap measurement of unigrams and bigrams, respectively, calculated as:
    \begin{equation}\label{eq:rouge_formula}
        ROUGE-N = \frac{\sum_{S \in R} \sum_{n-gram \in S} count_{match}(n-gram)}{\sum_{S \in R} \sum_{n-gram \in S} count(n-gram)}
    \end{equation}
    \item \textbf{ROUGE-L} uses the longest common subsequence (LCS) to measure the similarity between $c$ and $c^{*}$.
    The LCS is the longest sequence of words that appears in both summaries \emph{in the same order}, but \emph{not necessarily consecutively}.
    Thus, ROUGE-L is less sensitive to word order compared to ROUGE-N and can capture longer-range dependencies.
\end{enumerate}

While ROUGE scores are explainable and easily implementable, they have certain limitations like
\begin{enumerate*}
    \item inability to capture semantic similarity~\cite{akter-etal-2022-revisiting},
    \item insensitivity to summary coherence~\cite{christensen-etal-2013-towards}, and
    \item lack of correlation with human evaluation~\cite{liu2010exploring}
\end{enumerate*}.
Despite these limitations, ROUGE remains a widely-used evaluation measure, particularly in extractive summarization and we use it in this work as well.

\begin{example}\label{ex:rouge}
    We briefly demonstrate how to compute ROUGE-2 for a sentence pair:\\
    $c$ = ``Fox can run'' and $c^{*}$ = ``Fox can walk''.\\
    Here, the bigrams of $c$ are $\{(\text{``Fox''}, \text{``can''}), (\text{``can''}, \text{``run''})\}$. \\
    Meanwhile, the bigrams of $c^{*}$ are $\{(\text{``Fox''}, \text{``can''}), (\text{``can''}, \text{``walk''})\}$.\\
    The intersection of the two sets is $\{(\text{``Fox''}, \text{``can''})\}$, which is the only bigram that appears in both $c$ and $c^{*}$.\\
    Therefore, $\text{ROUGE-2 recall} = \frac{\text{Overlapping bigrams}}{\text{Total bigrams in reference summary}} = \frac{1}{2}=0.5$.
\end{example}
