\section{Background}\label{sec:background}

\subsection{Supervised Learning}\label{subsec:supervised-learning}

\subsection{TFIDF}\label{subsec:tfidf}

\subsection{Word Embeddings}\label{subsec:word-embeddings}

\subsection{Attention}\label{subsec:attention}

\subsection{RNN}\label{subsec:rnn}

\subsection{Transformers}\label{subsec:transformers}

\subsection{Text Summarisation}\label{subsec:text-summarisation}
Text summarisation is the task of transforming a piece of text into a shorter  version that retains the most important information.
There are two overarching categories: extractive and abstractive text summarisation.
The former formulates the problem as a subset selection problem by returning only the most salient text excerpts from the original document (\cite{zhong-etal-2020-extractive}), while the latter aims to generate content anew, similar to how humans would do.
In this work we will be solely exploring the extractive method, and more specifically - the \emph{supervised neural-based} (i.e., RNN, Transformer) type and the \emph{unsupervised graph-based} (i.e., TextRank, LexRank) type.


\subsection{LexRank}\label{subsec:lexrank}
LexRank (\cite{Erkan2004LexRankGC}) is an unsupervised extractive summarisation method consistently used as a baseline in the FNS21 and previous challenges.
It retrieves the most salient document sentences by computing their importance based on \emph{eigenvector centrality}.
To do that the algorithm creates a graph where each sentence represents a node and each edge is a weight between two nodes (\cite{Shearing2020AutomatedTS}).
The sentences are encoded as bag-of-words vectors of size $N$ - the vocabulary size, and the weight metric is a combination of tf-idf (Eq.\ref{eq:idf},\ref{eq:tfidf}) and cosine similarity - Eq.\ref{eq:cosinesimtfidf}.

\begin{equation}
    \text{idf}(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|} \label{eq:idf}
\end{equation}

\begin{equation}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
    \label{eq:tfidf}
\end{equation}

\begin{equation}
    \text{tf\_idf\_cosine\_similarity}(s_1, s_2) = \frac{\sum_{t \in T} \text{tf-idf}(t, s_1, D) \cdot \text{tf-idf}(t, s_2, D)}{ \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_1, D)^2} \cdot \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_2, D)^2}}
    \label{eq:cosinesimtfidf}
\end{equation}

where $t$ is a term, $d$ is a document within a collection of documents/sentences $D$.

Also, $s_1$ and $s_2$ are two sentences and $T$ represents the set of all terms in both of them while $tf(t, d)$ denotes the term frequency of $t$ in $d$, and $idf(t, D)$ is the inverse document frequency of $t$ in the collection $D$.

The authors further propose finding the most important sentences by \begin{enumerate*}
    \item applying a threshold for the creation of edges with Eq.\ref{eq:cosinesimtfidf},
    \item building an adjacency matrix and normalizing it to produce \emph{transition probabilities},
    \item computing in an iterative fashion the \emph{eigenvector centrality} until convergence, and finally
    \item ranking sentences based on their \emph{lexical} PageRank (\cite{page1998anatomy}) score.
\end{enumerate*}