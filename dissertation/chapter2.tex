\section{Background}\label{sec:background}

\subsection{Supervised Learning}\label{subsec:supervised-learning}

\subsection{TFIDF}\label{subsec:tfidf}

\subsection{Word Embeddings}\label{subsec:word-embeddings}
Historically, to represent a token (i.e., word) $w_{i}$ in a vocabulary $V$ numerically, we define a one-hot-encoding vector of all zeroes except of a one at the index of the word $w_{i}$ in $V$ (i.e., $i$).

The results are sparse individual word vectors being orthogonal to each other which \begin{enumerate*}
    \item waste memory (each word is a $|V|$-sized vector, hence a total of $|V|^{2}$ for all tokens) and more importantly
    \item fail to encode semantic similarity due to their cosine similarity being always zero
\end{enumerate*}.


Traditionally, AF research has represented an input text with the help of bag-of-words (BOW) models which can be viewed from the  \begin{enumerate*}
    \item the binary perspective - represent a whole document $d$ as a binary vector containing ones for all words $w_{i}$ occurring in $d$ from $V$, 
    \item the term frequency perspective - encode number of word occurrences in documents instead of binary representation (\cite{Xu2013AnAT}), and 
    \item the tf-idf perspective - extend the latter to penalise ubiquitous terms (\cite{SprckJones1972ASI}).
\end{enumerate*} 
Nevertheless, these vectors are very sparse and unable to encode more complex contextual and semantic meaning.

To address these shortcomings \emph{short}\footnote{i.e., with a small number of dimensions} and \emph{dense}\footnote{i.e., continuous real-numbered values instead of 0/1s} word embeddings like Word2Vec (\cite{mikolov2013efficient}) and FastText (\cite{bojanowski-etal-2017-enriching}) have been developed.
In~\cite{mikolov2013efficient} the authors manage to condense the vector space and ensure that word representations have \emph{multiple degrees of similarity} (\cite{mikolov-etal-2013-linguistic}) (e.g., semantic - the meaning of words, morphological - structure of sub-words, etc).


Furthermore, the proposed models - CBOW (Continuous Bag of Words\footnote{
    CBOW naming is derived from \begin{enumerate*}
        \item the continuous distributed representation of the context and 
        \item the projection layer being shared across context words, i.e., the order of words does not affect the projection (similar to how bag-of-words model fails to encode word order).     
    \end{enumerate*} 
}) and Skip-gram evidently capture subtle semantic relationships and allow intuitive arithmetic operations as shown in the popular analogy: $\overrightarrow{\text{king}}\ - \overrightarrow{\text{man}} +\overrightarrow{\text{woman}}\ \approx \overrightarrow{\text{queen}}$\footnote{
    For a formal explanation on how analogies are realised in word embeddings we direct the readers to~\cite{ethayarajh-etal-2019-towards}
}.


The intuition for the two Word2Vec models is that in CBOW, the context (i.e., the surrounding tokens) is used to predict the middle token, while in skip-gram, the input token is used to predict the context (i.e., the surrounding tokens) (Figure~\ref{fig:cbow_skipgram}).

Meanwhile, internally, the context prediction is cast as a binary classification task with positive examples being the target word and its surroundings, whereas the negatives ones are generated through random sampling from the dictionary. 
Then, the CBOW embeddings are the learned weights of a logistic regression classifier with future and history words (i.e., the context window) as the input and the goal of correctly classifying the word in-between. 
In contrast, the Skip-gram uses the middle word as an input to the classifier and predicts the individual context words around it.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.75\textwidth]{../charts/cbow_skipgram}
    \caption{Word-from-context and context-from-word prediction in CBOW and Skip-gram, respectively. Figure is from~\cite{mikolov2013efficient}}.
    \label{fig:cbow_skipgram}
\end{figure}

A downside to the Word2Vec models is that they cannot handle out-of-vocabulary (OOV) word tokens, i.e., they cannot generate an embedding vector for words missing from the training data, which is crucial in real-life problems with \emph{noisy} input or morphologically rich languages.
For this reason~\cite{bojanowski-etal-2017-enriching}, propose FastText as an extension to the Skip-gram model that makes use of character-level information to deal with unknown tokens.
Here, each word is itself a bag of character n-grams which captures meaning of prefixes, suffixes, and morphemes. 
Additionally, two symbols are further introduced to mark the beginning and the end of a token, and help differentiate between sub-words and short words.
For example, the character trigrams of the word \emph{believe} are \emph{$<$be, bel, eli, lie, iev, eve, ve$>$} where the sub-word \emph{lie} is different from the word token \emph{$<$lie$>$}.

Therefore, the final target word embedding is the sum of its constituent character n-grams which are learned via the Skip-gram model.
This makes FastText very convenient for representing unknown words as the sum of \emph{static} constituent n-grams (\cite{jurafsky2000}).

Nevertheless, when it comes to domain-specific problems, general pre-trained word embeddings do not perform very well~\cite{rahimikia2021realised}.
demonstrate that even state-of-the-art embedding models like Google's Word2Vec(skip-gram)\footnote{\url{https://code.google.com/archive/p/word2vec/}} and Facebook’s FastText(skip-gram)\footnote{\url{https://fasttext.cc/}}trained on 100 billion and 16 billion words, respectively, struggle to understand financial language like \begin{enumerate*}
    \item \emph{apple} standing for the company \emph{Apple},
    \item ticker analogies, e.g., \emph{amazon} is to \emph{X} as \emph{microsoft} is to \emph{msft},
    \item grouping company name to ticker, exchange and country
\end{enumerate*}.
In the same paper, the authors propose using the same algorithms (i.e., CBOW, Skip-gram, and FastText) but training solely on financial data instead - 15 years of financial news from the Dow Jones Newswires Text News Feed database, to produce the FinText models.
They report a substantial increase in performance in and sensitivity to detecting financial jargon and relationships.
In our project we acknowledge that a purpose-built financial word embedding (trained on proprietary data) will be more beneficial and more suited for the task of text summarisation of annual financial reports, which is why we select FinText as our preferred model.


\subsection{Attention}\label{subsec:attention}

\begin{figure}
    \begin{equation}
        \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\label{eq:equation2}
    \end{equation}
    \caption{Attention calculation (Query, Key, Value)}
    \label{fig:attention}
\end{figure}

\subsection{Recurrent Neural Networks (RNNs)}\label{subsec:rnn}
The vanilla RNN is a basic type of RNN architecture designed for processing sequential data.
It learns temporal patterns from the initial data by looping over the hidden layers which allow information to persist (i.e., they serve as a network memory) (\cite{olah2015understandingLSTM}).
The key component is the recurrent hidden state updated at each time step using input data and the previous hidden state. 
This allows the RNN to capture contextual information and temporal dependencies in the sequence.
However, due to the inherent vanishing and exploding gradient problems with the vanilla RNNs, they have limited ability to learn long-term dependencies (\cite{bengio1994learning}).
To resolve these issues more advanced RNN architectures like LSTMs and GRUs have been developed. \\

The Long Short-Term Memory (LSTM) recurrent neural network has become a ubiquitous method in sequential problems (e.g., language modelling, time series forecasting).
This is so because it allows long-term dependencies to propagate through the network with the help of control gates \- \emph{input} and \emph{forget}, which reduce the effect of the vanishing gradient issue in the vanilla RNN\footnote{
    We direct readers to~\cite{bayer2015learning} where the authors demonstrate that the LSTM's \enquote{temporal} gradient is unaffected by the fixed weight factor $W$ of the vanilla RNN that is driving the derivative to zero. 
    This is ensured by the additional architecture unit \- the \emph{forget} gate, which learns to  control the gradient flow in the network.
}.

A more simple variant of the LSTM is the Gated Recurrent Unit (GRU) which combines the \emph{input} and \emph{forget} gates into an \emph{update} gate for a model with fewer parameters and faster training (\cite{cahuantzi2021gru}).
Nevertheless, due to the sequential nature of the LSTM, the training process cannot be parallelised across GPUs, i.e., the learning cannot be made quicker by more computational resources.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/rnn}
        \caption{Unfolded RNN}
        \label{fig:rnn}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/bi-lstm}
        \caption{Unfolded bi-directional LSTM}
        \label{fig:bilstm}
    \end{subfigure}~\caption{Unfolded recurrent architectures (\cite{zhiyong2018bilstm})}
    \label{fig:recurrent_unfolded}
\end{figure}

\subsection{Transformers}\label{subsec:transformers}
The Transformer (\cite{vaswani2017attention}) is another sequence-to-sequence architecture which is parallelisable and attention-based. 



\begin{figure}
    \centering
    \begin{tikzpicture}[
  input/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=red!20},
  token/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=blue!20},
  seg_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=orange!20},
  pos_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=green!20},
  input_embed/.style={rectangle, rounded corners, minimum height=2.5em, minimum width=3em, draw, fill=purple!20},
  plus/.style={}
]

    % Tokens
    \node[input] (i1) {[CLS]};
    \node[input, right=0.5cm of i1] (i2) {It};
    \node[input, right=0.5cm of i2] (i3) {is};
    \node[input, right=0.5cm of i3] (i4) {sunny};
    \node[input, right=0.5cm of i4] (i5) {[SEP]};
    \node[input, right=0.5cm of i5] (i6) {But};
    \node[input, right=0.5cm of i6] (i7) {also};
    \node[input, right=0.5cm of i7] (i8) {cold};
    \node[input, right=0.5cm of i8] (i9) {[SEP]};

        % Tokens
    \node[token, below=1cm of i1] (t1) {$E_{[CLS]}$};
    \node[token, below=1cm of i2] (t2) {$E_{It}$};
    \node[token, below=1cm of i3] (t3) {$E_{is}$};
    \node[token, below=1cm of i4] (t4) {$E_{sunny}$};
    \node[token, below=1cm of i5] (t5) {$E_{[SEP]}$};
    \node[token, below=1cm of i6] (t6) {$E_{But}$};
    \node[token, below=1cm of i7] (t7) {$E_{also}$};
    \node[token, below=1cm of i8] (t8) {$E_{cold}$};
    \node[token, below=1cm of i9] (t9) {$E_{[SEP]}$};

    % Add plus nodes between inputs
    \draw ($(i1.south west) + (0,-0.3)$) -- ($(i9.south east) + (0,-0.3)$);

        % Segmentation embeddings
    \node[seg_embed, below=1cm of t1] (s1) {$E_{A}$};
    \node[seg_embed, below=1cm of t2] (s2) {$E_{A}$};
    \node[seg_embed, below=1cm of t3] (s3) {$E_{A}$};
    \node[seg_embed, below=1cm of t4] (s4) {$E_{A}$};
    \node[seg_embed, below=1cm of t5] (s5) {$E_{A}$};
    \node[seg_embed, below=1cm of t6] (s6) {$E_{B}$};
    \node[seg_embed, below=1cm of t7] (s7) {$E_{B}$};
    \node[seg_embed, below=1cm of t8] (s8) {$E_{B}$};
    \node[seg_embed, below=1cm of t9] (s9) {$E_{B}$};

    % Position embeddings
    \node[pos_embed, below=1cm of s1] (p1) {$E_{0}$};
    \node[pos_embed, below=1cm of s2] (p2) {$E_{1}$};
    \node[pos_embed, below=1cm of s3] (p3) {$E_{2}$};
    \node[pos_embed, below=1cm of s4] (p4) {$E_{3}$};
    \node[pos_embed, below=1cm of s5] (p5) {$E_{4}$};
    \node[pos_embed, below=1cm of s6] (p6) {$E_{5}$};
    \node[pos_embed, below=1cm of s7] (p7) {$E_{6}$};
    \node[pos_embed, below=1cm of s8] (p8) {$E_{7}$};
    \node[pos_embed, below=1cm of s9] (p9) {$E_{8}$};

    % Pluses at 0.25 between each (s, t) pair
    \node[above=0.25cm of $(s1)!.25!(t1)$, font=\large] {$+$};
    \node[above=0.25cm of $(s2)!.25!(t2)$, font=\large] {$+$};
    \node[above=0.25cm of $(s3)!.25!(t3)$, font=\large] {$+$};
    \node[above=0.25cm of $(s4)!.25!(t4)$, font=\large] {$+$};
    \node[above=0.25cm of $(s5)!.25!(t5)$, font=\large] {$+$};
    \node[above=0.25cm of $(s6)!.25!(t6)$, font=\large] {$+$};
    \node[above=0.25cm of $(s7)!.25!(t7)$, font=\large] {$+$};
    \node[above=0.25cm of $(s8)!.25!(t8)$, font=\large] {$+$};
    \node[above=0.25cm of $(s9)!.25!(t9)$, font=\large] {$+$};


    % Pluses at 0.25 between each (p, s) pair
    \node[above=0.25cm of $(p1)!.25!(s1)$, font=\large] {$+$};
    \node[above=0.25cm of $(p2)!.25!(s2)$, font=\large] {$+$};
    \node[above=0.25cm of $(p3)!.25!(s3)$, font=\large] {$+$};
    \node[above=0.25cm of $(p4)!.25!(s4)$, font=\large] {$+$};
    \node[above=0.25cm of $(p5)!.25!(s5)$, font=\large] {$+$};
    \node[above=0.25cm of $(p6)!.25!(s6)$, font=\large] {$+$};
    \node[above=0.25cm of $(p7)!.25!(s7)$, font=\large] {$+$};
    \node[above=0.25cm of $(p8)!.25!(s8)$, font=\large] {$+$};
    \node[above=0.25cm of $(p9)!.25!(s9)$, font=\large] {$+$};

\end{tikzpicture}
    \caption{BERT: Input Embeddings}
    \label{fig:bert_input}
\end{figure}

\subsection{Text Summarisation}\label{subsec:text-summarisation}
Text summarisation is the task of transforming a piece of text into a shorter  version that retains the most important information.
There are two overarching categories: extractive and abstractive text summarisation.
The former formulates the problem as a subset selection problem by returning only the most salient text excerpts from the original document (\cite{zhong-etal-2020-extractive}), while the latter aims to generate content anew, similar to how humans would do.

We will outline some key models that inspired our work below:
\begin{itemize}
    \item \textbf{Gokhan}: The authors employ an unsupervised summariser based on K-Means clustering of sentences encoded with SentenceBERT (\cite{reimers2019sentence}).
    However, their embeddings are pre-trained on general text, and they suggest that employing in-domain language models would result in a better performance.

    \item \textbf{AMUSE} (\cite{litvak-vanetik-2021-summarization}): The authors design an ETS system comprised of the following steps \begin{enumerate*}
        \item shortening of report with an existing Genetic Algorithm~\cite{litvak-last-2013-multilingual},
        \item encoding sentences with BERT vectors, and
        \item performing binary classification with LSTMs for salient sentence extraction
    \end{enumerate*}.
    They suggest that further work should incorporate \begin{enumerate*}
        \item efficient preliminary sentence removal, and
        \item additional neural modelling stages for the representation and detection of relevant input text parts.
    \end{enumerate*}

    \item \textbf{Hybrid model with RL} (\cite{zmandar-etal-2021-joint}): The authors train a joint extractive-abstractive summarisation model with reinforcement learning optimised for the ROUGE-2 F1 metric.
    Their networks are based on attentive LSTMs augmented with an additional copy mechanism (\cite{vinyals2015pointer}) achieving the second highest F1 score in the FNS21 competition.

    \item \textbf{T5} Hybrid (\cite{orzhenovskii-2021-t5}): The author used T5 (\cite{rayson2019t5}) for a hybrid model fine-tuned to generate the beginning of an abstractive summary and find the closest match of the output in the report’s full text.
    This is the best performing algorithm in the FNS21 competition but also the first to consider transformer models from an abstractive summarisation perspective in the FNP workshops so far.

\end{itemize}

In this work we will be solely exploring the extractive method, and more specifically - the \emph{supervised neural-based} (i.e., RNN, Transformer) type and the \emph{unsupervised graph-based} (i.e., TextRank, LexRank) type.


\subsection{LexRank}\label{subsec:lexrank}
LexRank (\cite{Erkan2004LexRankGC}) is an unsupervised extractive summarisation method consistently used as a baseline in the FNS21 and previous challenges.
It retrieves the most salient document sentences by computing their importance based on \emph{eigenvector centrality}.
To do that the algorithm creates a graph where each sentence represents a node and each edge is a weight between two nodes (\cite{Shearing2020AutomatedTS}).
The sentences are encoded as bag-of-words vectors of size $N$ - the vocabulary size, and the weight metric is a combination of tf-idf (Eq.\ref{eq:idf},\ref{eq:tfidf}) and cosine similarity - Eq.\ref{eq:cosinesimtfidf}.

\begin{equation}
    \text{idf}(t, D) = \log \frac{|D|}{|\{d \in D : t \in d\}|} \label{eq:idf}
\end{equation}

\begin{equation}
    \text{tf-idf}(t, d, D) = \text{tf}(t, d) \cdot \text{idf}(t, D)
    \label{eq:tfidf}
\end{equation}

\begin{equation}
    \text{tf\_idf\_cosine\_similarity}(s_1, s_2) = \frac{\sum_{t \in T} \text{tf-idf}(t, s_1, D) \cdot \text{tf-idf}(t, s_2, D)}{ \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_1, D)^2} \cdot \sqrt{\sum_{t \in T} \text{tf-idf}(t, s_2, D)^2}}
    \label{eq:cosinesimtfidf}
\end{equation}

where $t$ is a term, $d$ is a document within a collection of documents/sentences $D$.

Also, $s_1$ and $s_2$ are two sentences and $T$ represents the set of all terms in both of them while $tf(t, d)$ denotes the term frequency of $t$ in $d$, and $idf(t, D)$ is the inverse document frequency of $t$ in the collection $D$.

The authors further propose finding the most important sentences by \begin{enumerate*}
    \item applying a threshold for the creation of edges with Eq.\ref{eq:cosinesimtfidf},
    \item building an adjacency matrix and normalizing it to produce \emph{transition probabilities},
    \item computing in an iterative fashion the \emph{eigenvector centrality} until convergence, and finally
    \item ranking sentences based on their \emph{lexical} PageRank (\cite{page1998anatomy}) score.
\end{enumerate*}