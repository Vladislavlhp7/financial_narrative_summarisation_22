\section{Methodology}\label{sec:methodology}

\subsection{Data}\label{subsec:data}
The data for the FNS21 task is a collection of narrative parts of annual reports, converted from PDF to plain text.
As discussed in Section~\ref{subsec:fns} due to the rich visual representations in the PDFs, the resulting text suffers from various problems like
\begin{enumerate*}
    \item \emph{spacing inconsistencies} - mixing of tab-space word delimiters, over-segmentation (i.e., a split into incoherent chunks) and under-segmentation (i.e., merging of unrelated words),
    \item \emph{symbol encoding issues} - introduction of unreadable non-alphanumeric characters, and
    \item \emph{formatting issues} - words having different casing, hyphenation at the end of a line, etc
    \item \emph{conversion of tables to text} - financial figures spanning over multiple lines and being mixed with the text.
\end{enumerate*} (Figure~\ref{fig:pdf_to_text}).

% PDF-to-text conversion issues
\begin{figure}[ht]
    \centering
    \begin{enumerate}
        \item \begin{verbatim}
         Following my appointment as Chief
        E x e c u t i v e	in	J u l y	2 0 1 0 ,	g r e ate r	e m p h a s i s
        h a s	b e e n	p l a c e d	o n	f u l fi l l in g	t h e	s u p p l y	o f
        tonnage due under legacy contracts and
    \end{verbatim}
    \item \begin{verbatim}
        However,   the   Directors   further   believe   that   additional
        capital   	could   	be	  deployed	  to 	beneficial   	effect.
    \end{verbatim}
    \item \begin{verbatim}
        Opening net book amount 116,635 35,624 166,754 319,013
        Additions 51,380 7,647 307,546 366,573
    \end{verbatim}
    \item \begin{verbatim}
          This means that buyers can
        _0_@uk_ar06_front.indd   5 20/04/2007   09:13:30 05
        @UK PLC
        Annual Report and Accounts 2006
        use our network to purchase from their suppliers.
    \end{verbatim}
    \end{enumerate}
    \caption{PDF-to-text conversion issues.}
    \label{fig:pdf_to_text}
\end{figure}

To address these issues, we have developed a rigorous data cleaning pipeline that achieves the following key objectives:
\begin{enumerate*}
    \item handles space-tab mixing via hand-crafted rules (derived from observation\footnote{
      E.g., for some of the lines, characters were separated by spaces and words with tabs, hence the need for a custom rule.
    }),
    \item retains alphanumeric characters, punctuation, spaces, financial symbols, and
    \item removes sentences shorter than 3 words
\end{enumerate*}.

As discussed in Section~\ref{subsec:fns}, the annual reports are extremely long documents with an average length reported at 80 pages (\cite{litvak-vanetik-2021-summarization}).
Each one has at least two--three gold summaries provided by the FNS21 organisers, and we compute some statistics
\begin{enumerate*}
    \item helpful for grasping the nature of the output text, but also
    \item useful for the evaluation of the summarisation models
\end{enumerate*}.
On one hand, we can see that the average number of words in the longest summary is over $2,000$ (Figure~\ref{fig:longest_summary_word_count}), while the FNS21 regulations specify an expected output of at most $1,000$ words.
Furthermore, as we are not competing in the FNS21 task, for simplicity, during evaluation we will generate only summaries with at most 40 sentences.
We arrive at this number by observing that the median number of words in the longest summaries is 25 (Figure~\ref{fig:sentence_word_count}), and calculating that $\frac{1,000\text{words}}{25\text{words}}=40$ sentences can suffice.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering        \includegraphics[width=1\columnwidth]{../charts/longest_summary_word_count}
        \caption{Number of words in longest report summary}
        \label{fig:longest_summary_word_count}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/sentence_word_count}
        \caption{Number of words in training sentences}
        \label{fig:sentence_word_count}
    \end{subfigure}
    \caption{Distribution of number of words in training sentences and report summaries}
    \label{fig:word_count}
\end{figure}

As we were only provided with the training and the validation FNS21 datasets (Table~\ref{tab:fns21-data}),
we decided to treat the validation set as a testing set (Table~\ref{tab:fns21-my-data})\footnote{
    We observed that two of the annual report files were empty, hence the difference of 3,361 and 3,363 (Table~\ref{tab:fns21-data} without testing set).
} and perform our own training-validation data split on a sentence level instead due to the significant variation in report lengths (\cite{litvak-vanetik-2021-summarization}).
\begin{table}[h]
    \centering
    \begin{tabular}{lrr r}
        \hline
        Data Type & Training + Validation & Testing & Total \\
        \midrule
        Report full text & 2,998 & 363 & 3,361 \\
        \bottomrule
    \end{tabular}
    \caption{Training-Validation-Testing Data Split}
    \label{tab:fns21-my-data}
\end{table}
Specifically, we use a 90--10 \emph{stratified split} (i.e., the label distribution is retained in both sets) for training and validation, respectively.
We are aware that a validation and a training sentence can come from the same report, but we claim that this is not problematic for the following reasons:
\begin{enumerate}
    \item Sentences are \emph{de-contextualised} (i.e., without references or dependencies to others, taken out of context) and \emph{shuffled}.
    \item Sentences contain a \emph{great deal of textual noise} due to the PDF-to-text conversion.
    \item Annual reports are \emph{numerous} but also \emph{extremely long} (i.e., containing a lot of sentences).
\end{enumerate}
Therefore, we believe that the training and validation sentences are to a large extent independent, and as for the process of sentence extraction, we refer you to Section~\ref{subsec:sentence_extraction} for an in-depth discussion.

\subsection{Sentence Extraction}\label{subsec:sentence_extraction}
We approach the annual report summarisation problem from a supervised perspective - we cast the task of Extractive Text Summarisation (ETS) as a binary classification problem defined on the sentence level.
More formally, we can describe the annual report as $d=\{s_{1}, s_{2}, \dots, s_{n}\}$, where $d$ is a document, represented in terms of sentences $s_{i}, \  1 \leq i \leq n$ (\cite{liu2019finetuningbert}).

Then, a candidate summary\footnote{
    A candidate summary is generated from a model $m_{i}$ but it is not yet a \emph{best summary}.
} can be $c=\{s_{1}, s_{2}, \dots, s_{k} | s_{i} \in d \}, \ 0 \leq k \leq n$.
We further need to define the \emph{gold summary}, $c^{*}$ for a document $d$.
In the case of the FNS21 task, there are at least two summaries per report, hence we will use the following notation for the set of all gold summaries for each document $C^{*} = \{c^{*}_{1}, c^{*}_{2}, \dots, c^{*}_{p}\}$.
Furthermore, the supervised learning labels are $y_{i} \in \{1,0\}$ for each sentence $s_{i}$ in $d$ if the sentence is or is not in \textbf{\emph{any}} of the gold summaries $c^{*}_{j}$ for that document.
We argue that in order to increase the positive samples (i.e., the summarizing sentences) we should not restrict
ourselves to just one gold summary in the training process unlike~\cite{orzhenovskii-2021-t5}.
Our goal is to achieve better latent feature extraction of summaries through the employment of all existing data, hence using \textbf{\emph{any}} gold summaries.
However, we are aware that this approach is more likely to encounter standard ETS issues, specifically - extracted summary sentences could be retrieved from unrelated paragraphs in the report.
This can cause the \enquote{dangling anaphora} phenomenon, i.e.\ de-contextualised extracts are stitched together and can mislead the reader due to out-of-context references (\cite{lin2009summarization}).

While some authors (\cite{zmandar-etal-2021-joint}) follow the greedy ROUGE-maximisation method of matching summary
sentences to document sentences (established in~\cite{nallapati2017summarunner}), we approach the problem in a
more practical and faster fashion.
After manual observation of the reports against their gold summaries, it became clear that almost for all sentences
belonging to $c^{*}_{i}$, there was an exact match with a sentence in the whole annual report $d$.

This hypothesis was proven correct by one of the FNS21 contestants (\cite{orzhenovskii-2021-t5}) who reported that
99.4\% of the summaries were included in the report as whole subsequences.
Hence, after having pre-processed the text documents we iteratively match the sentences and generate the binary
classification labels ($\{1,0\}$ representing \emph{summary} and \emph{non-summary}, respectively) for both
the training and testing datasets.

We perform the sentence extraction as discussed above to produce a dataset (training and validation combined) of $3,554,800$ and $361,703$ sentences for classes 0 and 1, respectively.
Additionally, the training and validation sets are split in a 90--10 stratified fashion.

\subsection{Under-sampling and Data Augmentation}\label{subsec:data_augmentation}
Due to the fact that the annual reports are extremely long while the summaries are very short, the sentence dataset is highly imbalanced with a ratio of around 1:10.
Therefore, we took two\footnote{
    We also tried a third approach that is to augment sentences with the help of DINO (\cite{schick2021generating}) used for high-quality
    semantic augmentation, but we did not manage to recreate the desired output.
} different approaches to balance the classes:
\begin{enumerate}
    \item \textbf{Random under-sampling} -- As described in~\cite{weiss2013foundations, wongvorachan2023undersampling}, we \emph{randomly remove 90\% of the sentences} from the majority class (i.e., non-summary sentences) to produce a ratio of 1:1 summary to non-summary sentences (Figure~\ref{tab:random_under_sampling}).
    \begin{table}[ht]
        \centering
        \begin{tabular}{lccc}
            \toprule
                & \textbf{initial data} & \textbf{90\% under-sampled training} & \textbf{validation} \\
            \midrule
                \textbf{label 0} & 3,199,319 & 319,931 & 355,481 \\
                \textbf{label 1} & 325,533 & 325,533 & 36,170 \\
            \bottomrule
        \end{tabular}
        \caption{90\% Random Under-sampling}\label{tab:random_under_sampling}
    \end{table}
    \item \textbf{Data Augmentation} -- We use the \emph{back-translation} technique (\cite{hoang-etal-2018-iterative}) to generate new sentences from the minority class (i.e., summary sentences), followed by a 80\% random under-sampling of the majority class (Figure~\ref{tab:data_augmentation}).
    For that purpose, we translate all training summary sentences from English to French and back to English with the help of the MarianMT model (\cite{junczysdowmunt2018marian})\footnote{\url{https://huggingface.co/Helsinki-NLP/opus-mt-fr-en}}.
    The resulting dataset is then directly injected during the training process.
    \begin{table}[ht]
        \centering
        \begin{tabular}{lcccc}
        \toprule
            & \textbf{initial data} & \textbf{80\% under-sampled training} & \textbf{augmented training} & \textbf{validation} \\
        \midrule
            \textbf{label 0} & 3,199,319 & 639,863 & 639,863 & 355,481 \\
            \textbf{label 1} & 325,533 & 325,533 & 651,066 & 36,170 \\
        \end{tabular}
        \caption{Data Augmentation + 80\% Random Under-sampling}\label{tab:data_augmentation}
    \end{table}
\end{enumerate}.

Refer to Section~\ref{subsec:hyperparameters} for the experiments with the different data balancing techniques.


\subsection{Recurrent Extractor Model}\label{subsec:rnn_model}
As our main recurrent model we propose a GRU-based architecture (\cite{cho-etal-2014-learning}), inspired by~\cite{zmandar-etal-2021-joint} and depicted in Figure~\ref{fig:rnn_model}. \\

\begin{minipage}[ht]{0.5\textwidth}
    The model consists of a word embedding layer, a fully-connected feed-forward neural network (FCFFNN), two bidirectional gated recurrent units (GRU) layers, a dot-product attention layer, and a linear projection layer with softmax activation.

    The word embedding layer is used to convert the pre-processed input sentence into a vector representation.
    One of our implementation innovations is that we use FinText's FastText word embeddings~\cite{rahimikia2021realised} because they
    \begin{enumerate*}
        \item are character-based and thus can handle noisy or out-of-vocabulary words, and
        \item are pre-trained on large corpora of financial news, achieving considerable in-domain performance improvements over general-purpose embeddings
    \end{enumerate*}.

    We use the FCFFNN layer to \emph{map the vectorized sentences to a higher-level representation} (similar to~\cite{saikh2020deep}) capturing more complex features or patterns from the input text, but also to \emph{reduce the dimensionality} of the input.

    Two stacked GRU layers are used to \emph{extract the latent recurrent features} from the compressed vector representation in both directions - forward and backward (refer to Section~\ref{subsec:rnn} for details).
\end{minipage}\hfill
\noindent\begin{minipage}[ht]{0.4\textwidth}
    \begin{tikzpicture} [node distance = 0cm and 1cm,    box/.style={draw, rectangle, minimum height=0.5cm, minimum width=5cm},    arrow/.style={-{Stealth[length=4mm]}, thick, font=\footnotesize}]
        % Nodes
        \node[box] (input) {Pre-processed Input Sentence};
        \node[box, fill=blue!30, below=1cm of input] (we) {FinText Word Embedding};
        \node[box, fill=green!30, below=1cm of we] (fc) {Fully-Connected FFNN};
        \node[box, fill=yellow!30, below=1cm of fc] (biGRU1) {Bi-GRU Layer 1};
        \node[box, fill=yellow!30, below=1cm of biGRU1] (biGRU2) {Bi-GRU Layer 2};
        \node[box, fill=orange!30, below=1cm of biGRU2] (dpa) {Dot-Product Attention};
        \node[box, fill=green!30, below=1cm of dpa] (fc2) {Fully-Connected FFNN};
        \node[box, fill=purple!30, below=1cm of fc2] (sf) {Softmax};
        \node[circle, fill=white, draw=black, below left=0.5cm and 0.1cm of sf, minimum size=0.5cm] (bo1) {0};
        \node[circle, fill=white, draw=black, below right=0.5cm and 0.1cm of sf, minimum size=0.5cm] (bo2) {1};

        % Arrows
        \draw[arrow] (input) -- node[right] {} (we);
        \draw[arrow] (we) -- node[right] {} (fc);
        \draw[arrow] (fc) -- node[right] {} (biGRU1);
        \draw[arrow] (biGRU1) -- node[right] {} (biGRU2);
        \draw[arrow] (biGRU2) -- node[right] {} (dpa);
        \draw[arrow] (dpa) -- node[right] {} (fc2);
        \draw[arrow] (fc2) -- node[right] {} (sf);
        \draw[arrow] (sf) -- node[right] {} (bo1);
        \draw[arrow] (sf) -- node[right] {} (bo2);
    \end{tikzpicture}
    \captionof{figure}{GRU-based extractive model}
    \label{fig:rnn_model}
\end{minipage}\\

We further implement the scaled dot-product attention (Eq.\ref{eq:attention_score} from~\cite{vaswani2017attention}) to
compute a new weighted context-aware representation from the extracted features by the GRU layers.

The final layer is a fully-connected feed-forward neural network (FCFFNN) with a softmax activation function, which is
used to \emph{map the latent features to a binary classification} of the input sentence.

\subsection{Training}\label{subsec:training}
We trained both models on Tesla V100-SXM2-16GB\footnote{
    We extend our gratitude to the University of Manchester's Computational Shared Facility (CSF) for kindly agreeing to provide us with the computational resources for this research.
} and provide the following specifications:
\begin{enumerate}
    \item \textbf{RNN architectures} - below we provide the common general details, however for an in-depth discussion on the hyperparameter tuning, see Section~\ref{subsec:hyperparameters}.
        \begin{itemize}
            \item \textbf{Loss function:} Binary cross-entropy loss
            \item \textbf{Optimizer:} Adam~\cite{kingma2017adam}
            \item \textbf{Batch size:} 32
            \item \textbf{Epochs:} set to 60, but due to early stopping, the model practically trains for less than 10 epochs
            \item \textbf{Early stopping:} patience (i.e., the number of epochs to wait for improvement based on validation loss) set to 1
        \end{itemize}
    \item \textbf{FinBERT} - We followed the prescribed fine-tuning specifications provided in the original BERT paper~\cite{devlin-etal-2019-bert}:
        \begin{itemize}
            \item \textbf{Loss function:} Binary cross-entropy loss
            \item \textbf{Optimizer:} Adam~\cite{kingma2017adam}
            \item \textbf{Batch size:} 32
            \item \textbf{Epochs:} 3 - we found that the model started to over-fit after 3 epochs
            \item \textbf{Learning rate:} 2e-5
            \item \textbf{Weight decay:} 0.01
        \end{itemize}
\end{enumerate}


\subsection{Hyperparameter Tuning}\label{subsec:hyperparameters}
We have experimented with a number of hyperparameters for our recurrent model, including the use of a FCFFNN,
the recurrence type, the hidden units, the effect of applying attention, the dropout rate, the learning rate, and the effect of data augmentation.

For the analysis we will be extensively using the test accuracy, $F1$-score, and the \emph{summary recall} metric.
The latter is defined as the ratio of the number of correctly predicted summary sentences to the total number of summary sentences in the test dataset.
We consider this metric to be extremely relevant because in the context of extractive summarisation, our goal is to minimise the Type II error (i.e., the number of sentences that should be in the summary but are not).
Our reasoning is that our classifier must be as good as possible in recognising salient sentences (i.e., summarising sentences) even if it introduces some false positives (i.e., non-summarising sentences).
In practice, the user can always remove irrelevant sentences, but it is much harder to add sentences that should have already been in the summary.

Each sentence in the report is represented as a (100, 300)-sized word embeddings vector, where 100 is the longest possible sentence length (i.e., implying long sentences are trimmed) and 300 is the dimensionality of the word embeddings.
We test the effect of inserting an FCFFNN layer between the word embeddings and the GRU layers (each with 64 hidden units) and arrive at the following results:
adding an FCFFNN layer increases Summary Recall by 2.5\% (Fig.~\ref{fig:summary_recall_FFNN_effect}), but marginally reduces Test Accuracy by less than 1\% (Fig.~\ref{fig:test_accuracy_FFNN_effect}).
We attribute the increase in Summary Recall to the fact that the FCFFNN layer is able to extract an additional mix of features from the word embeddings, which are then used by the GRU layers to make better predictions.
As for the Test Accuracy, we believe that the small decrease is insignificant and we therefore choose to use the FCFFNN layer in our final model.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering \includegraphics[width=1\columnwidth]{../charts/summary_recall_FFNN_effect}
        \caption{Effect of FCFFNN layer on summary recall}
        \label{fig:summary_recall_FFNN_effect}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/test_accuracy_FFNN_effect}
        \caption{Effect of FCFFNN layer on test accuracy}
        \label{fig:test_accuracy_FFNN_effect}
    \end{subfigure}
    \caption{Effect of FCFFNN layer on summary recall and test accuracy}
    \label{fig:FCFFNN}
\end{figure}

We also explore the effect of using back-translated data (Section~\ref{subsec:data_augmentation}) on the model performance.
Results shown in Fig.~\ref{fig:data_augmentation_effect} suggest that the data augmentation does not improve the $F1$-score or the summary recall with a statistically significant margin.
At the same time, it seems to be amplifying the effect of the scaled dot-product attention (Section~\ref{subsec:seq2seq}, Figure~\ref{fig:summary_recall_data_augmentation_effect}).
\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering \includegraphics[width=1\columnwidth]{../charts/Summary Recall comparison}
        \caption{Effect of data augmentation on summary recall}
        \label{fig:summary_recall_data_augmentation_effect}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/f1 score comparison}
        \caption{Effect of data augmentation on $F1$-score}
        \label{fig:f1_score_comparison_data_augmentation_effect}
    \end{subfigure}
    \caption{Effect of back-translation data augmentation on summary recall and $F1$-score}
    \label{fig:data_augmentation_effect}
\end{figure}
While we are disappointed by these results, we believe that there can be a number of reasons for this.
For a machine translated sentence $s^m_i$ and its original sentence $s_i$, we \hypertarget{data_augment_hypothesis}{\emph{hypothesise}} that:
\begin{enumerate*}
    \item $s^m_i$ contain a similar amount of noise as $s_i$ (due to the pdf-to-text conversion process),
    \item $s^m_i$ does not introduce enough variation to $s_i$ (i.e., $s^m_i$ and $s_i$ are too similar), and
    \item while financial language itself is very domain-specific, it is not very semantically diverse (i.e., metaphors, idioms, etc.
    are limited in use)
\end{enumerate*}.

% GRU vs LSTM + hidden size 64 vs 256
In our proposed architecture we choose a bidirectional GRU (Bi-GRU) instead of a Bi-LSTM because
\begin{enumerate*}
    \item GRUs have a simpler structure than LSTMs and are easier to train (Section~\ref{subsec:rnn}), and
    \item our experiments show that GRUs outperform LSTMs with 1\% in terms of Summary Recall
\end{enumerate*}.
Furthermore, in terms of the hidden units, we select 64 over 256 because:
\begin{enumerate*}
    \item the architecture has $143,938$ and $2,050,306$ parameters, respectively (i.e., $256$ would result in an over-parameterised model),
    \item almost 4\% increase in Summary Recall (Fig.~\ref{fig:64vs256_summary_recall}),
    although a 2\% decrease in Test Accuracy (Fig.~\ref{fig:test_accuracy_dropout_and_hidden_size}), if we use $64$ hidden units.
\end{enumerate*}
\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering \includegraphics[width=1\columnwidth]{../charts/64vs256_summary_recall}
        \caption{Summary Recall: 256 vs 64 hidden units + dropout}
        \label{fig:64vs256_summary_recall}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/test_accuracy_dropout_and_hidden_size}
        \caption{Test Accuracy: 256 vs 64 hidden units + dropout}
        \label{fig:test_accuracy_dropout_and_hidden_size}
    \end{subfigure}
    \caption{Effect of hidden units and dropout on summary recall and test accuracy}
    \label{fig:dropout_and_hidden_size}
\end{figure}

From the above experiments it is hard to make any certain conclusions on the effect of dropout\footnote{
    It would make more sense for its application in the over-parameterised model, though this does not seem to be the case (Fig.~\ref{fig:dropout_and_hidden_size}).
} or the use of an attention mechanism (Section~\ref{subsec:seq2seq}).
We believe that only one \emph{single-head attention} is not sufficient to learn all the complex relationships between words in the sentences.
Our reasoning is that because a particular head specializes to only specific language aspects (i.e., syntactic, semantic, etc) (\cite{clark-etal-2019-bert})),
for future experiments it would be much more reasonable to use multiple heads instead.
Nevertheless, there still are some practical benefits of attention to our extractive summarisation system which we will discuss in Section~\ref{sec:evaluation}.