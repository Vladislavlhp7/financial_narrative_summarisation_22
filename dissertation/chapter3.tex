\section{Design \& Development}\label{sec:design-and-development}

\subsection{Methodology}\label{subsec:methodology}
We approach the annual report summarisation problem from a supervised perspective - we cast the task of Extractive Text Summarisation (ETS) as a binary classification problem defined on the sentence level.
More formally, we can describe the annual report as $d=\{s_{1}, s_{2}, \dots, s_{n}\}$, where $d$ is a document, represented in terms of sentences $s_{i}, \  1 \leq i \leq n$ (\cite{liu2019finetuningbert}).

Then, a candidate summary can be $c=\{s_{1}, s_{2}, \dots, s_{k} | s_{i} \in d \}, \ 0 \leq k \leq n$.

We further need to define the \emph{gold summary}, $c^{*}$ for a document $d$.

In the case of the FNS21 task, there are at least two summaries per report, hence we will use the following notation for the set of all gold summaries for each document $C^{*} = \{c^{*}_{1}, c^{*}_{2}, \dots, c^{*}_{p}\}$.
Furthermore, the supervised learning labels are $y_{i} \in \{1,0\}$ for each sentence $s_{i}$ in $d$ if the sentence is or is not in \textbf{\emph{any}}\footnote{
    To increase the positive samples (i.e., the summarizing sentences) we do not restrict ourselves to just one gold summary in the training process unlike~\cite{orzhenovskii-2021-t5}.
    Our goal is to achieve better latent feature extraction of summaries through the employment of all existing data.
    However, we are aware that this approach is more likely to encounter standard ETS issues, specifically - extracted summary sentences could be retrieved from unrelated paragraphs in the report.
    This causes the \enquote{dangling anaphora} phenomenon, i.e. decontextualised extracts are stitched together and could mislead the reader due to out-of-context references as specified in~\cite{lin2009summarization}.
} of the gold summaries $c^{*}_{j}$ for that document.

In general, to assess the quality of a candidate summary $c$, we measure its similarity with the gold summary $c^{*}$ based on their n-gram overlap $R=(c, c^{*})$, where $R$ is the ROUGE-$F_{1}$\footnote{
    We use a slightly different but faster version of ROUGE compared to the official metric~\cite{lin2004rouge}.
    It can be accessed at: \url{https://github.com/pltrdy/rouge}
    } metric(\cite{lin2004rouge}).

For the FNS21 task due to the extractive nature of our approach we will evaluate our models based on the ROUGE-maximising $c^{*}_{i}$ gold summary, i.e.,

\begin{figure}[h]
    \centering
    \begin{equation}\label{eq:rouge_max}
        r = \underset{c^{*} \in C^{*}}{\operatorname{argmax}} R(c, c^{*}_{i})
    \end{equation}
    \caption{Candidate summary evaluation as a gold summary ROUGE-maximisation}
    \label{fig:rouge_max}
\end{figure}


While some authors (\cite{zmandar-etal-2021-joint}) follow the greedy ROUGE-maximisation method of matching summary sentences to document sentences (established in~\cite{nallapati2017summarunner}), we approach the problem in a more practical and faster fashion.
After manual observation of the reports against their gold summaries, it became clear that for almost all sentences of $c^{*}_{i}$, there was an exact match with a sentence in the whole annual report $d$.

This hypothesis was proven correct by one of the FNS21 contestants (\cite{orzhenovskii-2021-t5}) who reported that 99.4\% of the summaries were included in the
report as whole subsequences.
Hence, after having pre-processed the text documents we iteratively match the sentences and generate the binary classification labels ($\{1,0\}$ representing \emph{summary} and \emph{non-summary}, respectively) for both the training and testing datasets.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering        \includegraphics[width=1\columnwidth]{../charts/longest_summary_word_count}
        \caption{Number of words in longest report summary}
        \label{fig:longest_summary_word_count}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/sentence_word_count}
        \caption{Number of words in training sentences}
        \label{fig:sentence_word_count}
    \end{subfigure}
    \caption{Distribution of number of words in training sentences and report summaries}
    \label{fig:word_count}
\end{figure}