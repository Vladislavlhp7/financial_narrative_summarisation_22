\section{Methodology}\label{sec:methodology}

\subsection{Data}\label{subsec:data}
The data for the FNS21 task is a collection of narrative parts of annual reports, converted from PDF to plain text.
As discussed in Section~\ref{subsec:fns} due to the rich visual representations in the PDFs, the resulting text suffers from various problems like
\begin{enumerate*}
    \item \emph{spacing inconsistencies} - mixing of tab-space word delimiters, over-segmentation (i.e., a split into incoherent chunks) and under-segmentation (i.e., merging of unrelated words),
    \item \emph{symbol encoding issues} - introduction of unreadable non-alphanumeric characters, and
    \item \emph{formatting issues} - words having different casing, hyphenation at the end of a line, etc
    \item \emph{conversion of tables to text} - financial figures spanning over multiple lines and being mixed with the text.
\end{enumerate*} (Figure~\ref{fig:pdf_to_text}).

% PDF-to-text conversion issues
\begin{figure}[ht]
    \centering
    \begin{enumerate}
        \item \begin{verbatim}
         Following my appointment as Chief
        E x e c u t i v e	in	J u l y	2 0 1 0 ,	g r e ate r	e m p h a s i s
        h a s	b e e n	p l a c e d	o n	f u l fi l l in g	t h e	s u p p l y	o f
        tonnage due under legacy contracts and
    \end{verbatim}
    \item \begin{verbatim}
        However,   the   Directors   further   believe   that   additional
        capital   	could   	be	  deployed	  to 	beneficial   	effect.
    \end{verbatim}
    \item \begin{verbatim}
        Opening net book amount 116,635 35,624 166,754 319,013
        Additions 51,380 7,647 307,546 366,573
    \end{verbatim}
    \item \begin{verbatim}
          This means that buyers can
        _0_@uk_ar06_front.indd   5 20/04/2007   09:13:30 05
        @UK PLC
        Annual Report and Accounts 2006
        use our network to purchase from their suppliers.
    \end{verbatim}
    \end{enumerate}
    \caption{PDF-to-text conversion issues.}
    \label{fig:pdf_to_text}
\end{figure}

To address these issues, we have developed a rigorous data cleaning pipeline that achieves the following key objectives:
\begin{enumerate*}
    \item handles space-tab mixing via hand-crafted rules (derived from observation\footnote{
      E.g., for some of the lines, characters were separated by spaces and words with tabs, hence the need for a custom rule.
    }),
    \item retains alphanumeric characters, punctuation, spaces, financial symbols, and
    \item removes sentences shorter than 3 words
\end{enumerate*}.

As discussed in Section~\ref{subsec:fns}, the annual reports are extremely long documents with an average length reported at 80 pages (\cite{litvak-vanetik-2021-summarization}).
Each one has at least two--three gold summaries provided by the FNS21 organisers, and we compute some statistics
\begin{enumerate*}
    \item helpful for grasping the nature of the output text, but also
    \item useful for the evaluation of the summarisation models
\end{enumerate*}.
On one hand, we can see that the average number of words in the longest summary is over $2,000$ (Figure~\ref{fig:longest_summary_word_count}), while the FNS21 regulations specify an expected output of at most $1,000$ words.
Furthermore, as we are not competing in the FNS21 task, for simplicity, during evaluation we will generate only summaries with at most 40 sentences.
We arrive at this number by observing that the median number of words in the longest summaries is 25 (Figure~\ref{fig:sentence_word_count}), and calculating that $\frac{1,000\text{words}}{25\text{words}}=40$ sentences can suffice.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering        \includegraphics[width=1\columnwidth]{../charts/longest_summary_word_count}
        \caption{Number of words in longest report summary}
        \label{fig:longest_summary_word_count}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/sentence_word_count}
        \caption{Number of words in training sentences}
        \label{fig:sentence_word_count}
    \end{subfigure}
    \caption{Distribution of number of words in training sentences and report summaries}
    \label{fig:word_count}
\end{figure}

As we were only provided with the training and the validation FNS21 datasets (Table~\ref{tab:fns21-data}),
we decided to treat the validation set as a testing set (Table~\ref{tab:fns21-my-data})\footnote{
    We observed that two of the annual report files were empty, hence the difference of 3,361 and 3,363 (Table~\ref{tab:fns21-data} without testing set).
} and perform our own training-validation data split on a sentence level instead due to the significant variation in report lengths (\cite{litvak-vanetik-2021-summarization}).
\begin{table}[h]
    \centering
    \begin{tabular}{lrr r}
        \hline
        Data Type & Training + Validation & Testing & Total \\
        \midrule
        Report full text & 2,998 & 363 & 3,361 \\
        \bottomrule
    \end{tabular}
    \caption{Training-Validation-Testing Data Split}
    \label{tab:fns21-my-data}
\end{table}
Specifically, we use a 90--10 \emph{stratified split} (i.e., the label distribution is retained in both sets) for training and validation, respectively.
We are aware that a validation and a training sentence can come from the same report, but we claim that this is not problematic for the following reasons:
\begin{enumerate}
    \item Sentences are \emph{de-contextualised} (i.e., without references or dependencies to others, taken out of context) and \emph{shuffled}.
    \item Sentences contain a \emph{great deal of textual noise} due to the PDF-to-text conversion.
    \item Annual reports are \emph{numerous} but also \emph{extremely long} (i.e., containing a lot of sentences).
\end{enumerate}
Therefore, we believe that the training and validation sentences are to a large extent independent, and as for the process of sentence extraction, we refer you to Section~\ref{subsec:sentence_extraction} for an in-depth discussion.

\subsection{Sentence Extraction}\label{subsec:sentence_extraction}
We approach the annual report summarisation problem from a supervised perspective - we cast the task of Extractive Text Summarisation (ETS) as a binary classification problem defined on the sentence level.
More formally, we can describe the annual report as $d=\{s_{1}, s_{2}, \dots, s_{n}\}$, where $d$ is a document, represented in terms of sentences $s_{i}, \  1 \leq i \leq n$ (\cite{liu2019finetuningbert}).

Then, a candidate summary\footnote{
    A candidate summary is generated from a model $m_{i}$ but it is not yet a \emph{best summary}.
} can be $c=\{s_{1}, s_{2}, \dots, s_{k} | s_{i} \in d \}, \ 0 \leq k \leq n$.
We further need to define the \emph{gold summary}, $c^{*}$ for a document $d$.
In the case of the FNS21 task, there are at least two summaries per report, hence we will use the following notation for the set of all gold summaries for each document $C^{*} = \{c^{*}_{1}, c^{*}_{2}, \dots, c^{*}_{p}\}$.
Furthermore, the supervised learning labels are $y_{i} \in \{1,0\}$ for each sentence $s_{i}$ in $d$ if the sentence is or is not in \textbf{\emph{any}} of the gold summaries $c^{*}_{j}$ for that document.
We argue that in order to increase the positive samples (i.e., the summarizing sentences) we should not restrict
ourselves to just one gold summary in the training process unlike~\cite{orzhenovskii-2021-t5}.
Our goal is to achieve better latent feature extraction of summaries through the employment of all existing data, hence using \textbf{\emph{any}} gold summaries.
However, we are aware that this approach is more likely to encounter standard ETS issues, specifically - extracted summary sentences could be retrieved from unrelated paragraphs in the report.
This can cause the \enquote{dangling anaphora} phenomenon, i.e.\ de-contextualised extracts are stitched together and can mislead the reader due to out-of-context references (\cite{lin2009summarization}).

While some authors (\cite{zmandar-etal-2021-joint}) follow the greedy ROUGE-maximisation method of matching summary
sentences to document sentences (established in~\cite{nallapati2017summarunner}), we approach the problem in a
more practical and faster fashion.
After manual observation of the reports against their gold summaries, it became clear that almost for all sentences
belonging to $c^{*}_{i}$, there was an exact match with a sentence in the whole annual report $d$.

This hypothesis was proven correct by one of the FNS21 contestants (\cite{orzhenovskii-2021-t5}) who reported that
99.4\% of the summaries were included in the report as whole subsequences.
Hence, after having pre-processed the text documents we iteratively match the sentences and generate the binary
classification labels ($\{1,0\}$ representing \emph{summary} and \emph{non-summary}, respectively) for both
the training and testing datasets.

\section{Recurrent Extractor Model}\label{sec:rnn_model}
As our main recurrent model we propose a GRU-based architecture (\cite{cho-etal-2014-learning}), inspired by~\cite{zmandar-etal-2021-joint} and depicted in Figure~\ref{fig:rnn_model}. \\

\begin{minipage}[ht]{0.5\textwidth}
    The model consists of a word embedding layer, a fully-connected feed-forward neural network (FCFFNN), two bidirectional gated recurrent units (GRU) layers, a dot-product attention layer, and a linear projection layer with softmax activation.

    The word embedding layer is used to convert the pre-processed input sentence into a vector representation.
    One of our implementation innovations is that we use FinText's FastText word embeddings~\cite{rahimikia2021realised} because they
    \begin{enumerate*}
        \item are character-based and thus can handle noisy or out-of-vocabulary words, and
        \item are pre-trained on large corpora of financial news, achieving considerable in-domain performance improvements over general-purpose embeddings
    \end{enumerate*}.

    We use the FCFFNN layer to \emph{map the vectorized sentences to a higher-level representation} (similar to~\cite{saikh2020deep}) capturing more complex features or patterns from the input text, but also to \emph{reduce the dimensionality} of the input.

    Two stacked GRU layers are used to \emph{extract the latent recurrent features} from the compressed vector representation in both directions - forward and backward (refer to Section~\ref{subsec:rnn} for details).
\end{minipage}\hfill
\noindent\begin{minipage}[ht]{0.4\textwidth}
    \begin{tikzpicture} [node distance = 0cm and 1cm,    box/.style={draw, rectangle, minimum height=0.5cm, minimum width=5cm},    arrow/.style={-{Stealth[length=4mm]}, thick, font=\footnotesize}]
        % Nodes
        \node[box] (input) {Pre-processed Input Sentence};
        \node[box, fill=blue!30, below=1cm of input] (we) {FinText Word Embedding};
        \node[box, fill=green!30, below=1cm of we] (fc) {Fully-Connected FFNN};
        \node[box, fill=yellow!30, below=1cm of fc] (biGRU1) {Bi-GRU Layer 1};
        \node[box, fill=yellow!30, below=1cm of biGRU1] (biGRU2) {Bi-GRU Layer 2};
        \node[box, fill=orange!30, below=1cm of biGRU2] (dpa) {Dot-Product Attention};
        \node[box, fill=green!30, below=1cm of dpa] (fc2) {Fully-Connected FFNN};
        \node[box, fill=purple!30, below=1cm of fc2] (sf) {Softmax};
        \node[circle, fill=white, draw=black, below left=0.5cm and 0.1cm of sf, minimum size=0.5cm] (bo1) {0};
        \node[circle, fill=white, draw=black, below right=0.5cm and 0.1cm of sf, minimum size=0.5cm] (bo2) {1};

        % Arrows
        \draw[arrow] (input) -- node[right] {} (we);
        \draw[arrow] (we) -- node[right] {} (fc);
        \draw[arrow] (fc) -- node[right] {} (biGRU1);
        \draw[arrow] (biGRU1) -- node[right] {} (biGRU2);
        \draw[arrow] (biGRU2) -- node[right] {} (dpa);
        \draw[arrow] (dpa) -- node[right] {} (fc2);
        \draw[arrow] (fc2) -- node[right] {} (sf);
        \draw[arrow] (sf) -- node[right] {} (bo1);
        \draw[arrow] (sf) -- node[right] {} (bo2);
    \end{tikzpicture}
    \captionof{figure}{GRU-based extractive model}
    \label{fig:rnn_model}
\end{minipage}\\

We further implement the scaled dot-product attention (Eq.\ref{eq:attention_score} from~\cite{vaswani2017attention}) to
compute a new weighted context-aware representation from the extracted features by the GRU layers.

The final layer is a fully-connected feed-forward neural network (FCFFNN) with a softmax activation function, which is
used to \emph{map the latent features to a binary classification} of the input sentence.

\subsection{Hyperparameter Tuning}\label{subsec:hyperparameters}
We have experimented with a number of hyperparameters for our recurrent model, including the use of a FCFFNN,
the recurrence type, the effect of applying attention, the dropout rate, the learning rate, and the effect of data augmentation.

For the analysis we will be extensively using the test accuracy, $F1$-score, and the \emph{summary recall} metric.
The latter is defined as the ratio of the number of correctly predicted summary sentences to the total number of summary sentences in the test dataset.
We consider this metric to be extremely relevant because in the context of extractive summarisation, our goal is to minimise the Type II error (i.e., the number of sentences that should be in the summary but are not).
Our reasoning is that our classifier must be as good as possible in recognising salient sentences (i.e., summarising sentences) even if it introduces some false positives (i.e., non-summarising sentences).
In practice, the user can always remove irrelevant sentences, but it is much harder to add sentences that should have already been in the summary.

Each sentence in the report is represented as a (100, 300)-sized word embeddings vector, where 100 is the longest possible sentence length (i.e., implying long sentences are trimmed) and 300 is the dimensionality of the word embeddings.
We test the effect of inserting an FCFFNN layer between the word embeddings and the GRU layers (each with 64 hidden units) and arrive at the following conclusions:
Adding an FCFFNN layer increases Summary Recall by 2.5\% (Fig.~\ref{fig:summary_recall_FFNN_effect}), but marginally reduces Test Accuracy by less than 1\% (Fig.~\ref{fig:test_accuracy_FFNN_effect}).
We attribute the increase in Summary Recall to the fact that the FCFFNN layer is able to extract an additional mix of features from the word embeddings, which are then used by the GRU layers to make better predictions.
As for the Test Accuracy, we believe that the small decrease is insignificant and we therefore choose to use the FCFFNN layer in our final model.

\begin{figure}[ht]
    \begin{subfigure}{0.49\textwidth}
        \centering \includegraphics[width=1\columnwidth]{../charts/summary_recall_FFNN_effect}
        \caption{Effect of FCFFNN layer on summary recall}
        \label{fig:summary_recall_FFNN_effect}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=1\columnwidth]{../charts/test_accuracy_FFNN_effect}
        \caption{Effect of FCFFNN layer on test accuracy}
        \label{fig:test_accuracy_FFNN_effect}
    \end{subfigure}
    \caption{Effect of FCFFNN layer on summary precision and test accuracy}
    \label{fig:FCFFNN}
\end{figure}




\begin{enumerate}

\end{enumerate}




