\section{Evaluation}\label{sec:evaluation}
In this section we will describe the evaluation process of our models.
For that purpose, we will describe how we use the ROUGE metrics (Section~\ref{subsubsec:rouge}) to compute the
similarity between the gold and candidate summaries, and how we arrive at the final evaluation aggregation (Section~\ref{subsec:evaluation-mechanism}).
Afterwards, we will provide the results of our evaluation (Section~\ref{subsec:quantitative-evaluation}), where we compare our models to the baselines and official FNS models.
Finally, we will discuss the qualitative limitations of our produced summaries (Section~\ref{subsec:qualitative-discussion}) before we conclude our work in Section~\ref{sec:conclusion}.

\subsection{Evaluation Mechanism}\label{subsec:evaluation-mechanism}
To assemble a candidate summary $c_{i}$ we prepare the sentences $s_{j}^{i}$ from a report $d_{m}$ (e.g., embed or tokenize sentences for the GRUs and FinBERT, respectively).
Afterwards, we feed them into our model computing the output summary probabilities $p_{j}^{i}$, and then we select the
top-$k$ sentences ($k=40$, Section~\ref{subsec:data}) based on the highest sentence probabilities $p_{j}^{i}$.
Finally, we concatenate them to form the summary $c_{i}$ in natural order (i.e., the order in the input text),
but also trim it to the maximum length of $1,000$ words (Section~\ref{subsec:fns}).

In general, to assess the quality of a candidate summary $c$, we measure its similarity with the gold summary $c^{*}$
based on their n-gram overlap $R=(c, c^{*})$, where $R$ is the ROUGE-2\footnote{
        We use a slightly different but faster version of ROUGE compared to the official metric~\cite{lin2004rouge}.
        It can be accessed at: \url{https://github.com/pltrdy/rouge}. \\
        The FNS evaluation metric is the $F1$-score of ROUGE-2, and we will use it for the final evaluation.
} metric(\cite{lin2004rouge}).
For the FNS task due to the extractive nature of our approach we will evaluate our models based on
the ROUGE-L-maximising\footnote{
    We say ROUGE-L-maximising for conciseness, because ROUGE is a metric with precision and recall components, which are combined into a single $F1$ score.
    It is precisely the $F1$ score that we maximise.
    However, we are aware this introduces more complexity to our naming convention, hence we will use the term ROUGE-L-maximising.
} $c^{*\max}_{i}$ gold summary (Section~\ref{subsubsec:rouge}), i.e.,

\begin{equation}\label{eq:rouge_max}
    c^{*\max} = \underset{c^{*} \in C^{*}}{\operatorname{argmax}} \text{ROUGE-L}(c, c^{*}_{i})
\end{equation}
where $C^{*}$ is the set of gold summaries for a given report $d$, and $c^{*\max}_{i}$ is the gold summary with maximal ROUGE-L score with the candidate summary $c$.

The intuition is that by extracting multiple sentences from the report, our generated candidate summary can
retain sentences from \emph{any} of the gold summaries.
Hence, there must be at least one such gold summary where the longest common subsequence overlap (ROUGE-L) is maximal.
The practical implications are that two models, $m_{1}$ and $m_{2}$ can produce two different candidate summaries
$c_{1}$ and $c_{2}$, respectively.
Their individual evaluation is based on gold summaries $c^{*}_{1}$ and $c^{*}_{2}$ (which can be the same when the
candidates $c_{1}$ and $c_{2}$ are identical).
This guarantees that we are always comparing candidate summaries based on their maximal evaluation scores (i.e., their maximal summarising potential).

Therefore, the final evaluation score for a model $m$ is the average ROUGE-2 score\footnote{
    Here, once again we mean the $F1$-measure of ROUGE-2.
} between the candidate summaries $c_{i}$ and their corresponding \emph{ROUGE-L-maximising gold summaries} $c^{*\max}_{i}$, i.e.,
\begin{equation}\label{eq:rouge_final}
    r_{m} = \frac{1}{|C|} \sum_{i=1}^{|C|} \text{ROUGE-2}(c_{i}, c^{*\max}_{i})
\end{equation}
where $|C|$ is the number of candidate summaries $c_{i}$.

\subsection{Quantitative Evaluation}\label{subsec:quantitative-evaluation}
Following this evaluation mechanism, we compare our models in terms of their ROUGE metrics in Table~\ref{tab:rouge_performance_validation}, which
shows the results on the official validation set used as a testing set for our models.
We again wish to remind the reader that we were not provided with the FNS22 testing set (Section~\ref{subsec:data}).

Therefore, in our comparison we include the following FNS22 models, namely:
\begin{enumerate}
    \item team LIPI's T5 model~\cite{el-haj-etal-2022-financial} (testing data is only available), which is the completely based on the T5-LONG-EXTRACT and also the best English model in that edition;
    \item team LSIR's mT5 model~\cite{foroutan-etal-2022-multilingual}, which is the best model overall for all languages in the competition (multilingual for English, Spanish, and Greek);
    \item the Longformer-Encoder-Decoder (LED)~\cite{khanna-etal-2022-transformer}
    \item the Top-K Narrative Extractor (NE) ~\cite{shukla-etal-2022-dimsum}  ranking in the top three models overall for Spanish and Greek;
\end{enumerate}.
We have the official English validation results for all but the LIPI's T5, for which we are going to use their testing evaluations (we are aware that this comparison is somewhat unfair).
We will highlight the FNS models with a $\heartsuit$, and our top-performing ones with a $\spadesuit$ for ease of reference.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} \\
        \midrule
            $\heartsuit$ Top-K NE & \textbf{0.546} & \textbf{0.425} & - \\
            $\spadesuit$ FinBERT-base & \emph{0.544} & \emph{0.382} & \emph{0.524} \\
            $\heartsuit$ LIPI's T5 (\emph{official testing set}) & 0.496 & 0.374 & 0.487 \\
            FinBERT-base + back-translation & 0.490 & 0.321 & 0.468 \\
            $\heartsuit$ LSIR's mT5 & 0.440 & 0.301 & 0.423 \\
            $\heartsuit$ LED & 0.442 & 0.302 & 0.434 \\
            $\spadesuit$ GRU-base + attention + back-translation & \emph{0.276} & \emph{0.106} & \emph{0.249} \\
            GRU-base + back-translation & 0.266 & 0.100 & 0.247 \\
            LexRank & 0.250 & 0.086 & 0.227 \\
            TaxtRank & 0.220 & 0.064 & 0.196 \\
            GRU-base & 0.220 & 0.063 & 0.201 \\
            GRU-base + attention & 0.221 & 0.062 & 0.204 \\
        \bottomrule
    \end{tabular}\caption{ROUGE $F1$ Scores on official validation set (used as testing in our models)}
    \label{tab:rouge_performance_validation}
\end{table}

We can provide the following commentary on the results:
\begin{itemize}
    \item Our best performing model, FinBERT-base~\cite{yang2020finbert}, which is a pre-trained on financial communication documents,
        achieves an average ROUGE-2 score of $0.382$, which outperforms by $0.081$ on the validation set the best performing
        model overall in the FNS22 competition - \emph{LSIR's mT5}~\cite{foroutan-etal-2022-multilingual}.
        Furthermore, our FinBERT-base also seems to \emph{slightly outperform the best English model in the FNS22} - LIPI's T5~\cite{el-haj-etal-2022-financial} with $0.008$,
        though this comparison is not entirely fair as we are not using the same datasets.
        Regarding the other FNS models, we observe that our FinBERT-base beats the LED~\cite{khanna-etal-2022-transformer} with a similar margin as the LSIR's mT5.
        However, the Top-K remains with the highest ROUGE-1 and ROUGE-2 $F1$ measures.
        Surprisingly, data augmentation does not improve the performance of our FinBERT-base, and we believe this was caused by
        the \hyperlink{data_augment_hypothesis}{back-translation hypothesis} we made in Section~\ref{subsec:hyperparameters}, which can be leading to over-fitting.
        Nevertheless, both models clearly outperform the official baselines: LexRank~\cite{Erkan2004LexRankGC} and TextRank~\cite{mihalcea-tarau-2004-textrank}.
    \item The GRU-base + attention + back-translation model is the best performing model out of all recurrent neural architectures.
    While preliminary binary classification results did not show any considerable differences between the models, clearly
    \begin{enumerate*}[label=(\alph*)]
        \item the attention mechanism helps the model to better recognise the summarising sentences (i.e., attends to the most descriptive linguistic features), and
        \item the back-translation data augmentation significantly improves the practical performance of the model (i.e., the probability distribution of the summarising sentences),
              which is clearly not the case for our FinBERT model.
    \end{enumerate*}
    Additionally, we must note that except of the Top-K NE~\cite{shukla-etal-2022-dimsum}, all other FNS models are transformer-based,
    hence they have more complex architectures and attention mechanisms than our GRU-based models with single-head attention (Sections~\ref{subsec:transformers} and~\ref{subsec:rnn_model}).
    \item At the same time, we acknowledge that the universal summarisation baselines: LexRank and TextRank, outperform
        our simple GRU models (Table~\ref{tab:rouge_performance_validation}), and we attribute this to both:
    \begin{enumerate*}[label=(\alph*)]
        \item the lack of sufficient descriptive training data from the positive class (i.e., the summarising sentences, Table~\ref{tab:random_under_sampling}),
        \item the 90\% random under-sampling of the majority class data (see Section~\ref{subsec:data}), and
        \item the summary generation process of selecting the top-$k$ sentences based on their sorted probabilities (see discussion in Sections~\ref{subsec:qualitative-discussion} and~\ref{subsec:limitations}).
    \end{enumerate*}
\end{itemize}

\newpage

\subsection{Qualitative Discussion}\label{subsec:qualitative-discussion}
After having established the quantitative performance of our models, we now turn to the qualitative discussion of the results.
For a random annual report we generated a summary using the FinBERT-base + data augmentation model and
the GRU-base + attention model (see Figures~\ref{fig:rnn_summary},~\ref{fig:finbert_summary}).
where green colour indicates the summarising sentences, and red colour -- the non-summarising sentences).
We chose these models over the best ones because they have slightly lower ROUGE scores, and make more mistakes, which will help us identify the issues in the summarisation process.
We can make the following conclusions based on observation:
\begin{enumerate}
    \item The FinBERT model has \emph{around 50\% of its contents} belonging to any of the gold summaries (Figure~\ref{fig:finbert_summary}),
        while all other sentences look very convincing in terms of their summarising potential (i.e., they are informative of the financial situation but also concise)
    \item At the same time, the GRU-model has the opposite characteristics:
    \begin{enumerate*}[label=(\alph*)]
        \item none of its sentences are in the gold summary (Figure~\ref{fig:rnn_summary}), while
        \item all of them are very long, containing uninformative but diverse sets of words, which in turn results in higher ROUGE scores
    \end{enumerate*}.
    This clearly represents the problem of using ROUGE as a metric for summarisation since it is only measures the lexical overlap, while being semantically unaware~\cite{akter-etal-2022-revisiting}.
\end{enumerate}
While we acknowledge that the GRU model seems to be \emph{biased towards long and noisy sentences}, we must note that in the example
only $5$ sentences have been generated to fit the word limit.
Therefore, we believe the summary generation process (i.e., the mechanism to combine predicted sentences into a single summary) further
exacerbates the accuracy of the summarisation.

Although, the practical results from Figures~\ref{fig:rnn_summary} might seem disappointing, we must once again remind the reader that the reports are extremely
long with an average number of sentences and words at around $2,700$ and $58,000$, respectively~\cite{litvak-vanetik-2021-summarization}.
Meanwhile, we are constrained to producing a summary of at most $40$ sentences (Section~\ref{subsec:data}) or $1,000$ words (Section~\ref{subsec:fns}).

\newpage

\section{Conclusion}\label{sec:conclusion}
In this section we summarise the main contributions and discuss the limitations of our work.

\subsection{Summary of Achievements}\label{subsec:summary}
In this project we have explored the problem of summarising UK annual reports.
To deal with the significant amount of noise in the plain text of these glossy documents, we have built a rigorous pre-processing pipeline (Section~\ref{subsec:data}).
We have further implemented a sentence extraction phase where we generate binary labels ($1$ being summary, $0$ - non-summary) from the reports and their multiple gold summaries (Section~\ref{subsec:sentence_extraction}).
Once our datasets are created, we
\begin{enumerate*}[label=(\alph*)]
    \item design a Recurrent Neural Network (RNN) architecture (Section~\ref{subsec:rnn_model}), and
    \item fine-tune a financial Transformer model - FinBERT (Section~\ref{subsec:finbert})
\end{enumerate*},
training them in a supervised manner for a binary classification task (Sections~\ref{subsec:training} and~\ref{subsec:hyperparameters}).
We quantitatively evaluate our models with the ROUGE metric and demonstrate them outperforming traditional baselines (Section~\ref{sec:evaluation}).
Furthermore, at least for FinBERT we observe a clear ROUGE-2 improvement on the validation set over the best overall FNS22 model~\cite{foroutan-etal-2022-multilingual}.
Additionally, we show that our FinBERT also achieves competitive performance with the best FNS22 English model,
although noting that the our models are tested on different data (Section~\ref{subsec:data}).
We also discuss the quality of the produced summaries (Section~\ref{subsec:qualitative-discussion}), and in
Section~\ref{subsec:limitations} we describe in more depth the limitations of our system and possible solutions.
\\
In terms of \emph{innovational aspects} of our project in the context of the FNS challenge, we are the first to our knowledge that:
\begin{itemize}
    \item \emph{Integrate FinText word embeddings} - While some FNS21 competitors use general-domain sentence embeddings
    based on BERT~\cite{litvak-vanetik-2021-summarization, gokhan-etal-2021-extractive}, we represent sentences as
    a vector of word embeddings purpose-built for financial text analysis~\cite{rahimikia2021realised}.
    \item \emph{Perform back-translation as data augmentation} - In contrast to approaches where only the first 10\% of
    the annual report is used~\cite{orzhenovskii-2021-t5}, we over-sample the summarising sentences (minority class)
    by back-translating from French.
    \item~\emph{Fine-tune FinBERT~\cite{yang2020finbert} for extractive summarisation} - Transformer-based models have become increasingly
    popular in the FNP22 Workshop~\cite{khanna-etal-2022-transformer, pant-chopra-2022-multilingual},
    where some have used FinBERT for classifying definitions~\cite{ghosh-etal-2022-finrad},
    detecting hypernyms~\cite{peng-etal-2022-discovering}, and classifying financial sentiments~\cite{stepisnik-perdih-etal-2022-sentiment}.
    However, we are the first to adapt FinBERT for extractive summarisation.
\end{itemize}

\subsection{Discussion of Limitations}\label{subsec:limitations}
Although, both our models have outperformed the baselines on ROUGE-2 and the fine-tuned FinBERT has achieved competitive
performance for the FNS22 task, there are several limitations that we would like to discuss:
\begin{itemize}
    \item \emph{Sentence embedding} - Although, we note our use of domain-specific FastText word embeddings due to their
    ability to handle noise and outperform general-domain embeddings~\cite{rahimikia2021realised}, we do not perform
    any sentence--level aggregation (i.e., dimensionality reduction) like averaging to condense the overall representation.
    While, this was a deliberate design decision to better capture the relationship between individual words,
    our sentence vectors became of size $(100, 300)$ instead of $(300,)$, which became more computationally expensive.
    Although, we are aware that FastText~\cite{bojanowski-etal-2017-enriching} provides average-pooling for any sequence,
    we were pessimistic of using it due to the loss of word order information (e.g., ``the company is good'' being represented just as ``is the company good'').
    Therefore, a limitation of our work is that we do not investigate the impact of using sentence embeddings
    (be it with positional encoding or average-pooling) on the performance of our models.
    \item \emph{Non-exhaustive evaluation} - Due to the FNS models being proprietary, and also evaluated on the official testing set,
    we are unable to make a more comprehensive comparison with the other models.
    However, in an ideal scenario we would perform further quantitative and qualitative evaluation of our performance.
    \item \emph{Summary Generation} - In our work we take top $k$ sentences based on the model's output probability distribution (Section~\ref{sec:evaluation}).
        However, this is a very simplistic approach to summarisation that
        \begin{enumerate*}[label=(\alph*)]
            \item introduces incoherence issues (like the \emph{dangling anaphora phenomenon} from Section~\ref{subsec:sentence_extraction}),
            \item trims the last sentence to fit the $1,000$ word limit, and it
            \item does not account for the \emph{informativeness} of the individual sentences
        \end{enumerate*}.
        To address the incoherence issues, we can try resolving the coreferences in the either through a
        graph-based approach on the generated summary~\cite{sonawane2016coreference}, or by introducing a more complex
        encoder architecture that represents and attends to entities as well as sentences~\cite{Huang2021ExtractiveSC}.
        Regarding the trimming heuristic, a natural improvement can be to use text compression techniques for the final predicted sentence~\cite{ghalandari2022efficient, KNIGHT200291}.
        As for the third point, we believe this is very exacerbating reason why our GRU architecture returns a summary
        without any single whole--sentence overlap with the gold standard (Fig.~\ref{fig:rnn_summary}).
        Instead, what~\cite{zmandar-etal-2021-joint} propose is a reinforcement learning approach which incorporates
        the \emph{sentence-level} ROUGE-2 score with the whole gold summary.
        While this method is much more sophisticated, it conveys the intuitive idea that the top-$k$ sentences comprising the
        \emph{optimal candidate summary} should be \emph{greedily maximising the global summary-level ROUGE-2} score.
\end{itemize}

\subsection{Future Work}\label{subsec:future-work}
While in our project we only consider the narrative summarisation of
financial reports already converted to plain text, we propose the following pipeline as a direction for future work:
\begin{enumerate}
    \item \emph{PDF-to-Text} - Integrate into the summarisation system a PDF-to-Text conversion tool for annual
    reports like the CFIE-FRSE\footnote{\url{https://github.com/drelhaj/CFIE-FRSE}}~\cite{elhaj2019multilingual}, which
    also extracts the text into 8 generic section headers (Section~\ref{subsec:uk-annual-reports}).
    \item \emph{Text-to-Summary} - Implement an extractive method that addresses the limitations from Section~\ref{subsec:limitations},
    or alternatively, an abstractive method producing \emph{lay summaries} for non-expert users~\cite{vinzelberg2023lay, Guo2020AutomatedLL}.
    \item \emph{Text-to-Analysis} - Apply NLP techniques like sentiment analysis~\cite{araci2019finbert}, named entity recognition
   ~\cite{zhang2022finbertmrc}, and detection of forward-looking sentences~\cite{stihec-etal-2021-preliminary},
    to extract useful information from the summary and the text.
    Additionally, important financial disclosure characteristics as amount, tone, and transparency~\cite{li2010textual, Li2011TextualAO}
    would be beneficial for AF researchers and users.
    \item \emph{Packaged Software} - Build a drag-and-drop software application that allows users to upload a PDF file of an
    annual report, where the backend will perform the steps above and return a summary.
    The suggested textual analysis features could be integrated as interactive visual elements.
    Furthermore, through recognising company names, the system could also provide dashboard of news and stock prices
    with the help of the company-to-identifier mapping~\cite{el-haj2019retrieving}
    (i.e., getting the company ticker, e.g., \emph{NASDAQ: AAPL} for Apple Inc).
\end{enumerate}