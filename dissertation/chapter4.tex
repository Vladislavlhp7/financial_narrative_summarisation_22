\section{Evaluation}\label{sec:evaluation}

In general, to assess the quality of a candidate summary $c$, we measure its similarity with the gold summary $c^{*}$
based on their n-gram overlap $R=(c, c^{*})$, where $R$ is the ROUGE-$F_{1}$\footnote{
    \begin{itemize}
        \item We use a slightly different but faster version of ROUGE compared to the official metric~\cite{lin2004rouge}.
              It can be accessed at: \url{https://github.com/pltrdy/rouge}
        \item The official FNS21 evaluation metric is the $F1$-score of ROUGE-2 and we will use it for the final evaluation.
    \end{itemize}
} metric(\cite{lin2004rouge}).
For the FNS21 task due to the extractive nature of our approach we will evaluate our models based on
the ROUGE--maximising $c^{*}_{i}$ gold summary, i.e.,

\begin{figure}[h]
    \centering
    \begin{equation}\label{eq:rouge_max}
        r = \underset{c^{*} \in C^{*}}{\operatorname{argmax}} R(c, c^{*}_{i})
    \end{equation}
    \caption{Candidate summary evaluation as a gold summary ROUGE-maximisation}
    \label{fig:rouge_max}
\end{figure}

The intuition is that by extracting multiple sentences from the report, our generated candidate summary can
retain sentences from \emph{any} of the gold summaries.
Hence, there must be at least one such gold summary where the overlap is maximal.
The practical implications are that two models, $m_{1}$ and $m_{2}$ can produce two different candidate summaries
$c_{1}$ and $c_{2}$, respectively.
Their individual evaluation is based on gold summaries $c^{*}_{1}$ and $c^{*}_{2}$ (which can be the same when the
candidates $c_{1}$ and $c_{2}$ are identical).

Following this evaluation mechanism, we compare our models in terms of their ROUGE metrics in Table~\ref{tab:rouge_performance}.
Please keep in mind that we include official results from the FNS21 competition for comparison, namely the T5-LONG-EXTRACT model
(\cite{orzhenovskii-2021-t5}) and the MUSE model (\cite{litvak-last-2013-multilingual}), which have been
\begin{enumerate*}
    \item trained on more data than our models due to us using the official validation as a test set (see Section~\ref{subsec:data}),
    \item evaluated on the official test set which we did not have access to
\end{enumerate*}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} \\
        \midrule
            $\star$ FinBERT-base & 0.544 & 0.382 & 0.524 \\
            T5-LONG-EXTRACT (\cite{orzhenovskii-2021-t5})* & 0.54 & 0.38 & 0.52 \\
            FinBERT-base + back-translation & 0.490 & 0.321 & 0.468 \\
            MUSE (\cite{litvak-last-2013-multilingual})* & 0.5 & 0.28 & 0.45 \\
            $\star$ GRU-base + attention + back-translation & 0.276 & 0.106 & 0.249 \\
            GRU-base + back-translation & 0.266 & 0.100 & 0.247 \\
            LexRank & 0.250 & 0.086 & 0.227 \\
            TaxtRank & 0.220 & 0.064 & 0.196 \\
            GRU-base & 0.220 & 0.063 & 0.201 \\
            GRU-base + attention & 0.221 & 0.062 & 0.204 \\
        \bottomrule
    \end{tabular}\caption{ROUGE: Model performance on the FNS21 test dataset.}
    \label{tab:rouge_performance}
\end{table}

We can provide the following commentary on the results:
\begin{itemize}
    \item The best performing model is FinBERT-base (\cite{yang2020finbert}), which is a pre-trained on financial communication documents,
        achieving a ROUGE-2 score of $0.382$, which is \emph{at least as good as the official best-performing model} in the FNS21 competition:
        the T5-LONG-EXTRACT (\cite{orzhenovskii-2021-t5}, Section~\ref{subsec:text-summarisation}).
        Surprisingly, data augmentation does not improve the performance of FinBERT-base, and we believe this was caused by
        the \hyperlink{data_augment_hypothesis}{back-translation hypothesis} we made in Section~\ref{subsec:hyperparameters}.
        Nevertheless, both models outperform the official top-line (\cite{litvak-last-2013-multilingual}) by a significant margin for ROUGE-2.
    \item The GRU-base + attention + back-translation model is the best performing model out of all recurrent neural architectures.
    While preliminary binary classification results did not show any considerable differences between the models, clearly
    \begin{enumerate*}
        \item the attention mechanism helps the model to better recognise the summarising sentences (i.e., attends to the most descriptive linguistic features), and
        \item the back-translation data augmentation significantly improves the practical performance of the model (i.e., the probability distribution of the summarising sentences).
    \end{enumerate*}
    \item At the same time, we acknowledge that the universal summarisation baselines: LexRank and TextRank, outperform
        our simple GRU models, and we attribute this to both:
    \begin{enumerate*}
        \item the lack of sufficient descriptive training data from the positive class (i.e., the summarising sentences, Table~\ref{tab:random_under_sampling}), and
        \item the 90\% random under-sampling of the majority class data (see Section~\ref{subsec:data}).
    \end{enumerate*}
\end{itemize}

\subsection{Qualitative Discussion}\label{subsec:qualitative-discussion}
After having established the quantitative performance of our models, we now turn to the qualitative discussion of the results.
For a random annual report we generated a summary using the FinBERT-base + data augmentation model and
the GRU-base + attention + back-translation model (see Figure~\ref{fig:summary_examples}
where green colour indicates the summarising sentences, and red colour -- the non-summarising sentences).
We can make the following conclusions based on observation:
\begin{enumerate}
    \item The FinBERT-base model has \emph{around 50\% of its contents} belonging to any of the gold summaries (Figure~\ref{fig:finbert_summary}),
        while all other sentences look very convincing in terms of their summarising potential (i.e., they are informative of the financial situation but also concise)
    \item At the same time, the GRU-model has the opposite characteristics:
    \begin{enumerate*}
        \item none of its sentences are in the gold summary (Figure~\ref{fig:rnn_summary}), and
        \item all the sentences are very long, containing uninformative but diverse sets of words.
    \end{enumerate*}
\end{enumerate}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{transformer-example-summary}
        \caption{Example summary from the FinBERT-base + data augmentation model.}
        \label{fig:finbert_summary}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{rnn_summary}
        \caption{Example summary from the GRU-base + attention + data augmentation model.}
        \label{fig:rnn_summary}
    \end{subfigure}
    \caption{Examples of summaries produced by our models.}
    \label{fig:summary_examples}
\end{figure}


\section{Conclusion}\label{sec:conclusion}
\subsection{Summary}\label{subsec:summary}
In this project we have explored the problem of summarising UK annual reports.


\subsection{Discussion}\label{subsec:discussion}

\subsection{Limitations}\label{subsec:limitations}
Although, both our models have outperformed the baselines on ROUGE-2 and the fine-tuned FinBERT has achieved competitive performance for the FNS21 task, there are several limitations that we would like to discuss:
\begin{itemize}
    \item
\end{itemize}

\subsection{Future Work}\label{subsec:future-work}