\section{Evaluation}\label{sec:evaluation}

In general, to assess the quality of a candidate summary $c$, we measure its similarity with the gold summary $c^{*}$
based on their n-gram overlap $R=(c, c^{*})$, where $R$ is the ROUGE-$F_{1}$\footnote{
    \begin{itemize}
        \item We use a slightly different but faster version of ROUGE compared to the official metric~\cite{lin2004rouge}.
              It can be accessed at: \url{https://github.com/pltrdy/rouge}
        \item The official FNS21 evaluation metric is the $F1$-score of ROUGE-2 and we will use it for the final evaluation.
    \end{itemize}
} metric(\cite{lin2004rouge}).
For the FNS21 task due to the extractive nature of our approach we will evaluate our models based on
the ROUGE--maximising $c^{*}_{i}$ gold summary, i.e.,

\begin{figure}[h]
    \centering
    \begin{equation}\label{eq:rouge_max}
        r = \underset{c^{*} \in C^{*}}{\operatorname{argmax}} R(c, c^{*}_{i})
    \end{equation}
    \caption{Candidate summary evaluation as a gold summary ROUGE-maximisation}
    \label{fig:rouge_max}
\end{figure}

The intuition is that by extracting multiple sentences from the report, our generated candidate summary can
retain sentences from \emph{any} of the gold summaries.
Hence, there must be at least one such gold summary where the overlap is maximal.
The practical implications are that two models, $m_{1}$ and $m_{2}$ can produce two different candidate summaries
$c_{1}$ and $c_{2}$, respectively.
Their individual evaluation is based on gold summaries $c^{*}_{1}$ and $c^{*}_{2}$ (which can be the same when the
candidates $c_{1}$ and $c_{2}$ are identical).

Following this evaluation mechanism, we compare our models in terms of their ROUGE metrics in Table~\ref{tab:rouge_performance}.
Please keep in mind that we include official results from the FNS21 competition for comparison, namely the T5-LONG-EXTRACT model
(\cite{orzhenovskii-2021-t5}) and the MUSE model (\cite{litvak-last-2013-multilingual}), which have been
\begin{enumerate*}
    \item trained on more data than our models due to us using the official validation as a test set (see Section~\ref{subsec:data}),
    \item evaluated on the official test set which we did not have access to
\end{enumerate*}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} \\
        \midrule
            $\star$ FinBERT-base & 0.544 & 0.382 & 0.524 \\
            T5-LONG-EXTRACT (\cite{orzhenovskii-2021-t5})* & 0.54 & 0.38 & 0.52 \\
            FinBERT-base + back-translation & 0.490 & 0.321 & 0.468 \\
            MUSE (\cite{litvak-last-2013-multilingual})* & 0.5 & 0.28 & 0.45 \\
            $\star$ GRU-base + attention + back-translation & 0.276 & 0.106 & 0.249 \\
            GRU-base + back-translation & 0.266 & 0.100 & 0.247 \\
            LexRank & 0.250 & 0.086 & 0.227 \\
            TaxtRank & 0.220 & 0.064 & 0.196 \\
            GRU-base & 0.220 & 0.063 & 0.201 \\
            GRU-base + attention & 0.221 & 0.062 & 0.204 \\
        \bottomrule
    \end{tabular}\caption{ROUGE: Model performance on the FNS21 test dataset.}
    \label{tab:rouge_performance}
\end{table}

We can provide the following commentary on the results:
\begin{itemize}
    \item The best performing model is FinBERT-base (\cite{yang2020finbert}), which is a pre-trained on financial communication documents,
        achieving a ROUGE-2 score of $0.382$, which is \emph{at least as good as the official best-performing model} in the FNS21 competition:
        the T5-LONG-EXTRACT (\cite{orzhenovskii-2021-t5}, Section~\ref{subsec:text-summarisation}).
        Surprisingly, data augmentation does not improve the performance of FinBERT-base, and we believe this was caused by
        the \hypertarget{data_augment_hypothesis}{back-translation hypothesis} we made in Section~\ref{subsec:hyperparameters}.
        Nevertheless, both models outperform the official top-line (\cite{litvak-last-2013-multilingual}) by a significant margin for ROUGE-2.
    \item The GRU-base + attention + back-translation model is the best performing model that uses a neural architecture.
          It is followed by the GRU-base + back-translation model, which is the same model but without attention.
          This shows that the attention mechanism is beneficial for the task.
    \item The LexRank and TextRank models are the best performing models that do not use a neural architecture.
          This shows that the neural architecture is beneficial for the task.
\end{itemize}

