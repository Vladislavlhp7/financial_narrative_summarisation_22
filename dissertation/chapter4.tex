\section{Evaluation}\label{sec:evaluation}

To assemble a candidate summary $c_{i}$ we prepare the sentences $s_{j}^{i}$ from a report $d_{m}$ (e.g., embed or tokenize sentences for the GRUs and FinBERT, respectively).
Afterwards, we feed them into our model computing the output summary probabilities $p_{j}^{i}$, and then we select the
top-$k$ sentences ($k=40$, Section~\ref{subsec:data}) based on the highest sentence probabilities $p_{j}^{i}$.
Finally, we concatenate them to form the summary $c_{i}$ in natural order (i.e., the order in the input text),
but also trim it to the maximum length of $1,000$ words (Section~\ref{subsec:fns}).

In general, to assess the quality of a candidate summary $c$, we measure its similarity with the gold summary $c^{*}$
based on their n-gram overlap $R=(c, c^{*})$, where $R$ is the ROUGE-$F_{1}$\footnote{
    \begin{itemize}
        \item We use a slightly different but faster version of ROUGE compared to the official metric~\cite{lin2004rouge}.
              It can be accessed at: \url{https://github.com/pltrdy/rouge}
        \item The FNS21 evaluation metric is the $F1$-score of ROUGE-2, and we will use it for the final evaluation.
    \end{itemize}
} metric(\cite{lin2004rouge}).
For the FNS21 task due to the extractive nature of our approach we will evaluate our models based on
the ROUGE-L--maximising $c^{*}_{i}$ gold summary (Section~\ref{subsubsec:rouge}), i.e.,

\begin{figure}[h]
    \centering
    \begin{equation}\label{eq:rouge_max}
        r = \underset{c^{*} \in C^{*}}{\operatorname{argmax}} R(c, c^{*}_{i})
    \end{equation}
    \caption{Candidate summary evaluation as a gold summary ROUGE-maximisation}
    \label{fig:rouge_max}
\end{figure}

The intuition is that by extracting multiple sentences from the report, our generated candidate summary can
retain sentences from \emph{any} of the gold summaries.
Hence, there must be at least one such gold summary where the longest common subsequence overlap (ROUGE-L) is maximal.
The practical implications are that two models, $m_{1}$ and $m_{2}$ can produce two different candidate summaries
$c_{1}$ and $c_{2}$, respectively.
Their individual evaluation is based on gold summaries $c^{*}_{1}$ and $c^{*}_{2}$ (which can be the same when the
candidates $c_{1}$ and $c_{2}$ are identical).
This guarantees that we are always comparing candidate summaries based on their maximal evaluation scores.

Following this evaluation mechanism, we compare our models in terms of their ROUGE metrics in Table~\ref{tab:rouge_performance}.
Please keep in mind that we include official results from the FNS21 competition for comparison, namely the T5-LONG-EXTRACT model
(\cite{orzhenovskii-2021-t5}) and the MUSE model (\cite{litvak-last-2013-multilingual}), which have been
\begin{enumerate*}
    \item trained on more data than our models due to us using the official validation as a test set (see Section~\ref{subsec:data}),
    \item evaluated on the official test set which we did not have access to
\end{enumerate*}.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{ROUGE-L} \\
        \midrule
            $\star$ FinBERT-base & 0.544 & 0.382 & 0.524 \\
            T5-LONG-EXTRACT (\cite{orzhenovskii-2021-t5})* & 0.54 & 0.38 & 0.52 \\
            FinBERT-base + back-translation & 0.490 & 0.321 & 0.468 \\
            MUSE (\cite{litvak-last-2013-multilingual})* & 0.5 & 0.28 & 0.45 \\
            $\star$ GRU-base + attention + back-translation & 0.276 & 0.106 & 0.249 \\
            GRU-base + back-translation & 0.266 & 0.100 & 0.247 \\
            LexRank & 0.250 & 0.086 & 0.227 \\
            TaxtRank & 0.220 & 0.064 & 0.196 \\
            GRU-base & 0.220 & 0.063 & 0.201 \\
            GRU-base + attention & 0.221 & 0.062 & 0.204 \\
        \bottomrule
    \end{tabular}\caption{ROUGE: Model performance on the FNS21 test dataset.}
    \label{tab:rouge_performance}
\end{table}

We can provide the following commentary on the results:
\begin{itemize}
    \item The best performing model is FinBERT-base (\cite{yang2020finbert}), which is a pre-trained on financial communication documents,
        achieving a ROUGE-2 score of $0.382$, which is \emph{at least as good as the official best-performing model} in the FNS21 competition:
        the T5-LONG-EXTRACT (\cite{orzhenovskii-2021-t5}, Section~\ref{subsec:text-summarisation}).
        Surprisingly, data augmentation does not improve the performance of FinBERT-base, and we believe this was caused by
        the \hyperlink{data_augment_hypothesis}{back-translation hypothesis} we made in Section~\ref{subsec:hyperparameters}.
        Nevertheless, both models outperform the official top-line (\cite{litvak-last-2013-multilingual}) by a significant margin for ROUGE-2.
    \item The GRU-base + attention + back-translation model is the best performing model out of all recurrent neural architectures.
    While preliminary binary classification results did not show any considerable differences between the models, clearly
    \begin{enumerate*}
        \item the attention mechanism helps the model to better recognise the summarising sentences (i.e., attends to the most descriptive linguistic features), and
        \item the back-translation data augmentation significantly improves the practical performance of the model (i.e., the probability distribution of the summarising sentences).
    \end{enumerate*}
    \item At the same time, we acknowledge that the universal summarisation baselines: LexRank and TextRank, outperform
        our simple GRU models, and we attribute this to both:
    \begin{enumerate*}
        \item the lack of sufficient descriptive training data from the positive class (i.e., the summarising sentences, Table~\ref{tab:random_under_sampling}),
        \item the 90\% random under-sampling of the majority class data (see Section~\ref{subsec:data}), and
        \item the summary generation process of selecting the top $k$ sentences based on their sorted probabilities (see discussion in Sections~\ref{subsec:qualitative-discussion} and~\ref{subsec:limitations}).
    \end{enumerate*}
\end{itemize}

\subsection{Qualitative Discussion}\label{subsec:qualitative-discussion}
After having established the quantitative performance of our models, we now turn to the qualitative discussion of the results.
For a random annual report we generated a summary using the FinBERT-base + data augmentation model and
the GRU-base + attention model (see Figure~\ref{fig:summary_examples}
where green colour indicates the summarising sentences, and red colour -- the non-summarising sentences).
We can make the following conclusions based on observation:
\begin{enumerate}
    \item The FinBERT-base model has \emph{around 50\% of its contents} belonging to any of the gold summaries (Figure~\ref{fig:finbert_summary}),
        while all other sentences look very convincing in terms of their summarising potential (i.e., they are informative of the financial situation but also concise)
    \item At the same time, the GRU-model has the opposite characteristics:
    \begin{enumerate*}
        \item none of its sentences are in the gold summary (Figure~\ref{fig:rnn_summary}), while
        \item all of them are very long, containing uninformative but diverse sets of words, which in turn results in higher ROUGE scores.
    \end{enumerate*}.
    This clearly represents the problem of using ROUGE as a metric for summarisation since it is only measures the lexical overlap, while being semantically unaware (\cite{akter-etal-2022-revisiting}).
\end{enumerate}
While we acknowledge that the GRU model seems to be biased towards long and noisy sentences, we must note that in the example
only $5$ sentences have been generated to fit the word limit.
As we demonstrated that our model is accurate at recognising the summarising sentences (Section~\ref{subsec:hyperparameters}),
we believe that it is more of a summary generation issue (i.e., the mechanism to combine predicted sentences into a single summary)
rather than a classification issue (see Section~\ref{subsec:limitations}).

Although, the practical results from Figure~\ref{fig:summary_examples} might seem disappointing, we must once again remind the reader that the reports are extremely
long with an average number of sentences and words at around $2,700$ and $58,000$, respectively (\cite{litvak-vanetik-2021-summarization}).
Meanwhile, we are constrained to producing a summary of at most $40$ sentences (Section~\ref{subsec:data}) or $1,000$ words (Section~\ref{subsec:fns}).

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../charts/transformer-example-summary}
        \caption{Example summary from the FinBERT-base + data augmentation model.}
        \label{fig:finbert_summary}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../charts/rnn_summary}
        \caption{Example summary from the GRU-base + attention model.}
        \label{fig:rnn_summary}
    \end{subfigure}
    \caption{Examples of summaries produced by our models.}
    \label{fig:summary_examples}
\end{figure}


\section{Conclusion}\label{sec:conclusion}
\subsection{Summary of Achievements}\label{subsec:summary}
In this project we have explored the problem of summarising UK annual reports.
To deal with the significant amount of noise in the plain text of these glossy documents, we have built a rigorous pre-processing pipeline (Section~\ref{subsec:data}).
We have further implemented a sentence extraction phase where we generate binary labels ($1$ being summary, $0$ - non-summary) from the reports and their multiple gold summaries (Section~\ref{subsec:sentence_extraction}).
Once our datasets are created, we
\begin{enumerate*}
    \item design a Recurrent Neural Network (RNN) architecture (Section~\ref{subsec:rnn_model}), and
    \item fine-tune a financial Transformer model - FinBERT (Section~\ref{subsec:finbert})
\end{enumerate*},
training them in a supervised manner for a binary classification task (Sections~\ref{subsec:training} and~\ref{subsec:hyperparameters}).
We quantitatively evaluate our models with the ROUGE metric and demonstrate them outperforming traditional baselines (Section~\ref{sec:evaluation}).
Furthermore, at least for FinBERT we observe a clear ROUGE-2 improvement over an official FNS21 topline but also a
competitive performance with the best FNS21 extractive model, although noting that the our models are tested on different data (Section~\ref{subsec:data}).
We also discuss the quality of the produced summaries (Section~\ref{subsec:qualitative-discussion}), and in
Section~\ref{subsec:limitations} we describe in more depth the limitations of our system and possible solutions.
\\
In terms of \emph{innovational aspects} of our project in the context of the FNS challenge, we are the first to our knowledge that:
\begin{itemize}
    \item \emph{Integrate FinText word embeddings} - While some FNS21 competitors use general-domain sentence embeddings
    based on BERT (\cite{litvak-vanetik-2021-summarization, gokhan-etal-2021-extractive}), we represent sentences as
    a vector of word embeddings purpose-built for financial text analysis (\cite{rahimikia2021realised}).
    \item \emph{Perform back-translation as data augmentation} - In contrast to approaches where only the first 10\% of
    the annual report is used (\cite{orzhenovskii-2021-t5}), we over-sample the summarising sentences (minority class)
    by back-translating from French.
    \item \emph{Fine-tune FinBERT (\cite{yang2020finbert}) for extractive summarisation} - Transformer-based models have become increasingly
    popular in the next edition of the FNP Workshop (FNP22) (\cite{khanna-etal-2022-transformer, pant-chopra-2022-multilingual}),
    where some have used FinBERT for classifying definitions (\cite{ghosh-etal-2022-finrad}),
    detecting hypernyms (\cite{peng-etal-2022-discovering}), and classifying financial sentiments (\cite{stepisnik-perdih-etal-2022-sentiment}).
    However, we are the first to adapt FinBERT for extractive summarisation.
\end{itemize}

\subsection{Discussion of Limitations}\label{subsec:limitations}
Although, both our models have outperformed the baselines on ROUGE-2 and the fine-tuned FinBERT has achieved competitive
performance for the FNS21 task, there are several limitations that we would like to discuss:
\begin{itemize}
    \item \emph{Summary Generation} - In our work we take top $k$ sentences based on the model's output probability distribution (Section~\ref{sec:evaluation}).
        However, this is a very simplistic approach to summarisation that
        \begin{enumerate*}
            \item introduces incoherence issues (like the \emph{dangling anaphora phenomenon} from Section~\ref{subsec:sentence_extraction}),
            \item trims the last sentence to fit the $1,000$ word limit, and it
            \item does not account for the \emph{informativeness} of the individual sentences
        \end{enumerate*}.
        To address the incoherence issues, we can try resolving the correferences in the either through a
        graph-based approach on the generated summary (\cite{sonawane2016coreference}), or by introducing a more complex
        encoder architecture that represents and attends to entities as well as sentences (\cite{Huang2021ExtractiveSC}).
        Regarding the trimming heuristic, a natural improvement can be to use text compression techniques for the final predicted sentence (\cite{ghalandari2022efficient, KNIGHT200291}).
        As for the third point, we believe this is very exacerbating reason why our GRU architecture returns a summary
        without any single whole--sentence overlap with the gold standard (Fig.~\ref{fig:rnn_summary}).
        Instead, what \cite{zmandar-etal-2021-joint} propose is a reinforcement learning approach which incorporates
        the \emph{sentence-level} ROUGE-2 score with the whole gold summary.
        While this method is much more sophisticated, it conveys the intuitive idea that the top-$k$ sentences comprising the
        \emph{optimal candidate summary} should be \emph{greedily maximising the global summary-level ROUGE-2} score.
\end{itemize}

\subsection{Future Work}\label{subsec:future-work}