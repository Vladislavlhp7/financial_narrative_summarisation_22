{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorboard --user\n",
    "!pip install numpy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from query import get_embedding_model, get_keyed_word_vectors_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word_embedding(model, word: str):\n",
    "    \"\"\"\n",
    "    Directly return word embedding\n",
    "    \"\"\"\n",
    "    try:  # if loaded directly from embedding model, e.g., FastText\n",
    "        return model.wv[word]\n",
    "    except AttributeError:  # if we use a pseudo-model, Keyed Word Vectors over Vocabulary\n",
    "        try:\n",
    "            return model[word]\n",
    "        except KeyError:\n",
    "            \n",
    "\n",
    "\n",
    "def get_sentence_tensor(embedding_model, sentence: str, seq_len: int = 50):\n",
    "    \"\"\"\n",
    "    Assemble a sentence tensor by directly loading word embeddings from a pre-trained embedding model up to max length\n",
    "    \"\"\"\n",
    "    sent_arr = []\n",
    "    for i, word in enumerate(word_tokenize(sentence)):\n",
    "        if i > seq_len:\n",
    "            break\n",
    "        sent_arr.append(get_word_embedding(embedding_model, word))\n",
    "    sent_tensor = torch.FloatTensor(np.array(sent_arr))\n",
    "    return sent_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyTrainingStop:\n",
    "    \"\"\"\n",
    "    Implement a class for early stopping of training when validation loss starts increasing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_loss: float = np.inf, delta: float = 0.0, counter: int = 0, patience: int = 1):\n",
    "        self.validation_loss = validation_loss\n",
    "        self.delta = delta\n",
    "        self.counter = counter\n",
    "        self.patience = patience\n",
    "\n",
    "    def early_stop(self, validation_loss: float):\n",
    "        if self.validation_loss <= validation_loss + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.validation_loss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad a batch of sentence tensors\n",
    "def pad_batch(batch_sent_arr):\n",
    "    \"\"\"\n",
    "    Provide a batch (list) of tensor sentences and pad them to the maximal size\n",
    "    Return a batch (list) of same-size sentences\n",
    "    \"\"\"\n",
    "    max_len = max([x.shape[0] for x in batch_sent_arr])\n",
    "    padded_batch = []\n",
    "    for train_sents in batch_sent_arr:\n",
    "        padded_train_sents = torch.zeros(max_len, train_sents.shape[1], dtype=torch.float32)\n",
    "        padded_train_sents[:train_sents.shape[0]] = train_sents\n",
    "        padded_batch.append(padded_train_sents)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def batch_str_to_batch_tensors(train_sents, embedding_model, seq_len: int = 50):\n",
    "    \"\"\"\n",
    "    Convert a list of batch sentences to a batch tensor\n",
    "    \"\"\"\n",
    "    # create a list of word embeddings per sentence\n",
    "    batch_sent_arr = [get_sentence_tensor(embedding_model=embedding_model,\n",
    "                                          sentence=str(sent),\n",
    "                                          seq_len=seq_len) for sent in train_sents]\n",
    "    # ensure all sentences (tensors) in the batch have the same length, hence padding\n",
    "    batch_sent_arr_padded = pad_batch(batch_sent_arr)\n",
    "    # stack sentence tensors onto each other for a batch tensor\n",
    "    batch_sent_tensor = torch.stack(batch_sent_arr_padded)\n",
    "    return batch_sent_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=256, num_layers=2, label_size=2, bidirectional=True,\n",
    "                 batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = True\n",
    "        dt = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "        self.name = f'LSTM_bin_classifier-{dt}.pt'\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=batch_first)\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "        else:\n",
    "            self.D = 1\n",
    "        self.hidden2label = nn.Linear(in_features=self.D * hidden_size, out_features=label_size)\n",
    "\n",
    "    def forward(self, sent):\n",
    "        out, _ = self.lstm(sent)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.hidden2label(out)\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FNS2021(Dataset):\n",
    "    def __init__(self, file: str, training: bool = True, train_ratio: float = 0.9, random_state: int = 1):\n",
    "        \"\"\"\n",
    "        Custom class for FNS 2021 Competition to load training and validation data. \\\n",
    "        Original validation data is used as testing\n",
    "        \"\"\"\n",
    "        self.total_data_df = pd.read_csv(file)\n",
    "        train_df, validation_df = train_test_split(self.total_data_df, test_size=1 - train_ratio,\n",
    "                                                   random_state=random_state, stratify=self.total_data_df.label)\n",
    "        if training:\n",
    "            self.sent_labels_df = train_df\n",
    "        else:\n",
    "            self.sent_labels_df = validation_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.total_data_df.loc[idx, 'sent']\n",
    "        label = self.total_data_df.loc[idx, 'label']\n",
    "        return sent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_one_epoch(model, train_dataloader, embedding_model, seq_len, epoch_index, writer, criterion, optimizer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "    with tqdm(total=len(train_dataloader)) as pbar:\n",
    "        for i, (train_sents, train_labels) in enumerate(train_dataloader):\n",
    "            batch_sent_tensor = batch_str_to_batch_tensors(train_sents=train_sents, embedding_model=embedding_model,\n",
    "                                                           seq_len=seq_len)\n",
    "            train_labels = train_labels.long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output_labels = model(batch_sent_tensor)\n",
    "            loss = criterion(output_labels, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if i % 1000 == 999:\n",
    "                last_loss = running_loss / 1000  # loss per batch\n",
    "                pbar.update(1000)\n",
    "                print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "                tb_x = epoch_index * len(train_dataloader) + i + 1\n",
    "                writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "                running_loss = 0.0\n",
    "                # torch.save(model.state_dict(), model.name)\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, embedding_model, train_dataloader, validation_dataloader, writer,\n",
    "          epochs: int = 60, lr: float = 1e-3, seq_len: int = 50):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    early_stopper = EarlyTrainingStop()\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print('EPOCH {}:'.format(epoch + 1))\n",
    "        # Training 1 Epoch\n",
    "        model.train(True)\n",
    "        training_loss = train_one_epoch(model=model, embedding_model=embedding_model, seq_len=seq_len,\n",
    "                                        epoch_index=epoch, writer=writer, criterion=criterion,\n",
    "                                        optimizer=optimizer, train_dataloader=train_dataloader)\n",
    "        # Validation\n",
    "        model.train(False)\n",
    "        running_vloss = 0.\n",
    "        i = 0\n",
    "        for i, (v_sents, v_labels) in enumerate(validation_dataloader):\n",
    "            batch_sent_tensor = batch_str_to_batch_tensors(train_sents=v_sents, embedding_model=embedding_model,\n",
    "                                                           seq_len=seq_len)\n",
    "            train_labels = v_labels.long()\n",
    "\n",
    "            output_labels = model(batch_sent_tensor)\n",
    "            vloss = criterion(output_labels, train_labels)\n",
    "            running_vloss += vloss\n",
    "        validation_loss = running_vloss / (i + 1)\n",
    "        print('LOSS train {} valid {}'.format(training_loss, validation_loss))\n",
    "        # Log the running loss averaged per batch for both training and validation\n",
    "        writer.add_scalars('Training vs. Validation Loss',\n",
    "                           {'Training': training_loss, 'Validation': validation_loss}, epoch + 1)\n",
    "        writer.flush()\n",
    "        # Stop training if validation loss starts growing and save model parameters\n",
    "        if early_stopper.early_stop(validation_loss=validation_loss):\n",
    "            torch.save(model.state_dict(), model.name)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def recalc_keyed_vector(embedding_weights, train_dataloader, validation_dataloader, file_path: str = None, save_file: bool = True, root: str = '..'):\n",
    "    # Try directly loading existing embedding dict from pickle file\n",
    "    default_file_path = f'{root}/tmp/corpus_embeddings_CSF.pickle'\n",
    "    if file_path is None:\n",
    "        file_path = default_file_path\n",
    "    if os.path.exists(file_path):\n",
    "        print(f'Loading Keyed Word Vectors from {file_path}')\n",
    "        with open(file_path, 'rb') as handle:\n",
    "            token2embedding = pickle.load(handle)\n",
    "        return token2embedding\n",
    "    print('Pulling out vocabulary')\n",
    "    corpus = set()\n",
    "    for i, (sents, labels) in enumerate(train_dataloader):\n",
    "        for j, s in enumerate(sents):\n",
    "            for t in word_tokenize(s):\n",
    "                    corpus.add(t)\n",
    "    for i, (sents, labels) in enumerate(validation_dataloader):\n",
    "        for j, s in enumerate(sents):\n",
    "            for t in word_tokenize(s):\n",
    "                    corpus.add(t)\n",
    "    tokens = sorted(list(corpus))\n",
    "    print('Recalculating Keyed Word Vector Embeddings')\n",
    "    token2embedding = {}\n",
    "    for token in tokens:\n",
    "        token2embedding[token] = embedding_weights[token]\n",
    "    if save_file:\n",
    "        if file_path is None:\n",
    "            file_path = default_file_path\n",
    "        print(f'Saving embedding dict to {file_path}')\n",
    "        with open(file_path, 'wb') as handle:\n",
    "            pickle.dump(token2embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return token2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from query import get_embedding_model, get_corpus_vocabulary\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(root: str = '..', batch_size: int = 16, EPOCHS: int = 3, lr: float = 1e-3):\n",
    "    REGEN_VOCAB = False\n",
    "    LOAD_KEYED_VECTOR = True\n",
    "    input_size = 300\n",
    "    seq_len = 50\n",
    "    num_layers = 2\n",
    "\n",
    "    # Set device to CPU or CUDA\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        print('Computational device chosen: CUDA')\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    else:\n",
    "        print('Computational device chosen: CPU')\n",
    "        \n",
    "    print('Loading Training & Validation Data')\n",
    "    data_filename = 'training_corpus_2023-02-07 16-33.csv'\n",
    "    training_data = FNS2021(file=f'{root}/tmp/{data_filename}', training=True)\n",
    "    validation_data = FNS2021(file=f'{root}/tmp/{data_filename}', training=False)\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch_size, drop_last=True)\n",
    "    validation_dataloader = DataLoader(validation_data, batch_size=batch_size, drop_last=True)\n",
    "    \n",
    "    if REGEN_VOCAB:\n",
    "        embedding_model_weights = get_embedding_model(root=root).wv\n",
    "        embedding_model = recalc_keyed_vector(root=root, train_dataloader=train_dataloader, validation_dataloader=validation_dataloader, embedding_weights=embedding_model_weights)\n",
    "    \n",
    "    # Load Embeddings either by vocabulary keyed vector or FastText model\n",
    "    if LOAD_KEYED_VECTOR:\n",
    "        embedding_model = get_keyed_word_vectors_pickle(root=root, file_path=f'{root}/tmp/corpus_embeddings_CSF.pickle')\n",
    "    elif not REGEN_VOCAB:\n",
    "        embedding_model = get_embedding_model(root=root)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    model = LSTM(input_size=input_size, num_layers=num_layers)\n",
    "\n",
    "    writer = SummaryWriter('PyCharm-' + model.name)\n",
    "\n",
    "    print('Starting LSTM training')\n",
    "    train(model=model, embedding_model=embedding_model,\n",
    "          train_dataloader=train_dataloader, validation_dataloader=validation_dataloader,\n",
    "          lr=lr, epochs=EPOCHS, seq_len=seq_len, writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment1(root: str = '..'):\n",
    "    lr = 1e-3\n",
    "    EPOCHS = 3\n",
    "    batch_size = 8\n",
    "    run(lr=lr, EPOCHS=EPOCHS, batch_size=batch_size, root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "root = '..'\n",
    "experiment1(root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e76bd8c231e292af9fed930952536024ca0ff12c564b03bb5fb6c7c303e50d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
