{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3.8\n",
      "3.8.0 (v3.8.0:fa919fdf25, Oct 14 2019, 10:23:27) \n",
      "[Clang 6.0 (clang-600.0.57)]\n",
      "sys.version_info(major=3, minor=8, micro=0, releaselevel='final', serial=0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorboard --user\n",
    "# !pip install numpy \n",
    "# !pip install pandas\n",
    "# !pip install torch\n",
    "# !pip install scikit\n",
    "# !pip install tqdm\n",
    "# !pip install --upgrade pip\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# from query import get_embedding_model, get_keyed_word_vectors_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word_embedding(model, word: str):\n",
    "    \"\"\"\n",
    "    Directly return word embedding\n",
    "    \"\"\"\n",
    "    try:  # if loaded directly from embedding model, e.g., FastText\n",
    "        return model.wv[word]\n",
    "    except AttributeError:  # if we use a pseudo-model, Keyed Word Vectors over Vocabulary\n",
    "        try: \n",
    "            return model[word]\n",
    "        except:\n",
    "            return model(word)\n",
    "            \n",
    "\n",
    "\n",
    "def get_sentence_tensor(embedding_model, sentence: str, seq_len: int = 50):\n",
    "    \"\"\"\n",
    "    Assemble a sentence tensor by directly loading word embeddings from a pre-trained embedding model up to max length\n",
    "    \"\"\"\n",
    "    sent_arr = []\n",
    "    for i, word in enumerate(word_tokenize(sentence)):\n",
    "        if i > seq_len:\n",
    "            break\n",
    "        sent_arr.append(get_word_embedding(embedding_model, word))\n",
    "    sent_tensor = torch.FloatTensor(np.array(sent_arr))\n",
    "    return sent_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyTrainingStop:\n",
    "    \"\"\"\n",
    "    Implement a class for early stopping of training when validation loss starts increasing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, validation_loss: float = np.inf, delta: float = 0.0, counter: int = 0, patience: int = 1):\n",
    "        self.validation_loss = validation_loss\n",
    "        self.delta = delta\n",
    "        self.counter = counter\n",
    "        self.patience = patience\n",
    "\n",
    "    def early_stop(self, validation_loss: float):\n",
    "        if self.validation_loss <= validation_loss + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter > self.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.counter = 0\n",
    "            self.validation_loss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad a batch of sentence tensors\n",
    "def pad_batch(batch_sent_arr):\n",
    "    \"\"\"\n",
    "    Provide a batch (list) of tensor sentences and pad them to the maximal size\n",
    "    Return a batch (list) of same-size sentences\n",
    "    \"\"\"\n",
    "    max_len = max([x.shape[0] for x in batch_sent_arr])\n",
    "    padded_batch = []\n",
    "    for train_sents in batch_sent_arr:\n",
    "        padded_train_sents = torch.zeros(max_len, train_sents.shape[1], dtype=torch.float32)\n",
    "        padded_train_sents[:train_sents.shape[0]] = train_sents\n",
    "        padded_batch.append(padded_train_sents)\n",
    "    return padded_batch\n",
    "\n",
    "\n",
    "def batch_str_to_batch_tensors(sentence_list, embedding_model, seq_len: int = 50):\n",
    "    \"\"\"\n",
    "    Convert a list of batch sentences to a batch tensor\n",
    "    \"\"\"\n",
    "    # create a list of word embeddings per sentence\n",
    "    batch_sent_arr = [get_sentence_tensor(embedding_model=embedding_model,\n",
    "                                          sentence=str(sent),\n",
    "                                          seq_len=seq_len) for sent in sentence_list]\n",
    "    # ensure all sentences (tensors) in the batch have the same length, hence padding\n",
    "    batch_sent_arr_padded = pad_batch(batch_sent_arr)\n",
    "    # stack sentence tensors onto each other for a batch tensor\n",
    "    batch_sent_tensor = torch.stack(batch_sent_arr_padded)\n",
    "    return batch_sent_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=256, num_layers=2, label_size=2, bidirectional=True,\n",
    "                 batch_first=True):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = True\n",
    "        dt = datetime.now().strftime(\"%Y-%m-%d-%H-%M\")\n",
    "        self.name = f'LSTM_bin_classifier-{dt}.pt'\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=batch_first)\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "        else:\n",
    "            self.D = 1\n",
    "        self.hidden2label = nn.Linear(in_features=self.D * hidden_size, out_features=label_size)\n",
    "\n",
    "    def forward(self, sent):\n",
    "        out, _ = self.lstm(sent)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.hidden2label(out)\n",
    "        return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNS2021(Dataset):\n",
    "    def __init__(self, file: str, type: str = 'training', train_ratio: float = 0.9, random_state: int = 1,\n",
    "                 downsample_rate: float = None):\n",
    "        \"\"\"\n",
    "        Custom class for FNS 2021 Competition to load training and validation data. \\\n",
    "        Original validation data is used as testing\n",
    "        \"\"\"\n",
    "        self.total_data_df = pd.read_csv(file).drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "        self.total_data_df.index.name = 'sent_index'\n",
    "        self.total_data_df.reset_index(inplace=True)\n",
    "        if type == 'testing':\n",
    "            self.sent_labels_df = self.total_data_df\n",
    "        else:\n",
    "            train_df, validation_df = train_test_split(self.total_data_df, test_size=1 - train_ratio,\n",
    "                                                    random_state=random_state, stratify=self.total_data_df.label)\n",
    "            if type == \"training\":\n",
    "                if downsample_rate is not None:\n",
    "                    train_df = self.downsample(df=train_df, rate=downsample_rate, random_state=random_state)\n",
    "                self.sent_labels_df = train_df\n",
    "            elif type == \"validation\":\n",
    "                self.sent_labels_df = validation_df\n",
    "        self.sent_labels_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    def downsample(self, df: pd.DataFrame, rate: float = 0.5, random_state: int = 1):\n",
    "        summary_df = df.loc[df['label'] == 1]\n",
    "        non_summary_df = df.loc[df['label'] == 0]\n",
    "        non_summary_df = resample(non_summary_df,\n",
    "                                  replace=True,\n",
    "                                  n_samples=int(len(non_summary_df) * (1 - rate)),\n",
    "                                  random_state=random_state)\n",
    "        df = pd.concat([summary_df, non_summary_df]).sort_values(['sent_index'])#.reset_index(drop=True)\n",
    "        # TODO: Downsample only when report data is predominantly 0-labeled\n",
    "        return df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sent_labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.sent_labels_df.loc[idx, 'sent']\n",
    "        label = self.sent_labels_df.loc[idx, 'label']\n",
    "        return sent, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '..'\n",
    "config = {'batch_size': 16}\n",
    "print('Loading Training Data')\n",
    "data_filename = 'training_corpus_2023-02-07 16-33.csv'\n",
    "training_data = FNS2021(file=f'{root}/tmp/{data_filename}', type='training', downsample_rate=None)  # aggressive downsample\n",
    "# train_dataloader = DataLoader(training_data, batch_size=config.batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.total_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.sent_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Validation Data')\n",
    "validation_data = FNS2021(file=f'{root}/tmp/{data_filename}', training=False,\n",
    "                            downsample_rate=None)  # use all validation data\n",
    "# validation_dataloader = DataLoader(validation_data, batch_size=config.batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.sent_labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.sent_labels_df.loc[validation_data.sent_labels_df.sent_index.isin(training_data.sent_labels_df.sent_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df = pd.read_csv('../tmp/validation_corpus_2023-02-07 16-33.csv')\n",
    "testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.loc[validation_data.sent_labels_df.sent_index.isin(training_data.sent_labels_df.sent_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "model_path = '../tmp/FNS-biLSTM-classification.h5'\n",
    "\n",
    "model = LSTM(hidden_size=128)\n",
    "model_path = '../tmp/model-0.0005-128-0.9-2023-02-21-11-29.h5'\n",
    "\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = get_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Testing Data')\n",
    "testing_data = FNS2021(file=f'../tmp/validation_corpus_2023-02-07 16-33.csv', type='testing')  # use all testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = lambda x: torch.rand(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9775)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def test(model, embedding_model, test_dataloader, seq_len, device):\n",
    "    running_acc = 0.0  # Total accuracy for both classes\n",
    "    running_acc_1 = 0.0  # Accuracy for summary class\n",
    "\n",
    "    # Initialize lists to store true labels and predicted labels\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (test_data, test_labels) in enumerate(test_dataloader):\n",
    "            batch_sent_tensor = batch_str_to_batch_tensors(sentence_list=test_data, embedding_model=embedding_model,\n",
    "                                                           seq_len=seq_len).to(device)\n",
    "            target = test_labels.long().to(device)\n",
    "            predicted = model(batch_sent_tensor)\n",
    "            # Calculate and record per-batch accuracy\n",
    "            winners = predicted.argmax(dim=1)  # each sentence has p0 and p1 probabilities with p0 + p1 = 1\n",
    "            corrects = (winners == target)  # match predicted output labels with observed labels\n",
    "            accuracy = corrects.sum().float() / float(target.size(0))\n",
    "            running_acc += accuracy\n",
    "            summary_winners = ((winners == target) * (target == 1)).float()\n",
    "            summary_winners_perc = summary_winners.sum() / max((target == 1).sum(), 1)\n",
    "            running_acc_1 += summary_winners_perc.sum()\n",
    "            # Prepare data for confusion matrix split\n",
    "            true_labels += target.cpu().numpy().tolist()\n",
    "            pred_labels += winners.cpu().numpy().tolist()\n",
    "            # Find False Positives and True Negatives\n",
    "            cm = confusion_matrix(true_labels, pred_labels)\n",
    "            # Calculate the false positive rate (FPR)\n",
    "            fpr = cm[0][1] / (cm[0][1] + cm[1][1])\n",
    "            # Calculate the true negative rate (TNR)\n",
    "            tnr = cm[1][1] / (cm[1][0] + cm[1][1])\n",
    "            # Calculate the precision\n",
    "            precision = cm[0][0] / (cm[0][0] + cm[0][1])\n",
    "            # Calculate the recall\n",
    "            recall = cm[0][0] / (cm[0][0] + cm[1][0])\n",
    "\n",
    "            last_acc = running_acc / (i + 1)  # total accuracy per batch\n",
    "            last_acc_1 = running_acc_1 / (i + 1)  # summary sent accuracy per batch\n",
    "            print('Testing total accuracy: {} summary accuracy: {} precision: {}, recall {}'.format(last_acc, last_acc_1, precision, recall))\n",
    "        # wandb.log({\n",
    "        #     \"Total Testing Accuracy\": last_acc,\n",
    "        #     \"Summary Testing Accuracy\": last_acc_1,\n",
    "        #     \"Recall\": recall,\n",
    "        #     \"Precision\": precision,\n",
    "        #     'False Positive Rate': fpr,\n",
    "        #     'True Negative Rate': tnr,\n",
    "        # })\n",
    "    return last_acc, last_acc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading Testing Data')\n",
    "testing_data = FNS2021(file=f'../tmp/validation_corpus_2023-02-07 16-33.csv', type='testing')  # use all testing data\n",
    "\n",
    "testing_data.sent_labels_df = testing_data.sent_labels_df.loc[testing_data.sent_labels_df.report.isin([31938])]\n",
    "# testing_data.sent_labels_df = testing_data.sent_labels_df.loc[testing_data.sent_labels_df.report.isin([31938, 31509, 30830, 31290, 32148, 31333, 31469, 30777, 30950,\n",
    "#        32809, 33054, 32376, 33097, 32389, 33083, 31681, 32149, 31440])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.groupby('report').sent.count().reset_index().sent.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data.sent_labels_df.report.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataloader = DataLoader(testing_data, batch_size=len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model=model, embedding_model=embedding_model, test_dataloader=testing_dataloader, seq_len=100, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(testing_df.report.unique()):\n",
    "    report = testing_df.loc[testing_df.report == i]\n",
    "    \n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    batch = batch_str_to_batch_tensors(list(report.sent), embedding_model, 100)\n",
    "    target = torch.tensor(list(report.label))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(batch)\n",
    "\n",
    "        running_acc, running_acc_1 = 0, 0\n",
    "        winners = predicted.argmax(dim=1)  # each sentence has p0 and p1 probabilities with p0 + p1 = 1\n",
    "        \n",
    "        corrects = (winners == target)  # match predicted output labels with observed labels\n",
    "        accuracy = corrects.sum().float() / float(target.size(0))\n",
    "        running_acc += accuracy\n",
    "        summary_winners = ((winners == target) * (target == 1)).float()\n",
    "        summary_winners_perc = summary_winners.sum() / max((target == 1).sum(), 1)\n",
    "        running_acc_1 += summary_winners_perc.sum()\n",
    "\n",
    "\n",
    "        # Append true and predicted labels to lists\n",
    "        true_labels += target.cpu().numpy().tolist()\n",
    "        pred_labels += winners.cpu().numpy().tolist()\n",
    "        cm = confusion_matrix(true_labels, pred_labels)\n",
    "        cm_acc = (cm[0][0] + cm[1][1]) / sum(sum(cm))\n",
    "\n",
    "        print(i, '--->', running_acc, cm_acc, running_acc_1)\n",
    "        accuracies.append({\n",
    "            'report': i,\n",
    "            'total_acc': running_acc, \n",
    "            'summary_acc': running_acc_1\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(accuracies)\n",
    "res.total_acc = [x.numpy() for x in res.total_acc]\n",
    "res.summary_acc = [x.numpy() for x in res.summary_acc]\n",
    "res.to_csv('testing_scores.csv')\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.total_acc = [x.numpy() for x in res.total_acc]\n",
    "res.summary_acc = [x.numpy() for x in res.summary_acc]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5%|▌         | 1/20 [00:27<08:44, 27.59s/it]31938 ---> tensor(0.7549) tensor(0.8587)\n",
    "#  10%|█         | 2/20 [01:00<09:09, 30.54s/it]31509 ---> tensor(0.7957) tensor(0.9464)\n",
    "#  15%|█▌        | 3/20 [01:31<08:46, 30.99s/it]30830 ---> tensor(0.7778) tensor(0.9239)\n",
    "#  20%|██        | 4/20 [01:43<06:11, 23.22s/it]31290 ---> tensor(0.9528) tensor(0.9028)\n",
    "#  25%|██▌       | 5/20 [03:55<15:40, 62.73s/it]32148 ---> tensor(0.7466) tensor(0.9663)\n",
    "#  30%|███       | 6/20 [04:24<11:55, 51.12s/it]31333 ---> tensor(0.8972) tensor(0.9735)\n",
    "#  35%|███▌      | 7/20 [06:08<14:51, 68.57s/it]31469 ---> tensor(0.7980) tensor(0.8135)\n",
    "#  35%|███▌      | 7/20 [07:06<13:11, 60.89s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../tmp/training_corpus_2023-02-07 16-33.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.loc[testing_df.report.isin(train_df.report.unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e76bd8c231e292af9fed930952536024ca0ff12c564b03bb5fb6c7c303e50d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
